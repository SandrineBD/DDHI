entry_id,updated,published,title,authors,summary,comment,journal_ref,doi,primary_category,categories,links,pdf_url,_raw,first_author,_result,ids
http://arxiv.org/abs/2106.01043v1,2021-06-02 09:33:05+00:00,2021-06-02 09:33:05+00:00,Causal Discovery in Knowledge Graphs by Exploiting Asymmetric Properties of Non-Gaussian Distributions,"[arxiv.Result.Author('Rohan Giriraj'), arxiv.Result.Author('Sinnu Susan Thomas')]","In recent years, causal modelling has been used widely to improve
generalization and to provide interpretability in machine learning models. To
determine cause-effect relationships in the absence of a randomized trial, we
can model causal systems with counterfactuals and interventions given enough
domain knowledge. However, there are several cases where domain knowledge is
almost absent and the only recourse is using a statistical method to estimate
causal relationships. While there have been several works done in estimating
causal relationships in unstructured data, we are yet to find a well-defined
framework for estimating causal relationships in Knowledge Graphs (KG). It is
commonly used to provide a semantic framework for data with complex
inter-domain relationships. In this work, we define a hybrid approach that
allows us to discover cause-effect relationships in KG. The proposed approach
is based around the finding of the instantaneous causal structure of a
non-experimental matrix using a non-Gaussian model, i.e; finding the causal
ordering of the variables in a non-Gaussian setting. The non-experimental
matrix is a low-dimensional tensor projection obtained by decomposing the
adjacency tensor of a KG. We use two different pre-existing algorithms, one for
the causal discovery and the other for decomposing the KG and combining them to
get the causal structure in a KG.","12 pages, 6 figures",,,cs.LG,"['cs.LG', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2106.01043v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.01043v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2106.01043v1,"{'id': 'http://arxiv.org/abs/2106.01043v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2106.01043v1', 'updated': '2021-06-02T09:33:05Z', 'updated_parsed': time.struct_time(tm_year=2021, tm_mon=6, tm_mday=2, tm_hour=9, tm_min=33, tm_sec=5, tm_wday=2, tm_yday=153, tm_isdst=0), 'published': '2021-06-02T09:33:05Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=6, tm_mday=2, tm_hour=9, tm_min=33, tm_sec=5, tm_wday=2, tm_yday=153, tm_isdst=0), 'title': 'Causal Discovery in Knowledge Graphs by Exploiting Asymmetric Properties\n  of Non-Gaussian Distributions', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Causal Discovery in Knowledge Graphs by Exploiting Asymmetric Properties\n  of Non-Gaussian Distributions'}, 'summary': 'In recent years, causal modelling has been used widely to improve\ngeneralization and to provide interpretability in machine learning models. To\ndetermine cause-effect relationships in the absence of a randomized trial, we\ncan model causal systems with counterfactuals and interventions given enough\ndomain knowledge. However, there are several cases where domain knowledge is\nalmost absent and the only recourse is using a statistical method to estimate\ncausal relationships. While there have been several works done in estimating\ncausal relationships in unstructured data, we are yet to find a well-defined\nframework for estimating causal relationships in Knowledge Graphs (KG). It is\ncommonly used to provide a semantic framework for data with complex\ninter-domain relationships. In this work, we define a hybrid approach that\nallows us to discover cause-effect relationships in KG. The proposed approach\nis based around the finding of the instantaneous causal structure of a\nnon-experimental matrix using a non-Gaussian model, i.e; finding the causal\nordering of the variables in a non-Gaussian setting. The non-experimental\nmatrix is a low-dimensional tensor projection obtained by decomposing the\nadjacency tensor of a KG. We use two different pre-existing algorithms, one for\nthe causal discovery and the other for decomposing the KG and combining them to\nget the causal structure in a KG.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'In recent years, causal modelling has been used widely to improve\ngeneralization and to provide interpretability in machine learning models. To\ndetermine cause-effect relationships in the absence of a randomized trial, we\ncan model causal systems with counterfactuals and interventions given enough\ndomain knowledge. However, there are several cases where domain knowledge is\nalmost absent and the only recourse is using a statistical method to estimate\ncausal relationships. While there have been several works done in estimating\ncausal relationships in unstructured data, we are yet to find a well-defined\nframework for estimating causal relationships in Knowledge Graphs (KG). It is\ncommonly used to provide a semantic framework for data with complex\ninter-domain relationships. In this work, we define a hybrid approach that\nallows us to discover cause-effect relationships in KG. The proposed approach\nis based around the finding of the instantaneous causal structure of a\nnon-experimental matrix using a non-Gaussian model, i.e; finding the causal\nordering of the variables in a non-Gaussian setting. The non-experimental\nmatrix is a low-dimensional tensor projection obtained by decomposing the\nadjacency tensor of a KG. We use two different pre-existing algorithms, one for\nthe causal discovery and the other for decomposing the KG and combining them to\nget the causal structure in a KG.'}, 'authors': [{'name': 'Rohan Giriraj'}, {'name': 'Sinnu Susan Thomas'}], 'author_detail': {'name': 'Sinnu Susan Thomas'}, 'author': 'Sinnu Susan Thomas', 'arxiv_comment': '12 pages, 6 figures', 'links': [{'href': 'http://arxiv.org/abs/2106.01043v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2106.01043v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Rohan Giriraj,http://arxiv.org/abs/2106.01043v1,2106.01043v1
http://arxiv.org/abs/2010.10596v3,2022-11-15 20:57:53+00:00,2020-10-20 20:08:42+00:00,Counterfactual Explanations and Algorithmic Recourses for Machine Learning: A Review,"[arxiv.Result.Author('Sahil Verma'), arxiv.Result.Author('Varich Boonsanong'), arxiv.Result.Author('Minh Hoang'), arxiv.Result.Author('Keegan E. Hines'), arxiv.Result.Author('John P. Dickerson'), arxiv.Result.Author('Chirag Shah')]","Machine learning plays a role in many deployed decision systems, often in
ways that are difficult or impossible to understand by human stakeholders.
Explaining, in a human-understandable way, the relationship between the input
and output of machine learning models is essential to the development of
trustworthy machine learning based systems. A burgeoning body of research seeks
to define the goals and methods of explainability in machine learning. In this
paper, we seek to review and categorize research on counterfactual
explanations, a specific class of explanation that provides a link between what
could have happened had input to a model been changed in a particular way.
Modern approaches to counterfactual explainability in machine learning draw
connections to the established legal doctrine in many countries, making them
appealing to fielded systems in high-impact areas such as finance and
healthcare. Thus, we design a rubric with desirable properties of
counterfactual explanation algorithms and comprehensively evaluate all
currently proposed algorithms against that rubric. Our rubric provides easy
comparison and comprehension of the advantages and disadvantages of different
approaches and serves as an introduction to major research themes in this
field. We also identify gaps and discuss promising research directions in the
space of counterfactual explainability.",23 pages (8 pages of references),,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2010.10596v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.10596v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2010.10596v3,"{'id': 'http://arxiv.org/abs/2010.10596v3', 'guidislink': True, 'link': 'http://arxiv.org/abs/2010.10596v3', 'updated': '2022-11-15T20:57:53Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=11, tm_mday=15, tm_hour=20, tm_min=57, tm_sec=53, tm_wday=1, tm_yday=319, tm_isdst=0), 'published': '2020-10-20T20:08:42Z', 'published_parsed': time.struct_time(tm_year=2020, tm_mon=10, tm_mday=20, tm_hour=20, tm_min=8, tm_sec=42, tm_wday=1, tm_yday=294, tm_isdst=0), 'title': 'Counterfactual Explanations and Algorithmic Recourses for Machine\n  Learning: A Review', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Counterfactual Explanations and Algorithmic Recourses for Machine\n  Learning: A Review'}, 'summary': 'Machine learning plays a role in many deployed decision systems, often in\nways that are difficult or impossible to understand by human stakeholders.\nExplaining, in a human-understandable way, the relationship between the input\nand output of machine learning models is essential to the development of\ntrustworthy machine learning based systems. A burgeoning body of research seeks\nto define the goals and methods of explainability in machine learning. In this\npaper, we seek to review and categorize research on counterfactual\nexplanations, a specific class of explanation that provides a link between what\ncould have happened had input to a model been changed in a particular way.\nModern approaches to counterfactual explainability in machine learning draw\nconnections to the established legal doctrine in many countries, making them\nappealing to fielded systems in high-impact areas such as finance and\nhealthcare. Thus, we design a rubric with desirable properties of\ncounterfactual explanation algorithms and comprehensively evaluate all\ncurrently proposed algorithms against that rubric. Our rubric provides easy\ncomparison and comprehension of the advantages and disadvantages of different\napproaches and serves as an introduction to major research themes in this\nfield. We also identify gaps and discuss promising research directions in the\nspace of counterfactual explainability.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Machine learning plays a role in many deployed decision systems, often in\nways that are difficult or impossible to understand by human stakeholders.\nExplaining, in a human-understandable way, the relationship between the input\nand output of machine learning models is essential to the development of\ntrustworthy machine learning based systems. A burgeoning body of research seeks\nto define the goals and methods of explainability in machine learning. In this\npaper, we seek to review and categorize research on counterfactual\nexplanations, a specific class of explanation that provides a link between what\ncould have happened had input to a model been changed in a particular way.\nModern approaches to counterfactual explainability in machine learning draw\nconnections to the established legal doctrine in many countries, making them\nappealing to fielded systems in high-impact areas such as finance and\nhealthcare. Thus, we design a rubric with desirable properties of\ncounterfactual explanation algorithms and comprehensively evaluate all\ncurrently proposed algorithms against that rubric. Our rubric provides easy\ncomparison and comprehension of the advantages and disadvantages of different\napproaches and serves as an introduction to major research themes in this\nfield. We also identify gaps and discuss promising research directions in the\nspace of counterfactual explainability.'}, 'authors': [{'name': 'Sahil Verma'}, {'name': 'Varich Boonsanong'}, {'name': 'Minh Hoang'}, {'name': 'Keegan E. Hines'}, {'name': 'John P. Dickerson'}, {'name': 'Chirag Shah'}], 'author_detail': {'name': 'Chirag Shah'}, 'author': 'Chirag Shah', 'arxiv_comment': '23 pages (8 pages of references)', 'links': [{'href': 'http://arxiv.org/abs/2010.10596v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2010.10596v3', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Sahil Verma,http://arxiv.org/abs/2010.10596v3,2010.10596v3
http://arxiv.org/abs/2303.01378v1,2023-03-02 16:03:12+00:00,2023-03-02 16:03:12+00:00,A Vision for Semantically Enriched Data Science,"[arxiv.Result.Author('Udayan Khurana'), arxiv.Result.Author('Kavitha Srinivas'), arxiv.Result.Author('Sainyam Galhotra'), arxiv.Result.Author('Horst Samulowitz')]","The recent efforts in automation of machine learning or data science has
achieved success in various tasks such as hyper-parameter optimization or model
selection. However, key areas such as utilizing domain knowledge and data
semantics are areas where we have seen little automation. Data Scientists have
long leveraged common sense reasoning and domain knowledge to understand and
enrich data for building predictive models. In this paper we discuss important
shortcomings of current data science and machine learning solutions. We then
envision how leveraging ""semantic"" understanding and reasoning on data in
combination with novel tools for data science automation can help with
consistent and explainable data augmentation and transformation. Additionally,
we discuss how semantics can assist data scientists in a new manner by helping
with challenges related to trust, bias, and explainability in machine learning.
Semantic annotation can also help better explore and organize large data
sources.",arXiv admin note: substantial text overlap with arXiv:2205.08018,,,cs.AI,"['cs.AI', 'cs.DB', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2303.01378v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2303.01378v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2303.01378v1,"{'id': 'http://arxiv.org/abs/2303.01378v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2303.01378v1', 'updated': '2023-03-02T16:03:12Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=3, tm_mday=2, tm_hour=16, tm_min=3, tm_sec=12, tm_wday=3, tm_yday=61, tm_isdst=0), 'published': '2023-03-02T16:03:12Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=3, tm_mday=2, tm_hour=16, tm_min=3, tm_sec=12, tm_wday=3, tm_yday=61, tm_isdst=0), 'title': 'A Vision for Semantically Enriched Data Science', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'A Vision for Semantically Enriched Data Science'}, 'summary': 'The recent efforts in automation of machine learning or data science has\nachieved success in various tasks such as hyper-parameter optimization or model\nselection. However, key areas such as utilizing domain knowledge and data\nsemantics are areas where we have seen little automation. Data Scientists have\nlong leveraged common sense reasoning and domain knowledge to understand and\nenrich data for building predictive models. In this paper we discuss important\nshortcomings of current data science and machine learning solutions. We then\nenvision how leveraging ""semantic"" understanding and reasoning on data in\ncombination with novel tools for data science automation can help with\nconsistent and explainable data augmentation and transformation. Additionally,\nwe discuss how semantics can assist data scientists in a new manner by helping\nwith challenges related to trust, bias, and explainability in machine learning.\nSemantic annotation can also help better explore and organize large data\nsources.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'The recent efforts in automation of machine learning or data science has\nachieved success in various tasks such as hyper-parameter optimization or model\nselection. However, key areas such as utilizing domain knowledge and data\nsemantics are areas where we have seen little automation. Data Scientists have\nlong leveraged common sense reasoning and domain knowledge to understand and\nenrich data for building predictive models. In this paper we discuss important\nshortcomings of current data science and machine learning solutions. We then\nenvision how leveraging ""semantic"" understanding and reasoning on data in\ncombination with novel tools for data science automation can help with\nconsistent and explainable data augmentation and transformation. Additionally,\nwe discuss how semantics can assist data scientists in a new manner by helping\nwith challenges related to trust, bias, and explainability in machine learning.\nSemantic annotation can also help better explore and organize large data\nsources.'}, 'authors': [{'name': 'Udayan Khurana'}, {'name': 'Kavitha Srinivas'}, {'name': 'Sainyam Galhotra'}, {'name': 'Horst Samulowitz'}], 'author_detail': {'name': 'Horst Samulowitz'}, 'author': 'Horst Samulowitz', 'arxiv_comment': 'arXiv admin note: substantial text overlap with arXiv:2205.08018', 'links': [{'href': 'http://arxiv.org/abs/2303.01378v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2303.01378v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.DB', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Udayan Khurana,http://arxiv.org/abs/2303.01378v1,2303.01378v1
http://arxiv.org/abs/2002.01650v5,2020-12-07 19:09:35+00:00,2020-02-05 05:28:09+00:00,Concept Whitening for Interpretable Image Recognition,"[arxiv.Result.Author('Zhi Chen'), arxiv.Result.Author('Yijie Bei'), arxiv.Result.Author('Cynthia Rudin')]","What does a neural network encode about a concept as we traverse through the
layers? Interpretability in machine learning is undoubtedly important, but the
calculations of neural networks are very challenging to understand. Attempts to
see inside their hidden layers can either be misleading, unusable, or rely on
the latent space to possess properties that it may not have. In this work,
rather than attempting to analyze a neural network posthoc, we introduce a
mechanism, called concept whitening (CW), to alter a given layer of the network
to allow us to better understand the computation leading up to that layer. When
a concept whitening module is added to a CNN, the axes of the latent space are
aligned with known concepts of interest. By experiment, we show that CW can
provide us a much clearer understanding for how the network gradually learns
concepts over layers. CW is an alternative to a batch normalization layer in
that it normalizes, and also decorrelates (whitens) the latent space. CW can be
used in any layer of the network without hurting predictive performance.","Authors' pre-publication version of a 2020 Nature Machine
  Intelligence article","Nature Machine Intelligence, Vol 2, Dec 2020, 772-782",10.1038/s42256-020-00265-z,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ML']","[arxiv.Result.Link('http://dx.doi.org/10.1038/s42256-020-00265-z', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2002.01650v5', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2002.01650v5', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2002.01650v5,"{'id': 'http://arxiv.org/abs/2002.01650v5', 'guidislink': True, 'link': 'http://arxiv.org/abs/2002.01650v5', 'updated': '2020-12-07T19:09:35Z', 'updated_parsed': time.struct_time(tm_year=2020, tm_mon=12, tm_mday=7, tm_hour=19, tm_min=9, tm_sec=35, tm_wday=0, tm_yday=342, tm_isdst=0), 'published': '2020-02-05T05:28:09Z', 'published_parsed': time.struct_time(tm_year=2020, tm_mon=2, tm_mday=5, tm_hour=5, tm_min=28, tm_sec=9, tm_wday=2, tm_yday=36, tm_isdst=0), 'title': 'Concept Whitening for Interpretable Image Recognition', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Concept Whitening for Interpretable Image Recognition'}, 'summary': 'What does a neural network encode about a concept as we traverse through the\nlayers? Interpretability in machine learning is undoubtedly important, but the\ncalculations of neural networks are very challenging to understand. Attempts to\nsee inside their hidden layers can either be misleading, unusable, or rely on\nthe latent space to possess properties that it may not have. In this work,\nrather than attempting to analyze a neural network posthoc, we introduce a\nmechanism, called concept whitening (CW), to alter a given layer of the network\nto allow us to better understand the computation leading up to that layer. When\na concept whitening module is added to a CNN, the axes of the latent space are\naligned with known concepts of interest. By experiment, we show that CW can\nprovide us a much clearer understanding for how the network gradually learns\nconcepts over layers. CW is an alternative to a batch normalization layer in\nthat it normalizes, and also decorrelates (whitens) the latent space. CW can be\nused in any layer of the network without hurting predictive performance.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'What does a neural network encode about a concept as we traverse through the\nlayers? Interpretability in machine learning is undoubtedly important, but the\ncalculations of neural networks are very challenging to understand. Attempts to\nsee inside their hidden layers can either be misleading, unusable, or rely on\nthe latent space to possess properties that it may not have. In this work,\nrather than attempting to analyze a neural network posthoc, we introduce a\nmechanism, called concept whitening (CW), to alter a given layer of the network\nto allow us to better understand the computation leading up to that layer. When\na concept whitening module is added to a CNN, the axes of the latent space are\naligned with known concepts of interest. By experiment, we show that CW can\nprovide us a much clearer understanding for how the network gradually learns\nconcepts over layers. CW is an alternative to a batch normalization layer in\nthat it normalizes, and also decorrelates (whitens) the latent space. CW can be\nused in any layer of the network without hurting predictive performance.'}, 'authors': [{'name': 'Zhi Chen'}, {'name': 'Yijie Bei'}, {'name': 'Cynthia Rudin'}], 'author_detail': {'name': 'Cynthia Rudin'}, 'author': 'Cynthia Rudin', 'arxiv_doi': '10.1038/s42256-020-00265-z', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.1038/s42256-020-00265-z', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2002.01650v5', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2002.01650v5', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': ""Authors' pre-publication version of a 2020 Nature Machine\n  Intelligence article"", 'arxiv_journal_ref': 'Nature Machine Intelligence, Vol 2, Dec 2020, 772-782', 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Zhi Chen,http://arxiv.org/abs/2002.01650v5,2002.01650v5
http://arxiv.org/abs/2103.12308v1,2021-03-23 05:00:21+00:00,2021-03-23 05:00:21+00:00,IAIA-BL: A Case-based Interpretable Deep Learning Model for Classification of Mass Lesions in Digital Mammography,"[arxiv.Result.Author('Alina Jade Barnett'), arxiv.Result.Author('Fides Regina Schwartz'), arxiv.Result.Author('Chaofan Tao'), arxiv.Result.Author('Chaofan Chen'), arxiv.Result.Author('Yinhao Ren'), arxiv.Result.Author('Joseph Y. Lo'), arxiv.Result.Author('Cynthia Rudin')]","Interpretability in machine learning models is important in high-stakes
decisions, such as whether to order a biopsy based on a mammographic exam.
Mammography poses important challenges that are not present in other computer
vision tasks: datasets are small, confounding information is present, and it
can be difficult even for a radiologist to decide between watchful waiting and
biopsy based on a mammogram alone. In this work, we present a framework for
interpretable machine learning-based mammography. In addition to predicting
whether a lesion is malignant or benign, our work aims to follow the reasoning
processes of radiologists in detecting clinically relevant semantic features of
each image, such as the characteristics of the mass margins. The framework
includes a novel interpretable neural network algorithm that uses case-based
reasoning for mammography. Our algorithm can incorporate a combination of data
with whole image labelling and data with pixel-wise annotations, leading to
better accuracy and interpretability even with a small number of images. Our
interpretable models are able to highlight the classification-relevant parts of
the image, whereas other methods highlight healthy tissue and confounding
information. Our models are decision aids, rather than decision makers, aimed
at better overall human-machine collaboration. We do not observe a loss in mass
margin classification accuracy over a black box neural network trained on the
same data.","24 pages, 5 figures, 2 tables",,,cs.LG,"['cs.LG', 'cs.AI', 'cs.CV', 'I.2.6; I.4.9; I.2.10']","[arxiv.Result.Link('http://arxiv.org/abs/2103.12308v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2103.12308v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2103.12308v1,"{'id': 'http://arxiv.org/abs/2103.12308v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2103.12308v1', 'updated': '2021-03-23T05:00:21Z', 'updated_parsed': time.struct_time(tm_year=2021, tm_mon=3, tm_mday=23, tm_hour=5, tm_min=0, tm_sec=21, tm_wday=1, tm_yday=82, tm_isdst=0), 'published': '2021-03-23T05:00:21Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=3, tm_mday=23, tm_hour=5, tm_min=0, tm_sec=21, tm_wday=1, tm_yday=82, tm_isdst=0), 'title': 'IAIA-BL: A Case-based Interpretable Deep Learning Model for\n  Classification of Mass Lesions in Digital Mammography', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'IAIA-BL: A Case-based Interpretable Deep Learning Model for\n  Classification of Mass Lesions in Digital Mammography'}, 'summary': 'Interpretability in machine learning models is important in high-stakes\ndecisions, such as whether to order a biopsy based on a mammographic exam.\nMammography poses important challenges that are not present in other computer\nvision tasks: datasets are small, confounding information is present, and it\ncan be difficult even for a radiologist to decide between watchful waiting and\nbiopsy based on a mammogram alone. In this work, we present a framework for\ninterpretable machine learning-based mammography. In addition to predicting\nwhether a lesion is malignant or benign, our work aims to follow the reasoning\nprocesses of radiologists in detecting clinically relevant semantic features of\neach image, such as the characteristics of the mass margins. The framework\nincludes a novel interpretable neural network algorithm that uses case-based\nreasoning for mammography. Our algorithm can incorporate a combination of data\nwith whole image labelling and data with pixel-wise annotations, leading to\nbetter accuracy and interpretability even with a small number of images. Our\ninterpretable models are able to highlight the classification-relevant parts of\nthe image, whereas other methods highlight healthy tissue and confounding\ninformation. Our models are decision aids, rather than decision makers, aimed\nat better overall human-machine collaboration. We do not observe a loss in mass\nmargin classification accuracy over a black box neural network trained on the\nsame data.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Interpretability in machine learning models is important in high-stakes\ndecisions, such as whether to order a biopsy based on a mammographic exam.\nMammography poses important challenges that are not present in other computer\nvision tasks: datasets are small, confounding information is present, and it\ncan be difficult even for a radiologist to decide between watchful waiting and\nbiopsy based on a mammogram alone. In this work, we present a framework for\ninterpretable machine learning-based mammography. In addition to predicting\nwhether a lesion is malignant or benign, our work aims to follow the reasoning\nprocesses of radiologists in detecting clinically relevant semantic features of\neach image, such as the characteristics of the mass margins. The framework\nincludes a novel interpretable neural network algorithm that uses case-based\nreasoning for mammography. Our algorithm can incorporate a combination of data\nwith whole image labelling and data with pixel-wise annotations, leading to\nbetter accuracy and interpretability even with a small number of images. Our\ninterpretable models are able to highlight the classification-relevant parts of\nthe image, whereas other methods highlight healthy tissue and confounding\ninformation. Our models are decision aids, rather than decision makers, aimed\nat better overall human-machine collaboration. We do not observe a loss in mass\nmargin classification accuracy over a black box neural network trained on the\nsame data.'}, 'authors': [{'name': 'Alina Jade Barnett'}, {'name': 'Fides Regina Schwartz'}, {'name': 'Chaofan Tao'}, {'name': 'Chaofan Chen'}, {'name': 'Yinhao Ren'}, {'name': 'Joseph Y. Lo'}, {'name': 'Cynthia Rudin'}], 'author_detail': {'name': 'Cynthia Rudin'}, 'author': 'Cynthia Rudin', 'arxiv_comment': '24 pages, 5 figures, 2 tables', 'links': [{'href': 'http://arxiv.org/abs/2103.12308v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2103.12308v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.2.6; I.4.9; I.2.10', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Alina Jade Barnett,http://arxiv.org/abs/2103.12308v1,2103.12308v1
http://arxiv.org/abs/2202.02830v3,2023-06-03 00:05:28+00:00,2022-02-06 18:45:15+00:00,Discovering Personalized Semantics for Soft Attributes in Recommender Systems using Concept Activation Vectors,"[arxiv.Result.Author('Christina Göpfert'), arxiv.Result.Author('Alex Haig'), arxiv.Result.Author('Yinlam Chow'), arxiv.Result.Author('Chih-wei Hsu'), arxiv.Result.Author('Ivan Vendrov'), arxiv.Result.Author('Tyler Lu'), arxiv.Result.Author('Deepak Ramachandran'), arxiv.Result.Author('Hubert Pham'), arxiv.Result.Author('Mohammad Ghavamzadeh'), arxiv.Result.Author('Craig Boutilier')]","Interactive recommender systems have emerged as a promising paradigm to
overcome the limitations of the primitive user feedback used by traditional
recommender systems (e.g., clicks, item consumption, ratings). They allow users
to express intent, preferences, constraints, and contexts in a richer fashion,
often using natural language (including faceted search and dialogue). Yet more
research is needed to find the most effective ways to use this feedback. One
challenge is inferring a user's semantic intent from the open-ended terms or
attributes often used to describe a desired item, and using it to refine
recommendation results. Leveraging concept activation vectors (CAVs) [26], a
recently developed approach for model interpretability in machine learning, we
develop a framework to learn a representation that captures the semantics of
such attributes and connects them to user preferences and behaviors in
recommender systems. One novel feature of our approach is its ability to
distinguish objective and subjective attributes (both subjectivity of degree
and of sense), and associate different senses of subjective attributes with
different users. We demonstrate on both synthetic and real-world data sets that
our CAV representation not only accurately interprets users' subjective
semantics, but can also be used to improve recommendations through interactive
item critiquing.",,,10.1145/1122445.1122456,cs.IR,"['cs.IR', 'cs.AI', 'cs.LG']","[arxiv.Result.Link('http://dx.doi.org/10.1145/1122445.1122456', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2202.02830v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2202.02830v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2202.02830v3,"{'id': 'http://arxiv.org/abs/2202.02830v3', 'guidislink': True, 'link': 'http://arxiv.org/abs/2202.02830v3', 'updated': '2023-06-03T00:05:28Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=3, tm_hour=0, tm_min=5, tm_sec=28, tm_wday=5, tm_yday=154, tm_isdst=0), 'published': '2022-02-06T18:45:15Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=2, tm_mday=6, tm_hour=18, tm_min=45, tm_sec=15, tm_wday=6, tm_yday=37, tm_isdst=0), 'title': 'Discovering Personalized Semantics for Soft Attributes in Recommender\n  Systems using Concept Activation Vectors', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Discovering Personalized Semantics for Soft Attributes in Recommender\n  Systems using Concept Activation Vectors'}, 'summary': ""Interactive recommender systems have emerged as a promising paradigm to\novercome the limitations of the primitive user feedback used by traditional\nrecommender systems (e.g., clicks, item consumption, ratings). They allow users\nto express intent, preferences, constraints, and contexts in a richer fashion,\noften using natural language (including faceted search and dialogue). Yet more\nresearch is needed to find the most effective ways to use this feedback. One\nchallenge is inferring a user's semantic intent from the open-ended terms or\nattributes often used to describe a desired item, and using it to refine\nrecommendation results. Leveraging concept activation vectors (CAVs) [26], a\nrecently developed approach for model interpretability in machine learning, we\ndevelop a framework to learn a representation that captures the semantics of\nsuch attributes and connects them to user preferences and behaviors in\nrecommender systems. One novel feature of our approach is its ability to\ndistinguish objective and subjective attributes (both subjectivity of degree\nand of sense), and associate different senses of subjective attributes with\ndifferent users. We demonstrate on both synthetic and real-world data sets that\nour CAV representation not only accurately interprets users' subjective\nsemantics, but can also be used to improve recommendations through interactive\nitem critiquing."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""Interactive recommender systems have emerged as a promising paradigm to\novercome the limitations of the primitive user feedback used by traditional\nrecommender systems (e.g., clicks, item consumption, ratings). They allow users\nto express intent, preferences, constraints, and contexts in a richer fashion,\noften using natural language (including faceted search and dialogue). Yet more\nresearch is needed to find the most effective ways to use this feedback. One\nchallenge is inferring a user's semantic intent from the open-ended terms or\nattributes often used to describe a desired item, and using it to refine\nrecommendation results. Leveraging concept activation vectors (CAVs) [26], a\nrecently developed approach for model interpretability in machine learning, we\ndevelop a framework to learn a representation that captures the semantics of\nsuch attributes and connects them to user preferences and behaviors in\nrecommender systems. One novel feature of our approach is its ability to\ndistinguish objective and subjective attributes (both subjectivity of degree\nand of sense), and associate different senses of subjective attributes with\ndifferent users. We demonstrate on both synthetic and real-world data sets that\nour CAV representation not only accurately interprets users' subjective\nsemantics, but can also be used to improve recommendations through interactive\nitem critiquing.""}, 'authors': [{'name': 'Christina Göpfert'}, {'name': 'Alex Haig'}, {'name': 'Yinlam Chow'}, {'name': 'Chih-wei Hsu'}, {'name': 'Ivan Vendrov'}, {'name': 'Tyler Lu'}, {'name': 'Deepak Ramachandran'}, {'name': 'Hubert Pham'}, {'name': 'Mohammad Ghavamzadeh'}, {'name': 'Craig Boutilier'}], 'author_detail': {'name': 'Craig Boutilier'}, 'author': 'Craig Boutilier', 'arxiv_doi': '10.1145/1122445.1122456', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.1145/1122445.1122456', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2202.02830v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2202.02830v3', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.IR', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.IR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Christina Göpfert,http://arxiv.org/abs/2202.02830v3,2202.02830v3
http://arxiv.org/abs/2010.07384v2,2021-12-20 17:53:43+00:00,2020-10-14 20:06:28+00:00,Human-interpretable model explainability on high-dimensional data,"[arxiv.Result.Author('Damien de Mijolla'), arxiv.Result.Author('Christopher Frye'), arxiv.Result.Author('Markus Kunesch'), arxiv.Result.Author('John Mansir'), arxiv.Result.Author('Ilya Feige')]","The importance of explainability in machine learning continues to grow, as
both neural-network architectures and the data they model become increasingly
complex. Unique challenges arise when a model's input features become high
dimensional: on one hand, principled model-agnostic approaches to
explainability become too computationally expensive; on the other, more
efficient explainability algorithms lack natural interpretations for general
users. In this work, we introduce a framework for human-interpretable
explainability on high-dimensional data, consisting of two modules. First, we
apply a semantically meaningful latent representation, both to reduce the raw
dimensionality of the data, and to ensure its human interpretability. These
latent features can be learnt, e.g. explicitly as disentangled representations
or implicitly through image-to-image translation, or they can be based on any
computable quantities the user chooses. Second, we adapt the Shapley paradigm
for model-agnostic explainability to operate on these latent features. This
leads to interpretable model explanations that are both theoretically
controlled and computationally tractable. We benchmark our approach on
synthetic data and demonstrate its effectiveness on several
image-classification tasks.","8 pages, 6 figures, 1 appendix",,,cs.LG,"['cs.LG', 'cs.AI', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2010.07384v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.07384v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2010.07384v2,"{'id': 'http://arxiv.org/abs/2010.07384v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2010.07384v2', 'updated': '2021-12-20T17:53:43Z', 'updated_parsed': time.struct_time(tm_year=2021, tm_mon=12, tm_mday=20, tm_hour=17, tm_min=53, tm_sec=43, tm_wday=0, tm_yday=354, tm_isdst=0), 'published': '2020-10-14T20:06:28Z', 'published_parsed': time.struct_time(tm_year=2020, tm_mon=10, tm_mday=14, tm_hour=20, tm_min=6, tm_sec=28, tm_wday=2, tm_yday=288, tm_isdst=0), 'title': 'Human-interpretable model explainability on high-dimensional data', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Human-interpretable model explainability on high-dimensional data'}, 'summary': ""The importance of explainability in machine learning continues to grow, as\nboth neural-network architectures and the data they model become increasingly\ncomplex. Unique challenges arise when a model's input features become high\ndimensional: on one hand, principled model-agnostic approaches to\nexplainability become too computationally expensive; on the other, more\nefficient explainability algorithms lack natural interpretations for general\nusers. In this work, we introduce a framework for human-interpretable\nexplainability on high-dimensional data, consisting of two modules. First, we\napply a semantically meaningful latent representation, both to reduce the raw\ndimensionality of the data, and to ensure its human interpretability. These\nlatent features can be learnt, e.g. explicitly as disentangled representations\nor implicitly through image-to-image translation, or they can be based on any\ncomputable quantities the user chooses. Second, we adapt the Shapley paradigm\nfor model-agnostic explainability to operate on these latent features. This\nleads to interpretable model explanations that are both theoretically\ncontrolled and computationally tractable. We benchmark our approach on\nsynthetic data and demonstrate its effectiveness on several\nimage-classification tasks."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""The importance of explainability in machine learning continues to grow, as\nboth neural-network architectures and the data they model become increasingly\ncomplex. Unique challenges arise when a model's input features become high\ndimensional: on one hand, principled model-agnostic approaches to\nexplainability become too computationally expensive; on the other, more\nefficient explainability algorithms lack natural interpretations for general\nusers. In this work, we introduce a framework for human-interpretable\nexplainability on high-dimensional data, consisting of two modules. First, we\napply a semantically meaningful latent representation, both to reduce the raw\ndimensionality of the data, and to ensure its human interpretability. These\nlatent features can be learnt, e.g. explicitly as disentangled representations\nor implicitly through image-to-image translation, or they can be based on any\ncomputable quantities the user chooses. Second, we adapt the Shapley paradigm\nfor model-agnostic explainability to operate on these latent features. This\nleads to interpretable model explanations that are both theoretically\ncontrolled and computationally tractable. We benchmark our approach on\nsynthetic data and demonstrate its effectiveness on several\nimage-classification tasks.""}, 'authors': [{'name': 'Damien de Mijolla'}, {'name': 'Christopher Frye'}, {'name': 'Markus Kunesch'}, {'name': 'John Mansir'}, {'name': 'Ilya Feige'}], 'author_detail': {'name': 'Ilya Feige'}, 'author': 'Ilya Feige', 'arxiv_comment': '8 pages, 6 figures, 1 appendix', 'links': [{'href': 'http://arxiv.org/abs/2010.07384v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2010.07384v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Damien de Mijolla,http://arxiv.org/abs/2010.07384v2,2010.07384v2
http://arxiv.org/abs/1910.10045v2,2019-12-26 08:09:25+00:00,2019-10-22 15:27:30+00:00,"Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI","[arxiv.Result.Author('Alejandro Barredo Arrieta'), arxiv.Result.Author('Natalia Díaz-Rodríguez'), arxiv.Result.Author('Javier Del Ser'), arxiv.Result.Author('Adrien Bennetot'), arxiv.Result.Author('Siham Tabik'), arxiv.Result.Author('Alberto Barbado'), arxiv.Result.Author('Salvador García'), arxiv.Result.Author('Sergio Gil-López'), arxiv.Result.Author('Daniel Molina'), arxiv.Result.Author('Richard Benjamins'), arxiv.Result.Author('Raja Chatila'), arxiv.Result.Author('Francisco Herrera')]","In the last years, Artificial Intelligence (AI) has achieved a notable
momentum that may deliver the best of expectations over many application
sectors across the field. For this to occur, the entire community stands in
front of the barrier of explainability, an inherent problem of AI techniques
brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not
present in the last hype of AI. Paradigms underlying this problem fall within
the so-called eXplainable AI (XAI) field, which is acknowledged as a crucial
feature for the practical deployment of AI models. This overview examines the
existing literature in the field of XAI, including a prospect toward what is
yet to be reached. We summarize previous efforts to define explainability in
Machine Learning, establishing a novel definition that covers prior conceptual
propositions with a major focus on the audience for which explainability is
sought. We then propose and discuss about a taxonomy of recent contributions
related to the explainability of different Machine Learning models, including
those aimed at Deep Learning methods for which a second taxonomy is built. This
literature analysis serves as the background for a series of challenges faced
by XAI, such as the crossroads between data fusion and explainability. Our
prospects lead toward the concept of Responsible Artificial Intelligence,
namely, a methodology for the large-scale implementation of AI methods in real
organizations with fairness, model explainability and accountability at its
core. Our ultimate goal is to provide newcomers to XAI with a reference
material in order to stimulate future research advances, but also to encourage
experts and professionals from other disciplines to embrace the benefits of AI
in their activity sectors, without any prior bias for its lack of
interpretability.","67 pages, 13 figures, accepted for its publication in Information
  Fusion",,,cs.AI,"['cs.AI', 'cs.LG', 'cs.NE']","[arxiv.Result.Link('http://arxiv.org/abs/1910.10045v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.10045v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1910.10045v2,"{'id': 'http://arxiv.org/abs/1910.10045v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/1910.10045v2', 'updated': '2019-12-26T08:09:25Z', 'updated_parsed': time.struct_time(tm_year=2019, tm_mon=12, tm_mday=26, tm_hour=8, tm_min=9, tm_sec=25, tm_wday=3, tm_yday=360, tm_isdst=0), 'published': '2019-10-22T15:27:30Z', 'published_parsed': time.struct_time(tm_year=2019, tm_mon=10, tm_mday=22, tm_hour=15, tm_min=27, tm_sec=30, tm_wday=1, tm_yday=295, tm_isdst=0), 'title': 'Explainable Artificial Intelligence (XAI): Concepts, Taxonomies,\n  Opportunities and Challenges toward Responsible AI', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Explainable Artificial Intelligence (XAI): Concepts, Taxonomies,\n  Opportunities and Challenges toward Responsible AI'}, 'summary': 'In the last years, Artificial Intelligence (AI) has achieved a notable\nmomentum that may deliver the best of expectations over many application\nsectors across the field. For this to occur, the entire community stands in\nfront of the barrier of explainability, an inherent problem of AI techniques\nbrought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not\npresent in the last hype of AI. Paradigms underlying this problem fall within\nthe so-called eXplainable AI (XAI) field, which is acknowledged as a crucial\nfeature for the practical deployment of AI models. This overview examines the\nexisting literature in the field of XAI, including a prospect toward what is\nyet to be reached. We summarize previous efforts to define explainability in\nMachine Learning, establishing a novel definition that covers prior conceptual\npropositions with a major focus on the audience for which explainability is\nsought. We then propose and discuss about a taxonomy of recent contributions\nrelated to the explainability of different Machine Learning models, including\nthose aimed at Deep Learning methods for which a second taxonomy is built. This\nliterature analysis serves as the background for a series of challenges faced\nby XAI, such as the crossroads between data fusion and explainability. Our\nprospects lead toward the concept of Responsible Artificial Intelligence,\nnamely, a methodology for the large-scale implementation of AI methods in real\norganizations with fairness, model explainability and accountability at its\ncore. Our ultimate goal is to provide newcomers to XAI with a reference\nmaterial in order to stimulate future research advances, but also to encourage\nexperts and professionals from other disciplines to embrace the benefits of AI\nin their activity sectors, without any prior bias for its lack of\ninterpretability.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'In the last years, Artificial Intelligence (AI) has achieved a notable\nmomentum that may deliver the best of expectations over many application\nsectors across the field. For this to occur, the entire community stands in\nfront of the barrier of explainability, an inherent problem of AI techniques\nbrought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not\npresent in the last hype of AI. Paradigms underlying this problem fall within\nthe so-called eXplainable AI (XAI) field, which is acknowledged as a crucial\nfeature for the practical deployment of AI models. This overview examines the\nexisting literature in the field of XAI, including a prospect toward what is\nyet to be reached. We summarize previous efforts to define explainability in\nMachine Learning, establishing a novel definition that covers prior conceptual\npropositions with a major focus on the audience for which explainability is\nsought. We then propose and discuss about a taxonomy of recent contributions\nrelated to the explainability of different Machine Learning models, including\nthose aimed at Deep Learning methods for which a second taxonomy is built. This\nliterature analysis serves as the background for a series of challenges faced\nby XAI, such as the crossroads between data fusion and explainability. Our\nprospects lead toward the concept of Responsible Artificial Intelligence,\nnamely, a methodology for the large-scale implementation of AI methods in real\norganizations with fairness, model explainability and accountability at its\ncore. Our ultimate goal is to provide newcomers to XAI with a reference\nmaterial in order to stimulate future research advances, but also to encourage\nexperts and professionals from other disciplines to embrace the benefits of AI\nin their activity sectors, without any prior bias for its lack of\ninterpretability.'}, 'authors': [{'name': 'Alejandro Barredo Arrieta'}, {'name': 'Natalia Díaz-Rodríguez'}, {'name': 'Javier Del Ser'}, {'name': 'Adrien Bennetot'}, {'name': 'Siham Tabik'}, {'name': 'Alberto Barbado'}, {'name': 'Salvador García'}, {'name': 'Sergio Gil-López'}, {'name': 'Daniel Molina'}, {'name': 'Richard Benjamins'}, {'name': 'Raja Chatila'}, {'name': 'Francisco Herrera'}], 'author_detail': {'name': 'Francisco Herrera'}, 'author': 'Francisco Herrera', 'arxiv_comment': '67 pages, 13 figures, accepted for its publication in Information\n  Fusion', 'links': [{'href': 'http://arxiv.org/abs/1910.10045v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/1910.10045v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.NE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Alejandro Barredo Arrieta,http://arxiv.org/abs/1910.10045v2,1910.10045v2
http://arxiv.org/abs/1807.01308v1,2018-07-03 17:49:14+00:00,2018-07-03 17:49:14+00:00,Proceedings of the 2018 ICML Workshop on Human Interpretability in Machine Learning (WHI 2018),"[arxiv.Result.Author('Been Kim'), arxiv.Result.Author('Kush R. Varshney'), arxiv.Result.Author('Adrian Weller')]","This is the Proceedings of the 2018 ICML Workshop on Human Interpretability
in Machine Learning (WHI 2018), which was held in Stockholm, Sweden, July 14,
2018. Invited speakers were Barbara Engelhardt, Cynthia Rudin, Fernanda
Vi\'egas, and Martin Wattenberg.",,,,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/1807.01308v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1807.01308v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1807.01308v1,"{'id': 'http://arxiv.org/abs/1807.01308v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/1807.01308v1', 'updated': '2018-07-03T17:49:14Z', 'updated_parsed': time.struct_time(tm_year=2018, tm_mon=7, tm_mday=3, tm_hour=17, tm_min=49, tm_sec=14, tm_wday=1, tm_yday=184, tm_isdst=0), 'published': '2018-07-03T17:49:14Z', 'published_parsed': time.struct_time(tm_year=2018, tm_mon=7, tm_mday=3, tm_hour=17, tm_min=49, tm_sec=14, tm_wday=1, tm_yday=184, tm_isdst=0), 'title': 'Proceedings of the 2018 ICML Workshop on Human Interpretability in\n  Machine Learning (WHI 2018)', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Proceedings of the 2018 ICML Workshop on Human Interpretability in\n  Machine Learning (WHI 2018)'}, 'summary': ""This is the Proceedings of the 2018 ICML Workshop on Human Interpretability\nin Machine Learning (WHI 2018), which was held in Stockholm, Sweden, July 14,\n2018. Invited speakers were Barbara Engelhardt, Cynthia Rudin, Fernanda\nVi\\'egas, and Martin Wattenberg."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""This is the Proceedings of the 2018 ICML Workshop on Human Interpretability\nin Machine Learning (WHI 2018), which was held in Stockholm, Sweden, July 14,\n2018. Invited speakers were Barbara Engelhardt, Cynthia Rudin, Fernanda\nVi\\'egas, and Martin Wattenberg.""}, 'authors': [{'name': 'Been Kim'}, {'name': 'Kush R. Varshney'}, {'name': 'Adrian Weller'}], 'author_detail': {'name': 'Adrian Weller'}, 'author': 'Adrian Weller', 'links': [{'href': 'http://arxiv.org/abs/1807.01308v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/1807.01308v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Been Kim,http://arxiv.org/abs/1807.01308v1,1807.01308v1
http://arxiv.org/abs/1708.02666v1,2017-08-08 22:21:11+00:00,2017-08-08 22:21:11+00:00,Proceedings of the 2017 ICML Workshop on Human Interpretability in Machine Learning (WHI 2017),"[arxiv.Result.Author('Been Kim'), arxiv.Result.Author('Dmitry M. Malioutov'), arxiv.Result.Author('Kush R. Varshney'), arxiv.Result.Author('Adrian Weller')]","This is the Proceedings of the 2017 ICML Workshop on Human Interpretability
in Machine Learning (WHI 2017), which was held in Sydney, Australia, August 10,
2017. Invited speakers were Tony Jebara, Pang Wei Koh, and David Sontag.",,,,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/1708.02666v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1708.02666v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1708.02666v1,"{'id': 'http://arxiv.org/abs/1708.02666v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/1708.02666v1', 'updated': '2017-08-08T22:21:11Z', 'updated_parsed': time.struct_time(tm_year=2017, tm_mon=8, tm_mday=8, tm_hour=22, tm_min=21, tm_sec=11, tm_wday=1, tm_yday=220, tm_isdst=0), 'published': '2017-08-08T22:21:11Z', 'published_parsed': time.struct_time(tm_year=2017, tm_mon=8, tm_mday=8, tm_hour=22, tm_min=21, tm_sec=11, tm_wday=1, tm_yday=220, tm_isdst=0), 'title': 'Proceedings of the 2017 ICML Workshop on Human Interpretability in\n  Machine Learning (WHI 2017)', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Proceedings of the 2017 ICML Workshop on Human Interpretability in\n  Machine Learning (WHI 2017)'}, 'summary': 'This is the Proceedings of the 2017 ICML Workshop on Human Interpretability\nin Machine Learning (WHI 2017), which was held in Sydney, Australia, August 10,\n2017. Invited speakers were Tony Jebara, Pang Wei Koh, and David Sontag.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'This is the Proceedings of the 2017 ICML Workshop on Human Interpretability\nin Machine Learning (WHI 2017), which was held in Sydney, Australia, August 10,\n2017. Invited speakers were Tony Jebara, Pang Wei Koh, and David Sontag.'}, 'authors': [{'name': 'Been Kim'}, {'name': 'Dmitry M. Malioutov'}, {'name': 'Kush R. Varshney'}, {'name': 'Adrian Weller'}], 'author_detail': {'name': 'Adrian Weller'}, 'author': 'Adrian Weller', 'links': [{'href': 'http://arxiv.org/abs/1708.02666v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/1708.02666v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Been Kim,http://arxiv.org/abs/1708.02666v1,1708.02666v1
http://arxiv.org/abs/1607.02531v2,2016-07-27 19:00:49+00:00,2016-07-08 21:07:54+00:00,Proceedings of the 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016),"[arxiv.Result.Author('Been Kim'), arxiv.Result.Author('Dmitry M. Malioutov'), arxiv.Result.Author('Kush R. Varshney')]","This is the Proceedings of the 2016 ICML Workshop on Human Interpretability
in Machine Learning (WHI 2016), which was held in New York, NY, June 23, 2016.
  Invited speakers were Susan Athey, Rich Caruana, Jacob Feldman, Percy Liang,
and Hanna Wallach.",,,,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/1607.02531v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1607.02531v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1607.02531v2,"{'id': 'http://arxiv.org/abs/1607.02531v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/1607.02531v2', 'updated': '2016-07-27T19:00:49Z', 'updated_parsed': time.struct_time(tm_year=2016, tm_mon=7, tm_mday=27, tm_hour=19, tm_min=0, tm_sec=49, tm_wday=2, tm_yday=209, tm_isdst=0), 'published': '2016-07-08T21:07:54Z', 'published_parsed': time.struct_time(tm_year=2016, tm_mon=7, tm_mday=8, tm_hour=21, tm_min=7, tm_sec=54, tm_wday=4, tm_yday=190, tm_isdst=0), 'title': 'Proceedings of the 2016 ICML Workshop on Human Interpretability in\n  Machine Learning (WHI 2016)', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Proceedings of the 2016 ICML Workshop on Human Interpretability in\n  Machine Learning (WHI 2016)'}, 'summary': 'This is the Proceedings of the 2016 ICML Workshop on Human Interpretability\nin Machine Learning (WHI 2016), which was held in New York, NY, June 23, 2016.\n  Invited speakers were Susan Athey, Rich Caruana, Jacob Feldman, Percy Liang,\nand Hanna Wallach.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'This is the Proceedings of the 2016 ICML Workshop on Human Interpretability\nin Machine Learning (WHI 2016), which was held in New York, NY, June 23, 2016.\n  Invited speakers were Susan Athey, Rich Caruana, Jacob Feldman, Percy Liang,\nand Hanna Wallach.'}, 'authors': [{'name': 'Been Kim'}, {'name': 'Dmitry M. Malioutov'}, {'name': 'Kush R. Varshney'}], 'author_detail': {'name': 'Kush R. Varshney'}, 'author': 'Kush R. Varshney', 'links': [{'href': 'http://arxiv.org/abs/1607.02531v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/1607.02531v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Been Kim,http://arxiv.org/abs/1607.02531v2,1607.02531v2
http://arxiv.org/abs/1811.01439v1,2018-11-04 21:35:16+00:00,2018-11-04 21:35:16+00:00,Explaining Explanations in AI,"[arxiv.Result.Author('Brent Mittelstadt'), arxiv.Result.Author('Chris Russell'), arxiv.Result.Author('Sandra Wachter')]","Recent work on interpretability in machine learning and AI has focused on the
building of simplified models that approximate the true criteria used to make
decisions. These models are a useful pedagogical device for teaching trained
professionals how to predict what decisions will be made by the complex system,
and most importantly how the system might break. However, when considering any
such model it's important to remember Box's maxim that ""All models are wrong
but some are useful."" We focus on the distinction between these models and
explanations in philosophy and sociology. These models can be understood as a
""do it yourself kit"" for explanations, allowing a practitioner to directly
answer ""what if questions"" or generate contrastive explanations without
external assistance. Although a valuable ability, giving these models as
explanations appears more difficult than necessary, and other forms of
explanation may not have the same trade-offs. We contrast the different schools
of thought on what makes an explanation, and suggest that machine learning
might benefit from viewing the problem more broadly.",FAT* 2019 Proceedings,,10.1145/3287560.3287574,cs.AI,['cs.AI'],"[arxiv.Result.Link('http://dx.doi.org/10.1145/3287560.3287574', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1811.01439v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1811.01439v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1811.01439v1,"{'id': 'http://arxiv.org/abs/1811.01439v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/1811.01439v1', 'updated': '2018-11-04T21:35:16Z', 'updated_parsed': time.struct_time(tm_year=2018, tm_mon=11, tm_mday=4, tm_hour=21, tm_min=35, tm_sec=16, tm_wday=6, tm_yday=308, tm_isdst=0), 'published': '2018-11-04T21:35:16Z', 'published_parsed': time.struct_time(tm_year=2018, tm_mon=11, tm_mday=4, tm_hour=21, tm_min=35, tm_sec=16, tm_wday=6, tm_yday=308, tm_isdst=0), 'title': 'Explaining Explanations in AI', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Explaining Explanations in AI'}, 'summary': 'Recent work on interpretability in machine learning and AI has focused on the\nbuilding of simplified models that approximate the true criteria used to make\ndecisions. These models are a useful pedagogical device for teaching trained\nprofessionals how to predict what decisions will be made by the complex system,\nand most importantly how the system might break. However, when considering any\nsuch model it\'s important to remember Box\'s maxim that ""All models are wrong\nbut some are useful."" We focus on the distinction between these models and\nexplanations in philosophy and sociology. These models can be understood as a\n""do it yourself kit"" for explanations, allowing a practitioner to directly\nanswer ""what if questions"" or generate contrastive explanations without\nexternal assistance. Although a valuable ability, giving these models as\nexplanations appears more difficult than necessary, and other forms of\nexplanation may not have the same trade-offs. We contrast the different schools\nof thought on what makes an explanation, and suggest that machine learning\nmight benefit from viewing the problem more broadly.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Recent work on interpretability in machine learning and AI has focused on the\nbuilding of simplified models that approximate the true criteria used to make\ndecisions. These models are a useful pedagogical device for teaching trained\nprofessionals how to predict what decisions will be made by the complex system,\nand most importantly how the system might break. However, when considering any\nsuch model it\'s important to remember Box\'s maxim that ""All models are wrong\nbut some are useful."" We focus on the distinction between these models and\nexplanations in philosophy and sociology. These models can be understood as a\n""do it yourself kit"" for explanations, allowing a practitioner to directly\nanswer ""what if questions"" or generate contrastive explanations without\nexternal assistance. Although a valuable ability, giving these models as\nexplanations appears more difficult than necessary, and other forms of\nexplanation may not have the same trade-offs. We contrast the different schools\nof thought on what makes an explanation, and suggest that machine learning\nmight benefit from viewing the problem more broadly.'}, 'authors': [{'name': 'Brent Mittelstadt'}, {'name': 'Chris Russell'}, {'name': 'Sandra Wachter'}], 'author_detail': {'name': 'Sandra Wachter'}, 'author': 'Sandra Wachter', 'arxiv_doi': '10.1145/3287560.3287574', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.1145/3287560.3287574', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/1811.01439v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/1811.01439v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': 'FAT* 2019 Proceedings', 'arxiv_primary_category': {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Brent Mittelstadt,http://arxiv.org/abs/1811.01439v1,1811.01439v1
http://arxiv.org/abs/2203.03729v1,2022-03-07 21:30:48+00:00,2022-03-07 21:30:48+00:00,Robustness and Usefulness in AI Explanation Methods,[arxiv.Result.Author('Erick Galinkin')],"Explainability in machine learning has become incredibly important as machine
learning-powered systems become ubiquitous and both regulation and public
sentiment begin to demand an understanding of how these systems make decisions.
As a result, a number of explanation methods have begun to receive widespread
adoption. This work summarizes, compares, and contrasts three popular
explanation methods: LIME, SmoothGrad, and SHAP. We evaluate these methods with
respect to: robustness, in the sense of sample complexity and stability;
understandability, in the sense that provided explanations are consistent with
user expectations; and usability, in the sense that the explanations allow for
the model to be modified based on the output. This work concludes that current
explanation methods are insufficient; that putting faith in and adopting these
methods may actually be worse than simply not using them.",,,,cs.LG,['cs.LG'],"[arxiv.Result.Link('http://arxiv.org/abs/2203.03729v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.03729v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2203.03729v1,"{'id': 'http://arxiv.org/abs/2203.03729v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2203.03729v1', 'updated': '2022-03-07T21:30:48Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=7, tm_hour=21, tm_min=30, tm_sec=48, tm_wday=0, tm_yday=66, tm_isdst=0), 'published': '2022-03-07T21:30:48Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=7, tm_hour=21, tm_min=30, tm_sec=48, tm_wday=0, tm_yday=66, tm_isdst=0), 'title': 'Robustness and Usefulness in AI Explanation Methods', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Robustness and Usefulness in AI Explanation Methods'}, 'summary': 'Explainability in machine learning has become incredibly important as machine\nlearning-powered systems become ubiquitous and both regulation and public\nsentiment begin to demand an understanding of how these systems make decisions.\nAs a result, a number of explanation methods have begun to receive widespread\nadoption. This work summarizes, compares, and contrasts three popular\nexplanation methods: LIME, SmoothGrad, and SHAP. We evaluate these methods with\nrespect to: robustness, in the sense of sample complexity and stability;\nunderstandability, in the sense that provided explanations are consistent with\nuser expectations; and usability, in the sense that the explanations allow for\nthe model to be modified based on the output. This work concludes that current\nexplanation methods are insufficient; that putting faith in and adopting these\nmethods may actually be worse than simply not using them.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Explainability in machine learning has become incredibly important as machine\nlearning-powered systems become ubiquitous and both regulation and public\nsentiment begin to demand an understanding of how these systems make decisions.\nAs a result, a number of explanation methods have begun to receive widespread\nadoption. This work summarizes, compares, and contrasts three popular\nexplanation methods: LIME, SmoothGrad, and SHAP. We evaluate these methods with\nrespect to: robustness, in the sense of sample complexity and stability;\nunderstandability, in the sense that provided explanations are consistent with\nuser expectations; and usability, in the sense that the explanations allow for\nthe model to be modified based on the output. This work concludes that current\nexplanation methods are insufficient; that putting faith in and adopting these\nmethods may actually be worse than simply not using them.'}, 'authors': [{'name': 'Erick Galinkin'}], 'author_detail': {'name': 'Erick Galinkin'}, 'author': 'Erick Galinkin', 'links': [{'href': 'http://arxiv.org/abs/2203.03729v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.03729v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Erick Galinkin,http://arxiv.org/abs/2203.03729v1,2203.03729v1
http://arxiv.org/abs/2202.10335v1,2022-02-21 16:15:57+00:00,2022-02-21 16:15:57+00:00,Explainability in Machine Learning: a Pedagogical Perspective,"[arxiv.Result.Author('Andreas Bueff'), arxiv.Result.Author('Ioannis Papantonis'), arxiv.Result.Author('Auste Simkute'), arxiv.Result.Author('Vaishak Belle')]","Given the importance of integrating of explainability into machine learning,
at present, there are a lack of pedagogical resources exploring this.
Specifically, we have found a need for resources in explaining how one can
teach the advantages of explainability in machine learning. Often pedagogical
approaches in the field of machine learning focus on getting students prepared
to apply various models in the real world setting, but much less attention is
given to teaching students the various techniques one could employ to explain a
model's decision-making process. Furthermore, explainability can benefit from a
narrative structure that aids one in understanding which techniques are
governed by which questions about the data.
  We provide a pedagogical perspective on how to structure the learning process
to better impart knowledge to students and researchers in machine learning,
when and how to implement various explainability techniques as well as how to
interpret the results. We discuss a system of teaching explainability in
machine learning, by exploring the advantages and disadvantages of various
opaque and transparent machine learning models, as well as when to utilize
specific explainability techniques and the various frameworks used to structure
the tools for explainability. Among discussing concrete assignments, we will
also discuss ways to structure potential assignments to best help students
learn to use explainability as a tool alongside any given machine learning
application.
  Data science professionals completing the course will have a birds-eye view
of a rapidly developing area and will be confident to deploy machine learning
more widely. A preliminary analysis on the effectiveness of a recently
delivered course following the structure presented here is included as evidence
supporting our pedagogical approach.",,,,cs.HC,"['cs.HC', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2202.10335v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2202.10335v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2202.10335v1,"{'id': 'http://arxiv.org/abs/2202.10335v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2202.10335v1', 'updated': '2022-02-21T16:15:57Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=2, tm_mday=21, tm_hour=16, tm_min=15, tm_sec=57, tm_wday=0, tm_yday=52, tm_isdst=0), 'published': '2022-02-21T16:15:57Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=2, tm_mday=21, tm_hour=16, tm_min=15, tm_sec=57, tm_wday=0, tm_yday=52, tm_isdst=0), 'title': 'Explainability in Machine Learning: a Pedagogical Perspective', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Explainability in Machine Learning: a Pedagogical Perspective'}, 'summary': ""Given the importance of integrating of explainability into machine learning,\nat present, there are a lack of pedagogical resources exploring this.\nSpecifically, we have found a need for resources in explaining how one can\nteach the advantages of explainability in machine learning. Often pedagogical\napproaches in the field of machine learning focus on getting students prepared\nto apply various models in the real world setting, but much less attention is\ngiven to teaching students the various techniques one could employ to explain a\nmodel's decision-making process. Furthermore, explainability can benefit from a\nnarrative structure that aids one in understanding which techniques are\ngoverned by which questions about the data.\n  We provide a pedagogical perspective on how to structure the learning process\nto better impart knowledge to students and researchers in machine learning,\nwhen and how to implement various explainability techniques as well as how to\ninterpret the results. We discuss a system of teaching explainability in\nmachine learning, by exploring the advantages and disadvantages of various\nopaque and transparent machine learning models, as well as when to utilize\nspecific explainability techniques and the various frameworks used to structure\nthe tools for explainability. Among discussing concrete assignments, we will\nalso discuss ways to structure potential assignments to best help students\nlearn to use explainability as a tool alongside any given machine learning\napplication.\n  Data science professionals completing the course will have a birds-eye view\nof a rapidly developing area and will be confident to deploy machine learning\nmore widely. A preliminary analysis on the effectiveness of a recently\ndelivered course following the structure presented here is included as evidence\nsupporting our pedagogical approach."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""Given the importance of integrating of explainability into machine learning,\nat present, there are a lack of pedagogical resources exploring this.\nSpecifically, we have found a need for resources in explaining how one can\nteach the advantages of explainability in machine learning. Often pedagogical\napproaches in the field of machine learning focus on getting students prepared\nto apply various models in the real world setting, but much less attention is\ngiven to teaching students the various techniques one could employ to explain a\nmodel's decision-making process. Furthermore, explainability can benefit from a\nnarrative structure that aids one in understanding which techniques are\ngoverned by which questions about the data.\n  We provide a pedagogical perspective on how to structure the learning process\nto better impart knowledge to students and researchers in machine learning,\nwhen and how to implement various explainability techniques as well as how to\ninterpret the results. We discuss a system of teaching explainability in\nmachine learning, by exploring the advantages and disadvantages of various\nopaque and transparent machine learning models, as well as when to utilize\nspecific explainability techniques and the various frameworks used to structure\nthe tools for explainability. Among discussing concrete assignments, we will\nalso discuss ways to structure potential assignments to best help students\nlearn to use explainability as a tool alongside any given machine learning\napplication.\n  Data science professionals completing the course will have a birds-eye view\nof a rapidly developing area and will be confident to deploy machine learning\nmore widely. A preliminary analysis on the effectiveness of a recently\ndelivered course following the structure presented here is included as evidence\nsupporting our pedagogical approach.""}, 'authors': [{'name': 'Andreas Bueff'}, {'name': 'Ioannis Papantonis'}, {'name': 'Auste Simkute'}, {'name': 'Vaishak Belle'}], 'author_detail': {'name': 'Vaishak Belle'}, 'author': 'Vaishak Belle', 'links': [{'href': 'http://arxiv.org/abs/2202.10335v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2202.10335v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.HC', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.HC', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Andreas Bueff,http://arxiv.org/abs/2202.10335v1,2202.10335v1
http://arxiv.org/abs/2211.02166v1,2022-11-03 22:34:50+00:00,2022-11-03 22:34:50+00:00,A $k$-additive Choquet integral-based approach to approximate the SHAP values for local interpretability in machine learning,"[arxiv.Result.Author('Guilherme Dean Pelegrina'), arxiv.Result.Author('Leonardo Tomazeli Duarte'), arxiv.Result.Author('Michel Grabisch')]","Besides accuracy, recent studies on machine learning models have been
addressing the question on how the obtained results can be interpreted. Indeed,
while complex machine learning models are able to provide very good results in
terms of accuracy even in challenging applications, it is difficult to
interpret them. Aiming at providing some interpretability for such models, one
of the most famous methods, called SHAP, borrows the Shapley value concept from
game theory in order to locally explain the predicted outcome of an instance of
interest. As the SHAP values calculation needs previous computations on all
possible coalitions of attributes, its computational cost can be very high.
Therefore, a SHAP-based method called Kernel SHAP adopts an efficient strategy
that approximate such values with less computational effort. In this paper, we
also address local interpretability in machine learning based on Shapley
values. Firstly, we provide a straightforward formulation of a SHAP-based
method for local interpretability by using the Choquet integral, which leads to
both Shapley values and Shapley interaction indices. Moreover, we also adopt
the concept of $k$-additive games from game theory, which contributes to reduce
the computational effort when estimating the SHAP values. The obtained results
attest that our proposal needs less computations on coalitions of attributes to
approximate the SHAP values.",,,,cs.LG,['cs.LG'],"[arxiv.Result.Link('http://arxiv.org/abs/2211.02166v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2211.02166v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2211.02166v1,"{'id': 'http://arxiv.org/abs/2211.02166v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2211.02166v1', 'updated': '2022-11-03T22:34:50Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=11, tm_mday=3, tm_hour=22, tm_min=34, tm_sec=50, tm_wday=3, tm_yday=307, tm_isdst=0), 'published': '2022-11-03T22:34:50Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=11, tm_mday=3, tm_hour=22, tm_min=34, tm_sec=50, tm_wday=3, tm_yday=307, tm_isdst=0), 'title': 'A $k$-additive Choquet integral-based approach to approximate the SHAP\n  values for local interpretability in machine learning', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'A $k$-additive Choquet integral-based approach to approximate the SHAP\n  values for local interpretability in machine learning'}, 'summary': 'Besides accuracy, recent studies on machine learning models have been\naddressing the question on how the obtained results can be interpreted. Indeed,\nwhile complex machine learning models are able to provide very good results in\nterms of accuracy even in challenging applications, it is difficult to\ninterpret them. Aiming at providing some interpretability for such models, one\nof the most famous methods, called SHAP, borrows the Shapley value concept from\ngame theory in order to locally explain the predicted outcome of an instance of\ninterest. As the SHAP values calculation needs previous computations on all\npossible coalitions of attributes, its computational cost can be very high.\nTherefore, a SHAP-based method called Kernel SHAP adopts an efficient strategy\nthat approximate such values with less computational effort. In this paper, we\nalso address local interpretability in machine learning based on Shapley\nvalues. Firstly, we provide a straightforward formulation of a SHAP-based\nmethod for local interpretability by using the Choquet integral, which leads to\nboth Shapley values and Shapley interaction indices. Moreover, we also adopt\nthe concept of $k$-additive games from game theory, which contributes to reduce\nthe computational effort when estimating the SHAP values. The obtained results\nattest that our proposal needs less computations on coalitions of attributes to\napproximate the SHAP values.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Besides accuracy, recent studies on machine learning models have been\naddressing the question on how the obtained results can be interpreted. Indeed,\nwhile complex machine learning models are able to provide very good results in\nterms of accuracy even in challenging applications, it is difficult to\ninterpret them. Aiming at providing some interpretability for such models, one\nof the most famous methods, called SHAP, borrows the Shapley value concept from\ngame theory in order to locally explain the predicted outcome of an instance of\ninterest. As the SHAP values calculation needs previous computations on all\npossible coalitions of attributes, its computational cost can be very high.\nTherefore, a SHAP-based method called Kernel SHAP adopts an efficient strategy\nthat approximate such values with less computational effort. In this paper, we\nalso address local interpretability in machine learning based on Shapley\nvalues. Firstly, we provide a straightforward formulation of a SHAP-based\nmethod for local interpretability by using the Choquet integral, which leads to\nboth Shapley values and Shapley interaction indices. Moreover, we also adopt\nthe concept of $k$-additive games from game theory, which contributes to reduce\nthe computational effort when estimating the SHAP values. The obtained results\nattest that our proposal needs less computations on coalitions of attributes to\napproximate the SHAP values.'}, 'authors': [{'name': 'Guilherme Dean Pelegrina'}, {'name': 'Leonardo Tomazeli Duarte'}, {'name': 'Michel Grabisch'}], 'author_detail': {'name': 'Michel Grabisch'}, 'author': 'Michel Grabisch', 'links': [{'href': 'http://arxiv.org/abs/2211.02166v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2211.02166v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Guilherme Dean Pelegrina,http://arxiv.org/abs/2211.02166v1,2211.02166v1
http://arxiv.org/abs/2210.08443v2,2022-11-07 20:29:21+00:00,2022-10-16 04:35:32+00:00,CLEAR: Generative Counterfactual Explanations on Graphs,"[arxiv.Result.Author('Jing Ma'), arxiv.Result.Author('Ruocheng Guo'), arxiv.Result.Author('Saumitra Mishra'), arxiv.Result.Author('Aidong Zhang'), arxiv.Result.Author('Jundong Li')]","Counterfactual explanations promote explainability in machine learning models
by answering the question ""how should an input instance be perturbed to obtain
a desired predicted label?"". The comparison of this instance before and after
perturbation can enhance human interpretation. Most existing studies on
counterfactual explanations are limited in tabular data or image data. In this
work, we study the problem of counterfactual explanation generation on graphs.
A few studies have explored counterfactual explanations on graphs, but many
challenges of this problem are still not well-addressed: 1) optimizing in the
discrete and disorganized space of graphs; 2) generalizing on unseen graphs;
and 3) maintaining the causality in the generated counterfactuals without prior
knowledge of the causal model. To tackle these challenges, we propose a novel
framework CLEAR which aims to generate counterfactual explanations on graphs
for graph-level prediction models. Specifically, CLEAR leverages a graph
variational autoencoder based mechanism to facilitate its optimization and
generalization, and promotes causality by leveraging an auxiliary variable to
better identify the underlying causal model. Extensive experiments on both
synthetic and real-world graphs validate the superiority of CLEAR over the
state-of-the-art methods in different aspects.","18 pages, 9 figures",,,cs.LG,['cs.LG'],"[arxiv.Result.Link('http://arxiv.org/abs/2210.08443v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2210.08443v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2210.08443v2,"{'id': 'http://arxiv.org/abs/2210.08443v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2210.08443v2', 'updated': '2022-11-07T20:29:21Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=11, tm_mday=7, tm_hour=20, tm_min=29, tm_sec=21, tm_wday=0, tm_yday=311, tm_isdst=0), 'published': '2022-10-16T04:35:32Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=10, tm_mday=16, tm_hour=4, tm_min=35, tm_sec=32, tm_wday=6, tm_yday=289, tm_isdst=0), 'title': 'CLEAR: Generative Counterfactual Explanations on Graphs', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'CLEAR: Generative Counterfactual Explanations on Graphs'}, 'summary': 'Counterfactual explanations promote explainability in machine learning models\nby answering the question ""how should an input instance be perturbed to obtain\na desired predicted label?"". The comparison of this instance before and after\nperturbation can enhance human interpretation. Most existing studies on\ncounterfactual explanations are limited in tabular data or image data. In this\nwork, we study the problem of counterfactual explanation generation on graphs.\nA few studies have explored counterfactual explanations on graphs, but many\nchallenges of this problem are still not well-addressed: 1) optimizing in the\ndiscrete and disorganized space of graphs; 2) generalizing on unseen graphs;\nand 3) maintaining the causality in the generated counterfactuals without prior\nknowledge of the causal model. To tackle these challenges, we propose a novel\nframework CLEAR which aims to generate counterfactual explanations on graphs\nfor graph-level prediction models. Specifically, CLEAR leverages a graph\nvariational autoencoder based mechanism to facilitate its optimization and\ngeneralization, and promotes causality by leveraging an auxiliary variable to\nbetter identify the underlying causal model. Extensive experiments on both\nsynthetic and real-world graphs validate the superiority of CLEAR over the\nstate-of-the-art methods in different aspects.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Counterfactual explanations promote explainability in machine learning models\nby answering the question ""how should an input instance be perturbed to obtain\na desired predicted label?"". The comparison of this instance before and after\nperturbation can enhance human interpretation. Most existing studies on\ncounterfactual explanations are limited in tabular data or image data. In this\nwork, we study the problem of counterfactual explanation generation on graphs.\nA few studies have explored counterfactual explanations on graphs, but many\nchallenges of this problem are still not well-addressed: 1) optimizing in the\ndiscrete and disorganized space of graphs; 2) generalizing on unseen graphs;\nand 3) maintaining the causality in the generated counterfactuals without prior\nknowledge of the causal model. To tackle these challenges, we propose a novel\nframework CLEAR which aims to generate counterfactual explanations on graphs\nfor graph-level prediction models. Specifically, CLEAR leverages a graph\nvariational autoencoder based mechanism to facilitate its optimization and\ngeneralization, and promotes causality by leveraging an auxiliary variable to\nbetter identify the underlying causal model. Extensive experiments on both\nsynthetic and real-world graphs validate the superiority of CLEAR over the\nstate-of-the-art methods in different aspects.'}, 'authors': [{'name': 'Jing Ma'}, {'name': 'Ruocheng Guo'}, {'name': 'Saumitra Mishra'}, {'name': 'Aidong Zhang'}, {'name': 'Jundong Li'}], 'author_detail': {'name': 'Jundong Li'}, 'author': 'Jundong Li', 'arxiv_comment': '18 pages, 9 figures', 'links': [{'href': 'http://arxiv.org/abs/2210.08443v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2210.08443v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Jing Ma,http://arxiv.org/abs/2210.08443v2,2210.08443v2
http://arxiv.org/abs/1904.10922v1,2019-04-24 17:01:43+00:00,2019-04-24 17:01:43+00:00,The Scientific Method in the Science of Machine Learning,"[arxiv.Result.Author('Jessica Zosa Forde'), arxiv.Result.Author('Michela Paganini')]","In the quest to align deep learning with the sciences to address calls for
rigor, safety, and interpretability in machine learning systems, this
contribution identifies key missing pieces: the stages of hypothesis
formulation and testing, as well as statistical and systematic uncertainty
estimation -- core tenets of the scientific method. This position paper
discusses the ways in which contemporary science is conducted in other domains
and identifies potentially useful practices. We present a case study from
physics and describe how this field has promoted rigor through specific
methodological practices, and provide recommendations on how machine learning
researchers can adopt these practices into the research ecosystem. We argue
that both domain-driven experiments and application-agnostic questions of the
inner workings of fundamental building blocks of machine learning models ought
to be examined with the tools of the scientific method, to ensure we not only
understand effect, but also begin to understand cause, which is the raison
d'\^{e}tre of science.","4 pages + 1 appendix. Presented at the ICLR 2019 Debugging Machine
  Learning Models workshop",,,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1904.10922v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1904.10922v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1904.10922v1,"{'id': 'http://arxiv.org/abs/1904.10922v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/1904.10922v1', 'updated': '2019-04-24T17:01:43Z', 'updated_parsed': time.struct_time(tm_year=2019, tm_mon=4, tm_mday=24, tm_hour=17, tm_min=1, tm_sec=43, tm_wday=2, tm_yday=114, tm_isdst=0), 'published': '2019-04-24T17:01:43Z', 'published_parsed': time.struct_time(tm_year=2019, tm_mon=4, tm_mday=24, tm_hour=17, tm_min=1, tm_sec=43, tm_wday=2, tm_yday=114, tm_isdst=0), 'title': 'The Scientific Method in the Science of Machine Learning', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'The Scientific Method in the Science of Machine Learning'}, 'summary': ""In the quest to align deep learning with the sciences to address calls for\nrigor, safety, and interpretability in machine learning systems, this\ncontribution identifies key missing pieces: the stages of hypothesis\nformulation and testing, as well as statistical and systematic uncertainty\nestimation -- core tenets of the scientific method. This position paper\ndiscusses the ways in which contemporary science is conducted in other domains\nand identifies potentially useful practices. We present a case study from\nphysics and describe how this field has promoted rigor through specific\nmethodological practices, and provide recommendations on how machine learning\nresearchers can adopt these practices into the research ecosystem. We argue\nthat both domain-driven experiments and application-agnostic questions of the\ninner workings of fundamental building blocks of machine learning models ought\nto be examined with the tools of the scientific method, to ensure we not only\nunderstand effect, but also begin to understand cause, which is the raison\nd'\\^{e}tre of science."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""In the quest to align deep learning with the sciences to address calls for\nrigor, safety, and interpretability in machine learning systems, this\ncontribution identifies key missing pieces: the stages of hypothesis\nformulation and testing, as well as statistical and systematic uncertainty\nestimation -- core tenets of the scientific method. This position paper\ndiscusses the ways in which contemporary science is conducted in other domains\nand identifies potentially useful practices. We present a case study from\nphysics and describe how this field has promoted rigor through specific\nmethodological practices, and provide recommendations on how machine learning\nresearchers can adopt these practices into the research ecosystem. We argue\nthat both domain-driven experiments and application-agnostic questions of the\ninner workings of fundamental building blocks of machine learning models ought\nto be examined with the tools of the scientific method, to ensure we not only\nunderstand effect, but also begin to understand cause, which is the raison\nd'\\^{e}tre of science.""}, 'authors': [{'name': 'Jessica Zosa Forde'}, {'name': 'Michela Paganini'}], 'author_detail': {'name': 'Michela Paganini'}, 'author': 'Michela Paganini', 'arxiv_comment': '4 pages + 1 appendix. Presented at the ICLR 2019 Debugging Machine\n  Learning Models workshop', 'links': [{'href': 'http://arxiv.org/abs/1904.10922v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/1904.10922v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Jessica Zosa Forde,http://arxiv.org/abs/1904.10922v1,1904.10922v1
http://arxiv.org/abs/2103.15114v1,2021-03-28 12:26:56+00:00,2021-03-28 12:26:56+00:00,Explaining Representation by Mutual Information,[arxiv.Result.Author('Lifeng Gu')],"Science is used to discover the law of world. Machine learning can be used to
discover the law of data. In recent years, there are more and more research
about interpretability in machine learning community. We hope the machine
learning methods are safe, interpretable, and they can help us to find
meaningful pattern in data. In this paper, we focus on interpretability of deep
representation. We propose a interpretable method of representation based on
mutual information, which summarizes the interpretation of representation into
three types of information between input data and representation. We further
proposed MI-LR module, which can be inserted into the model to estimate the
amount of information to explain the model's representation. Finally, we verify
the method through the visualization of the prototype network.",,,,cs.LG,"['cs.LG', 'cs.CV']","[arxiv.Result.Link('http://arxiv.org/abs/2103.15114v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2103.15114v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2103.15114v1,"{'id': 'http://arxiv.org/abs/2103.15114v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2103.15114v1', 'updated': '2021-03-28T12:26:56Z', 'updated_parsed': time.struct_time(tm_year=2021, tm_mon=3, tm_mday=28, tm_hour=12, tm_min=26, tm_sec=56, tm_wday=6, tm_yday=87, tm_isdst=0), 'published': '2021-03-28T12:26:56Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=3, tm_mday=28, tm_hour=12, tm_min=26, tm_sec=56, tm_wday=6, tm_yday=87, tm_isdst=0), 'title': 'Explaining Representation by Mutual Information', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Explaining Representation by Mutual Information'}, 'summary': ""Science is used to discover the law of world. Machine learning can be used to\ndiscover the law of data. In recent years, there are more and more research\nabout interpretability in machine learning community. We hope the machine\nlearning methods are safe, interpretable, and they can help us to find\nmeaningful pattern in data. In this paper, we focus on interpretability of deep\nrepresentation. We propose a interpretable method of representation based on\nmutual information, which summarizes the interpretation of representation into\nthree types of information between input data and representation. We further\nproposed MI-LR module, which can be inserted into the model to estimate the\namount of information to explain the model's representation. Finally, we verify\nthe method through the visualization of the prototype network."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""Science is used to discover the law of world. Machine learning can be used to\ndiscover the law of data. In recent years, there are more and more research\nabout interpretability in machine learning community. We hope the machine\nlearning methods are safe, interpretable, and they can help us to find\nmeaningful pattern in data. In this paper, we focus on interpretability of deep\nrepresentation. We propose a interpretable method of representation based on\nmutual information, which summarizes the interpretation of representation into\nthree types of information between input data and representation. We further\nproposed MI-LR module, which can be inserted into the model to estimate the\namount of information to explain the model's representation. Finally, we verify\nthe method through the visualization of the prototype network.""}, 'authors': [{'name': 'Lifeng Gu'}], 'author_detail': {'name': 'Lifeng Gu'}, 'author': 'Lifeng Gu', 'links': [{'href': 'http://arxiv.org/abs/2103.15114v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2103.15114v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Lifeng Gu,http://arxiv.org/abs/2103.15114v1,2103.15114v1
http://arxiv.org/abs/2311.02078v1,2023-09-18 11:59:02+00:00,2023-09-18 11:59:02+00:00,Interpretability is not Explainability: New Quantitative XAI Approach with a focus on Recommender Systems in Education,[arxiv.Result.Author('Riccardo Porcedda')],"The field of eXplainable Artificial Intelligence faces challenges due to the
absence of a widely accepted taxonomy that facilitates the quantitative
evaluation of explainability in Machine Learning algorithms. In this paper, we
propose a novel taxonomy that addresses the current gap in the literature by
providing a clear and unambiguous understanding of the key concepts and
relationships in XAI. Our approach is rooted in a systematic analysis of
existing definitions and frameworks, with a focus on transparency,
interpretability, completeness, complexity and understandability as essential
dimensions of explainability. This comprehensive taxonomy aims to establish a
shared vocabulary for future research. To demonstrate the utility of our
proposed taxonomy, we examine a case study of a Recommender System designed to
curate and recommend the most suitable online resources from MERLOT. By
employing the SHAP package, we quantify and enhance the explainability of the
RS within the context of our newly developed taxonomy.",,"David C. Wyld et al. (Eds): MLNLP, NWCOM, DTMN, ASOFT, SIGPRO,
  AIFZ, CSITY, CLSB - 2023, pp. 171-188, 2023. CS & IT - CSCP 2023",10.5121/csit.2023.131612,cs.IR,"['cs.IR', 'cs.LG']","[arxiv.Result.Link('http://dx.doi.org/10.5121/csit.2023.131612', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2311.02078v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2311.02078v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2311.02078v1,"{'id': 'http://arxiv.org/abs/2311.02078v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2311.02078v1', 'updated': '2023-09-18T11:59:02Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=9, tm_mday=18, tm_hour=11, tm_min=59, tm_sec=2, tm_wday=0, tm_yday=261, tm_isdst=0), 'published': '2023-09-18T11:59:02Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=9, tm_mday=18, tm_hour=11, tm_min=59, tm_sec=2, tm_wday=0, tm_yday=261, tm_isdst=0), 'title': 'Interpretability is not Explainability: New Quantitative XAI Approach\n  with a focus on Recommender Systems in Education', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Interpretability is not Explainability: New Quantitative XAI Approach\n  with a focus on Recommender Systems in Education'}, 'summary': 'The field of eXplainable Artificial Intelligence faces challenges due to the\nabsence of a widely accepted taxonomy that facilitates the quantitative\nevaluation of explainability in Machine Learning algorithms. In this paper, we\npropose a novel taxonomy that addresses the current gap in the literature by\nproviding a clear and unambiguous understanding of the key concepts and\nrelationships in XAI. Our approach is rooted in a systematic analysis of\nexisting definitions and frameworks, with a focus on transparency,\ninterpretability, completeness, complexity and understandability as essential\ndimensions of explainability. This comprehensive taxonomy aims to establish a\nshared vocabulary for future research. To demonstrate the utility of our\nproposed taxonomy, we examine a case study of a Recommender System designed to\ncurate and recommend the most suitable online resources from MERLOT. By\nemploying the SHAP package, we quantify and enhance the explainability of the\nRS within the context of our newly developed taxonomy.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'The field of eXplainable Artificial Intelligence faces challenges due to the\nabsence of a widely accepted taxonomy that facilitates the quantitative\nevaluation of explainability in Machine Learning algorithms. In this paper, we\npropose a novel taxonomy that addresses the current gap in the literature by\nproviding a clear and unambiguous understanding of the key concepts and\nrelationships in XAI. Our approach is rooted in a systematic analysis of\nexisting definitions and frameworks, with a focus on transparency,\ninterpretability, completeness, complexity and understandability as essential\ndimensions of explainability. This comprehensive taxonomy aims to establish a\nshared vocabulary for future research. To demonstrate the utility of our\nproposed taxonomy, we examine a case study of a Recommender System designed to\ncurate and recommend the most suitable online resources from MERLOT. By\nemploying the SHAP package, we quantify and enhance the explainability of the\nRS within the context of our newly developed taxonomy.'}, 'authors': [{'name': 'Riccardo Porcedda'}], 'author_detail': {'name': 'Riccardo Porcedda'}, 'author': 'Riccardo Porcedda', 'arxiv_doi': '10.5121/csit.2023.131612', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.5121/csit.2023.131612', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2311.02078v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.02078v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_journal_ref': 'David C. Wyld et al. (Eds): MLNLP, NWCOM, DTMN, ASOFT, SIGPRO,\n  AIFZ, CSITY, CLSB - 2023, pp. 171-188, 2023. CS & IT - CSCP 2023', 'arxiv_primary_category': {'term': 'cs.IR', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.IR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Riccardo Porcedda,http://arxiv.org/abs/2311.02078v1,2311.02078v1
http://arxiv.org/abs/1807.06722v2,2018-07-25 07:23:45+00:00,2018-07-18 00:50:18+00:00,Machine Learning Interpretability: A Science rather than a tool,"[arxiv.Result.Author('Abdul Karim'), arxiv.Result.Author('Avinash Mishra'), arxiv.Result.Author('MA Hakim Newton'), arxiv.Result.Author('Abdul Sattar')]","The term ""interpretability"" is oftenly used by machine learning researchers
each with their own intuitive understanding of it. There is no universal well
agreed upon definition of interpretability in machine learning. As any type of
science discipline is mainly driven by the set of formulated questions rather
than by different tools in that discipline, e.g. astrophysics is the discipline
that learns the composition of stars, not as the discipline that use the
spectroscopes. Similarly, we propose that machine learning interpretability
should be a discipline that answers specific questions related to
interpretability. These questions can be of statistical, causal and
counterfactual nature. Therefore, there is a need to look into the
interpretability problem of machine learning in the context of questions that
need to be addressed rather than different tools. We discuss about a
hypothetical interpretability framework driven by a question based scientific
approach rather than some specific machine learning model. Using a question
based notion of interpretability, we can step towards understanding the science
of machine learning rather than its engineering. This notion will also help us
understanding any specific problem more in depth rather than relying solely on
machine learning methods.",,,,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/1807.06722v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1807.06722v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1807.06722v2,"{'id': 'http://arxiv.org/abs/1807.06722v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/1807.06722v2', 'updated': '2018-07-25T07:23:45Z', 'updated_parsed': time.struct_time(tm_year=2018, tm_mon=7, tm_mday=25, tm_hour=7, tm_min=23, tm_sec=45, tm_wday=2, tm_yday=206, tm_isdst=0), 'published': '2018-07-18T00:50:18Z', 'published_parsed': time.struct_time(tm_year=2018, tm_mon=7, tm_mday=18, tm_hour=0, tm_min=50, tm_sec=18, tm_wday=2, tm_yday=199, tm_isdst=0), 'title': 'Machine Learning Interpretability: A Science rather than a tool', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Machine Learning Interpretability: A Science rather than a tool'}, 'summary': 'The term ""interpretability"" is oftenly used by machine learning researchers\neach with their own intuitive understanding of it. There is no universal well\nagreed upon definition of interpretability in machine learning. As any type of\nscience discipline is mainly driven by the set of formulated questions rather\nthan by different tools in that discipline, e.g. astrophysics is the discipline\nthat learns the composition of stars, not as the discipline that use the\nspectroscopes. Similarly, we propose that machine learning interpretability\nshould be a discipline that answers specific questions related to\ninterpretability. These questions can be of statistical, causal and\ncounterfactual nature. Therefore, there is a need to look into the\ninterpretability problem of machine learning in the context of questions that\nneed to be addressed rather than different tools. We discuss about a\nhypothetical interpretability framework driven by a question based scientific\napproach rather than some specific machine learning model. Using a question\nbased notion of interpretability, we can step towards understanding the science\nof machine learning rather than its engineering. This notion will also help us\nunderstanding any specific problem more in depth rather than relying solely on\nmachine learning methods.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'The term ""interpretability"" is oftenly used by machine learning researchers\neach with their own intuitive understanding of it. There is no universal well\nagreed upon definition of interpretability in machine learning. As any type of\nscience discipline is mainly driven by the set of formulated questions rather\nthan by different tools in that discipline, e.g. astrophysics is the discipline\nthat learns the composition of stars, not as the discipline that use the\nspectroscopes. Similarly, we propose that machine learning interpretability\nshould be a discipline that answers specific questions related to\ninterpretability. These questions can be of statistical, causal and\ncounterfactual nature. Therefore, there is a need to look into the\ninterpretability problem of machine learning in the context of questions that\nneed to be addressed rather than different tools. We discuss about a\nhypothetical interpretability framework driven by a question based scientific\napproach rather than some specific machine learning model. Using a question\nbased notion of interpretability, we can step towards understanding the science\nof machine learning rather than its engineering. This notion will also help us\nunderstanding any specific problem more in depth rather than relying solely on\nmachine learning methods.'}, 'authors': [{'name': 'Abdul Karim'}, {'name': 'Avinash Mishra'}, {'name': 'MA Hakim Newton'}, {'name': 'Abdul Sattar'}], 'author_detail': {'name': 'Abdul Sattar'}, 'author': 'Abdul Sattar', 'links': [{'href': 'http://arxiv.org/abs/1807.06722v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/1807.06722v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Abdul Karim,http://arxiv.org/abs/1807.06722v2,1807.06722v2
http://arxiv.org/abs/2010.13764v2,2020-10-28 17:34:32+00:00,2020-10-26 17:52:34+00:00,Enforcing Interpretability and its Statistical Impacts: Trade-offs between Accuracy and Interpretability,"[arxiv.Result.Author('Gintare Karolina Dziugaite'), arxiv.Result.Author('Shai Ben-David'), arxiv.Result.Author('Daniel M. Roy')]","To date, there has been no formal study of the statistical cost of
interpretability in machine learning. As such, the discourse around potential
trade-offs is often informal and misconceptions abound. In this work, we aim to
initiate a formal study of these trade-offs. A seemingly insurmountable
roadblock is the lack of any agreed upon definition of interpretability.
Instead, we propose a shift in perspective. Rather than attempt to define
interpretability, we propose to model the \emph{act} of \emph{enforcing}
interpretability. As a starting point, we focus on the setting of empirical
risk minimization for binary classification, and view interpretability as a
constraint placed on learning. That is, we assume we are given a subset of
hypothesis that are deemed to be interpretable, possibly depending on the data
distribution and other aspects of the context. We then model the act of
enforcing interpretability as that of performing empirical risk minimization
over the set of interpretable hypotheses. This model allows us to reason about
the statistical implications of enforcing interpretability, using known results
in statistical learning theory. Focusing on accuracy, we perform a case
analysis, explaining why one may or may not observe a trade-off between
accuracy and interpretability when the restriction to interpretable classifiers
does or does not come at the cost of some excess statistical risk. We close
with some worked examples and some open problems, which we hope will spur
further theoretical development around the tradeoffs involved in
interpretability.",12 pages; minor edits,,,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2010.13764v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2010.13764v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2010.13764v2,"{'id': 'http://arxiv.org/abs/2010.13764v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2010.13764v2', 'updated': '2020-10-28T17:34:32Z', 'updated_parsed': time.struct_time(tm_year=2020, tm_mon=10, tm_mday=28, tm_hour=17, tm_min=34, tm_sec=32, tm_wday=2, tm_yday=302, tm_isdst=0), 'published': '2020-10-26T17:52:34Z', 'published_parsed': time.struct_time(tm_year=2020, tm_mon=10, tm_mday=26, tm_hour=17, tm_min=52, tm_sec=34, tm_wday=0, tm_yday=300, tm_isdst=0), 'title': 'Enforcing Interpretability and its Statistical Impacts: Trade-offs\n  between Accuracy and Interpretability', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Enforcing Interpretability and its Statistical Impacts: Trade-offs\n  between Accuracy and Interpretability'}, 'summary': 'To date, there has been no formal study of the statistical cost of\ninterpretability in machine learning. As such, the discourse around potential\ntrade-offs is often informal and misconceptions abound. In this work, we aim to\ninitiate a formal study of these trade-offs. A seemingly insurmountable\nroadblock is the lack of any agreed upon definition of interpretability.\nInstead, we propose a shift in perspective. Rather than attempt to define\ninterpretability, we propose to model the \\emph{act} of \\emph{enforcing}\ninterpretability. As a starting point, we focus on the setting of empirical\nrisk minimization for binary classification, and view interpretability as a\nconstraint placed on learning. That is, we assume we are given a subset of\nhypothesis that are deemed to be interpretable, possibly depending on the data\ndistribution and other aspects of the context. We then model the act of\nenforcing interpretability as that of performing empirical risk minimization\nover the set of interpretable hypotheses. This model allows us to reason about\nthe statistical implications of enforcing interpretability, using known results\nin statistical learning theory. Focusing on accuracy, we perform a case\nanalysis, explaining why one may or may not observe a trade-off between\naccuracy and interpretability when the restriction to interpretable classifiers\ndoes or does not come at the cost of some excess statistical risk. We close\nwith some worked examples and some open problems, which we hope will spur\nfurther theoretical development around the tradeoffs involved in\ninterpretability.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'To date, there has been no formal study of the statistical cost of\ninterpretability in machine learning. As such, the discourse around potential\ntrade-offs is often informal and misconceptions abound. In this work, we aim to\ninitiate a formal study of these trade-offs. A seemingly insurmountable\nroadblock is the lack of any agreed upon definition of interpretability.\nInstead, we propose a shift in perspective. Rather than attempt to define\ninterpretability, we propose to model the \\emph{act} of \\emph{enforcing}\ninterpretability. As a starting point, we focus on the setting of empirical\nrisk minimization for binary classification, and view interpretability as a\nconstraint placed on learning. That is, we assume we are given a subset of\nhypothesis that are deemed to be interpretable, possibly depending on the data\ndistribution and other aspects of the context. We then model the act of\nenforcing interpretability as that of performing empirical risk minimization\nover the set of interpretable hypotheses. This model allows us to reason about\nthe statistical implications of enforcing interpretability, using known results\nin statistical learning theory. Focusing on accuracy, we perform a case\nanalysis, explaining why one may or may not observe a trade-off between\naccuracy and interpretability when the restriction to interpretable classifiers\ndoes or does not come at the cost of some excess statistical risk. We close\nwith some worked examples and some open problems, which we hope will spur\nfurther theoretical development around the tradeoffs involved in\ninterpretability.'}, 'authors': [{'name': 'Gintare Karolina Dziugaite'}, {'name': 'Shai Ben-David'}, {'name': 'Daniel M. Roy'}], 'author_detail': {'name': 'Daniel M. Roy'}, 'author': 'Daniel M. Roy', 'arxiv_comment': '12 pages; minor edits', 'links': [{'href': 'http://arxiv.org/abs/2010.13764v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2010.13764v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Gintare Karolina Dziugaite,http://arxiv.org/abs/2010.13764v2,2010.13764v2
http://arxiv.org/abs/2103.11251v2,2021-07-10 01:20:27+00:00,2021-03-20 21:58:27+00:00,Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges,"[arxiv.Result.Author('Cynthia Rudin'), arxiv.Result.Author('Chaofan Chen'), arxiv.Result.Author('Zhi Chen'), arxiv.Result.Author('Haiyang Huang'), arxiv.Result.Author('Lesia Semenova'), arxiv.Result.Author('Chudi Zhong')]","Interpretability in machine learning (ML) is crucial for high stakes
decisions and troubleshooting. In this work, we provide fundamental principles
for interpretable ML, and dispel common misunderstandings that dilute the
importance of this crucial topic. We also identify 10 technical challenge areas
in interpretable machine learning and provide history and background on each
problem. Some of these problems are classically important, and some are recent
problems that have arisen in the last few years. These problems are: (1)
Optimizing sparse logical models such as decision trees; (2) Optimization of
scoring systems; (3) Placing constraints into generalized additive models to
encourage sparsity and better interpretability; (4) Modern case-based
reasoning, including neural networks and matching for causal inference; (5)
Complete supervised disentanglement of neural networks; (6) Complete or even
partial unsupervised disentanglement of neural networks; (7) Dimensionality
reduction for data visualization; (8) Machine learning models that can
incorporate physics and other generative or causal constraints; (9)
Characterization of the ""Rashomon set"" of good models; and (10) Interpretable
reinforcement learning. This survey is suitable as a starting point for
statisticians and computer scientists interested in working in interpretable
machine learning.",,"Statistics Surveys, 2021",,cs.LG,"['cs.LG', 'stat.ML', '68T01', 'I.2.6']","[arxiv.Result.Link('http://arxiv.org/abs/2103.11251v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2103.11251v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2103.11251v2,"{'id': 'http://arxiv.org/abs/2103.11251v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2103.11251v2', 'updated': '2021-07-10T01:20:27Z', 'updated_parsed': time.struct_time(tm_year=2021, tm_mon=7, tm_mday=10, tm_hour=1, tm_min=20, tm_sec=27, tm_wday=5, tm_yday=191, tm_isdst=0), 'published': '2021-03-20T21:58:27Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=3, tm_mday=20, tm_hour=21, tm_min=58, tm_sec=27, tm_wday=5, tm_yday=79, tm_isdst=0), 'title': 'Interpretable Machine Learning: Fundamental Principles and 10 Grand\n  Challenges', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Interpretable Machine Learning: Fundamental Principles and 10 Grand\n  Challenges'}, 'summary': 'Interpretability in machine learning (ML) is crucial for high stakes\ndecisions and troubleshooting. In this work, we provide fundamental principles\nfor interpretable ML, and dispel common misunderstandings that dilute the\nimportance of this crucial topic. We also identify 10 technical challenge areas\nin interpretable machine learning and provide history and background on each\nproblem. Some of these problems are classically important, and some are recent\nproblems that have arisen in the last few years. These problems are: (1)\nOptimizing sparse logical models such as decision trees; (2) Optimization of\nscoring systems; (3) Placing constraints into generalized additive models to\nencourage sparsity and better interpretability; (4) Modern case-based\nreasoning, including neural networks and matching for causal inference; (5)\nComplete supervised disentanglement of neural networks; (6) Complete or even\npartial unsupervised disentanglement of neural networks; (7) Dimensionality\nreduction for data visualization; (8) Machine learning models that can\nincorporate physics and other generative or causal constraints; (9)\nCharacterization of the ""Rashomon set"" of good models; and (10) Interpretable\nreinforcement learning. This survey is suitable as a starting point for\nstatisticians and computer scientists interested in working in interpretable\nmachine learning.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Interpretability in machine learning (ML) is crucial for high stakes\ndecisions and troubleshooting. In this work, we provide fundamental principles\nfor interpretable ML, and dispel common misunderstandings that dilute the\nimportance of this crucial topic. We also identify 10 technical challenge areas\nin interpretable machine learning and provide history and background on each\nproblem. Some of these problems are classically important, and some are recent\nproblems that have arisen in the last few years. These problems are: (1)\nOptimizing sparse logical models such as decision trees; (2) Optimization of\nscoring systems; (3) Placing constraints into generalized additive models to\nencourage sparsity and better interpretability; (4) Modern case-based\nreasoning, including neural networks and matching for causal inference; (5)\nComplete supervised disentanglement of neural networks; (6) Complete or even\npartial unsupervised disentanglement of neural networks; (7) Dimensionality\nreduction for data visualization; (8) Machine learning models that can\nincorporate physics and other generative or causal constraints; (9)\nCharacterization of the ""Rashomon set"" of good models; and (10) Interpretable\nreinforcement learning. This survey is suitable as a starting point for\nstatisticians and computer scientists interested in working in interpretable\nmachine learning.'}, 'authors': [{'name': 'Cynthia Rudin'}, {'name': 'Chaofan Chen'}, {'name': 'Zhi Chen'}, {'name': 'Haiyang Huang'}, {'name': 'Lesia Semenova'}, {'name': 'Chudi Zhong'}], 'author_detail': {'name': 'Chudi Zhong'}, 'author': 'Chudi Zhong', 'arxiv_journal_ref': 'Statistics Surveys, 2021', 'links': [{'href': 'http://arxiv.org/abs/2103.11251v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2103.11251v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': '68T01', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.2.6', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Cynthia Rudin,http://arxiv.org/abs/2103.11251v2,2103.11251v2
http://arxiv.org/abs/2305.19837v1,2023-05-31 13:25:26+00:00,2023-05-31 13:25:26+00:00,EAMDrift: An interpretable self retrain model for time series,"[arxiv.Result.Author('Gonçalo Mateus'), arxiv.Result.Author('Cláudia Soares'), arxiv.Result.Author('João Leitão'), arxiv.Result.Author('António Rodrigues')]","The use of machine learning for time series prediction has become
increasingly popular across various industries thanks to the availability of
time series data and advancements in machine learning algorithms. However,
traditional methods for time series forecasting rely on pre-optimized models
that are ill-equipped to handle unpredictable patterns in data. In this paper,
we present EAMDrift, a novel method that combines forecasts from multiple
individual predictors by weighting each prediction according to a performance
metric. EAMDrift is designed to automatically adapt to out-of-distribution
patterns in data and identify the most appropriate models to use at each moment
through interpretable mechanisms, which include an automatic retraining
process. Specifically, we encode different concepts with different models, each
functioning as an observer of specific behaviors. The activation of the overall
model then identifies which subset of the concept observers is identifying
concepts in the data. This activation is interpretable and based on learned
rules, allowing to study of input variables relations. Our study on real-world
datasets shows that EAMDrift outperforms individual baseline models by 20% and
achieves comparable accuracy results to non-interpretable ensemble models.
These findings demonstrate the efficacy of EAMDrift for time-series prediction
and highlight the importance of interpretability in machine learning models.",Submitted to ECML PKDD 2023,,,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2305.19837v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.19837v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.19837v1,"{'id': 'http://arxiv.org/abs/2305.19837v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.19837v1', 'updated': '2023-05-31T13:25:26Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=31, tm_hour=13, tm_min=25, tm_sec=26, tm_wday=2, tm_yday=151, tm_isdst=0), 'published': '2023-05-31T13:25:26Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=31, tm_hour=13, tm_min=25, tm_sec=26, tm_wday=2, tm_yday=151, tm_isdst=0), 'title': 'EAMDrift: An interpretable self retrain model for time series', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'EAMDrift: An interpretable self retrain model for time series'}, 'summary': 'The use of machine learning for time series prediction has become\nincreasingly popular across various industries thanks to the availability of\ntime series data and advancements in machine learning algorithms. However,\ntraditional methods for time series forecasting rely on pre-optimized models\nthat are ill-equipped to handle unpredictable patterns in data. In this paper,\nwe present EAMDrift, a novel method that combines forecasts from multiple\nindividual predictors by weighting each prediction according to a performance\nmetric. EAMDrift is designed to automatically adapt to out-of-distribution\npatterns in data and identify the most appropriate models to use at each moment\nthrough interpretable mechanisms, which include an automatic retraining\nprocess. Specifically, we encode different concepts with different models, each\nfunctioning as an observer of specific behaviors. The activation of the overall\nmodel then identifies which subset of the concept observers is identifying\nconcepts in the data. This activation is interpretable and based on learned\nrules, allowing to study of input variables relations. Our study on real-world\ndatasets shows that EAMDrift outperforms individual baseline models by 20% and\nachieves comparable accuracy results to non-interpretable ensemble models.\nThese findings demonstrate the efficacy of EAMDrift for time-series prediction\nand highlight the importance of interpretability in machine learning models.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'The use of machine learning for time series prediction has become\nincreasingly popular across various industries thanks to the availability of\ntime series data and advancements in machine learning algorithms. However,\ntraditional methods for time series forecasting rely on pre-optimized models\nthat are ill-equipped to handle unpredictable patterns in data. In this paper,\nwe present EAMDrift, a novel method that combines forecasts from multiple\nindividual predictors by weighting each prediction according to a performance\nmetric. EAMDrift is designed to automatically adapt to out-of-distribution\npatterns in data and identify the most appropriate models to use at each moment\nthrough interpretable mechanisms, which include an automatic retraining\nprocess. Specifically, we encode different concepts with different models, each\nfunctioning as an observer of specific behaviors. The activation of the overall\nmodel then identifies which subset of the concept observers is identifying\nconcepts in the data. This activation is interpretable and based on learned\nrules, allowing to study of input variables relations. Our study on real-world\ndatasets shows that EAMDrift outperforms individual baseline models by 20% and\nachieves comparable accuracy results to non-interpretable ensemble models.\nThese findings demonstrate the efficacy of EAMDrift for time-series prediction\nand highlight the importance of interpretability in machine learning models.'}, 'authors': [{'name': 'Gonçalo Mateus'}, {'name': 'Cláudia Soares'}, {'name': 'João Leitão'}, {'name': 'António Rodrigues'}], 'author_detail': {'name': 'António Rodrigues'}, 'author': 'António Rodrigues', 'arxiv_comment': 'Submitted to ECML PKDD 2023', 'links': [{'href': 'http://arxiv.org/abs/2305.19837v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.19837v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Gonçalo Mateus,http://arxiv.org/abs/2305.19837v1,2305.19837v1
http://arxiv.org/abs/2307.08485v1,2023-07-17 13:47:41+00:00,2023-07-17 13:47:41+00:00,Cross Feature Selection to Eliminate Spurious Interactions and Single Feature Dominance Explainable Boosting Machines,"[arxiv.Result.Author('Shree Charran R'), arxiv.Result.Author('Sandipan Das Mahapatra')]","Interpretability is a crucial aspect of machine learning models that enables
humans to understand and trust the decision-making process of these models. In
many real-world applications, the interpretability of models is essential for
legal, ethical, and practical reasons. For instance, in the banking domain,
interpretability is critical for lenders and borrowers to understand the
reasoning behind the acceptance or rejection of loan applications as per fair
lending laws. However, achieving interpretability in machine learning models is
challenging, especially for complex high-performance models. Hence Explainable
Boosting Machines (EBMs) have been gaining popularity due to their
interpretable and high-performance nature in various prediction tasks. However,
these models can suffer from issues such as spurious interactions with
redundant features and single-feature dominance across all interactions, which
can affect the interpretability and reliability of the model's predictions. In
this paper, we explore novel approaches to address these issues by utilizing
alternate Cross-feature selection, ensemble features and model configuration
alteration techniques. Our approach involves a multi-step feature selection
procedure that selects a set of candidate features, ensemble features and then
benchmark the same using the EBM model. We evaluate our method on three
benchmark datasets and show that the alternate techniques outperform vanilla
EBM methods, while providing better interpretability and feature selection
stability, and improving the model's predictive performance. Moreover, we show
that our approach can identify meaningful interactions and reduce the dominance
of single features in the model's predictions, leading to more reliable and
interpretable models.
  Index Terms- Interpretability, EBM's, ensemble, feature selection.",,,,stat.ML,"['stat.ML', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2307.08485v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2307.08485v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2307.08485v1,"{'id': 'http://arxiv.org/abs/2307.08485v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2307.08485v1', 'updated': '2023-07-17T13:47:41Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=17, tm_hour=13, tm_min=47, tm_sec=41, tm_wday=0, tm_yday=198, tm_isdst=0), 'published': '2023-07-17T13:47:41Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=17, tm_hour=13, tm_min=47, tm_sec=41, tm_wday=0, tm_yday=198, tm_isdst=0), 'title': 'Cross Feature Selection to Eliminate Spurious Interactions and Single\n  Feature Dominance Explainable Boosting Machines', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Cross Feature Selection to Eliminate Spurious Interactions and Single\n  Feature Dominance Explainable Boosting Machines'}, 'summary': ""Interpretability is a crucial aspect of machine learning models that enables\nhumans to understand and trust the decision-making process of these models. In\nmany real-world applications, the interpretability of models is essential for\nlegal, ethical, and practical reasons. For instance, in the banking domain,\ninterpretability is critical for lenders and borrowers to understand the\nreasoning behind the acceptance or rejection of loan applications as per fair\nlending laws. However, achieving interpretability in machine learning models is\nchallenging, especially for complex high-performance models. Hence Explainable\nBoosting Machines (EBMs) have been gaining popularity due to their\ninterpretable and high-performance nature in various prediction tasks. However,\nthese models can suffer from issues such as spurious interactions with\nredundant features and single-feature dominance across all interactions, which\ncan affect the interpretability and reliability of the model's predictions. In\nthis paper, we explore novel approaches to address these issues by utilizing\nalternate Cross-feature selection, ensemble features and model configuration\nalteration techniques. Our approach involves a multi-step feature selection\nprocedure that selects a set of candidate features, ensemble features and then\nbenchmark the same using the EBM model. We evaluate our method on three\nbenchmark datasets and show that the alternate techniques outperform vanilla\nEBM methods, while providing better interpretability and feature selection\nstability, and improving the model's predictive performance. Moreover, we show\nthat our approach can identify meaningful interactions and reduce the dominance\nof single features in the model's predictions, leading to more reliable and\ninterpretable models.\n  Index Terms- Interpretability, EBM's, ensemble, feature selection."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""Interpretability is a crucial aspect of machine learning models that enables\nhumans to understand and trust the decision-making process of these models. In\nmany real-world applications, the interpretability of models is essential for\nlegal, ethical, and practical reasons. For instance, in the banking domain,\ninterpretability is critical for lenders and borrowers to understand the\nreasoning behind the acceptance or rejection of loan applications as per fair\nlending laws. However, achieving interpretability in machine learning models is\nchallenging, especially for complex high-performance models. Hence Explainable\nBoosting Machines (EBMs) have been gaining popularity due to their\ninterpretable and high-performance nature in various prediction tasks. However,\nthese models can suffer from issues such as spurious interactions with\nredundant features and single-feature dominance across all interactions, which\ncan affect the interpretability and reliability of the model's predictions. In\nthis paper, we explore novel approaches to address these issues by utilizing\nalternate Cross-feature selection, ensemble features and model configuration\nalteration techniques. Our approach involves a multi-step feature selection\nprocedure that selects a set of candidate features, ensemble features and then\nbenchmark the same using the EBM model. We evaluate our method on three\nbenchmark datasets and show that the alternate techniques outperform vanilla\nEBM methods, while providing better interpretability and feature selection\nstability, and improving the model's predictive performance. Moreover, we show\nthat our approach can identify meaningful interactions and reduce the dominance\nof single features in the model's predictions, leading to more reliable and\ninterpretable models.\n  Index Terms- Interpretability, EBM's, ensemble, feature selection.""}, 'authors': [{'name': 'Shree Charran R'}, {'name': 'Sandipan Das Mahapatra'}], 'author_detail': {'name': 'Sandipan Das Mahapatra'}, 'author': 'Sandipan Das Mahapatra', 'links': [{'href': 'http://arxiv.org/abs/2307.08485v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2307.08485v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Shree Charran R,http://arxiv.org/abs/2307.08485v1,2307.08485v1
http://arxiv.org/abs/1901.01963v5,2020-07-28 03:06:36+00:00,2019-01-07 18:37:07+00:00,Machine learning topological phases in real space,"[arxiv.Result.Author('N. L. Holanda'), arxiv.Result.Author('M. A. R. Griffith')]","We develop a supervised machine learning algorithm that is able to learn
topological phases of finite condensed matter systems from bulk data in real
lattice space. The algorithm employs diagonalization in real space together
with any supervised learning algorithm to learn topological phases through an
eigenvector ensembling procedure. We combine our algorithm with decision trees
and random forests to successfully recover topological phase diagrams of
Su-Schrieffer-Heeger (SSH) models from bulk lattice data in real space and show
how the Shannon information entropy of ensembles of lattice eigenvectors can be
used to retrieve a signal detailing how topological information is distributed
in the bulk. We further use insights obtained from these information entropy
signatures to engineer global topological features from real space lattice data
that still carry most of the topological information in the lattice, while
greatly diminishing the size of feature space, thus effectively amounting to a
topological lattice compression. Finally, we explore the theoretical
possibility of interpreting the information entropy topological signatures in
terms of emergent information entropy wave functions, which lead us to
Heisenberg and Hirschman uncertainty relations for topological phase
transitions. The discovery of Shannon information entropy signals associated
with topological phase transitions from the analysis of data from several
thousand SSH systems illustrates how model explainability in machine learning
can advance the research of exotic quantum materials with properties that may
power future technological applications such as qubit engineering for quantum
computing.",,"Phys. Rev. B 102, 054107 (2020)",10.1103/PhysRevB.102.054107,cond-mat.mes-hall,"['cond-mat.mes-hall', 'cs.LG']","[arxiv.Result.Link('http://dx.doi.org/10.1103/PhysRevB.102.054107', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1901.01963v5', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1901.01963v5', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/1901.01963v5,"{'id': 'http://arxiv.org/abs/1901.01963v5', 'guidislink': True, 'link': 'http://arxiv.org/abs/1901.01963v5', 'updated': '2020-07-28T03:06:36Z', 'updated_parsed': time.struct_time(tm_year=2020, tm_mon=7, tm_mday=28, tm_hour=3, tm_min=6, tm_sec=36, tm_wday=1, tm_yday=210, tm_isdst=0), 'published': '2019-01-07T18:37:07Z', 'published_parsed': time.struct_time(tm_year=2019, tm_mon=1, tm_mday=7, tm_hour=18, tm_min=37, tm_sec=7, tm_wday=0, tm_yday=7, tm_isdst=0), 'title': 'Machine learning topological phases in real space', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Machine learning topological phases in real space'}, 'summary': 'We develop a supervised machine learning algorithm that is able to learn\ntopological phases of finite condensed matter systems from bulk data in real\nlattice space. The algorithm employs diagonalization in real space together\nwith any supervised learning algorithm to learn topological phases through an\neigenvector ensembling procedure. We combine our algorithm with decision trees\nand random forests to successfully recover topological phase diagrams of\nSu-Schrieffer-Heeger (SSH) models from bulk lattice data in real space and show\nhow the Shannon information entropy of ensembles of lattice eigenvectors can be\nused to retrieve a signal detailing how topological information is distributed\nin the bulk. We further use insights obtained from these information entropy\nsignatures to engineer global topological features from real space lattice data\nthat still carry most of the topological information in the lattice, while\ngreatly diminishing the size of feature space, thus effectively amounting to a\ntopological lattice compression. Finally, we explore the theoretical\npossibility of interpreting the information entropy topological signatures in\nterms of emergent information entropy wave functions, which lead us to\nHeisenberg and Hirschman uncertainty relations for topological phase\ntransitions. The discovery of Shannon information entropy signals associated\nwith topological phase transitions from the analysis of data from several\nthousand SSH systems illustrates how model explainability in machine learning\ncan advance the research of exotic quantum materials with properties that may\npower future technological applications such as qubit engineering for quantum\ncomputing.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'We develop a supervised machine learning algorithm that is able to learn\ntopological phases of finite condensed matter systems from bulk data in real\nlattice space. The algorithm employs diagonalization in real space together\nwith any supervised learning algorithm to learn topological phases through an\neigenvector ensembling procedure. We combine our algorithm with decision trees\nand random forests to successfully recover topological phase diagrams of\nSu-Schrieffer-Heeger (SSH) models from bulk lattice data in real space and show\nhow the Shannon information entropy of ensembles of lattice eigenvectors can be\nused to retrieve a signal detailing how topological information is distributed\nin the bulk. We further use insights obtained from these information entropy\nsignatures to engineer global topological features from real space lattice data\nthat still carry most of the topological information in the lattice, while\ngreatly diminishing the size of feature space, thus effectively amounting to a\ntopological lattice compression. Finally, we explore the theoretical\npossibility of interpreting the information entropy topological signatures in\nterms of emergent information entropy wave functions, which lead us to\nHeisenberg and Hirschman uncertainty relations for topological phase\ntransitions. The discovery of Shannon information entropy signals associated\nwith topological phase transitions from the analysis of data from several\nthousand SSH systems illustrates how model explainability in machine learning\ncan advance the research of exotic quantum materials with properties that may\npower future technological applications such as qubit engineering for quantum\ncomputing.'}, 'authors': [{'name': 'N. L. Holanda'}, {'name': 'M. A. R. Griffith'}], 'author_detail': {'name': 'M. A. R. Griffith'}, 'author': 'M. A. R. Griffith', 'arxiv_doi': '10.1103/PhysRevB.102.054107', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.1103/PhysRevB.102.054107', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/1901.01963v5', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/1901.01963v5', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_journal_ref': 'Phys. Rev. B 102, 054107 (2020)', 'arxiv_primary_category': {'term': 'cond-mat.mes-hall', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cond-mat.mes-hall', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",N. L. Holanda,http://arxiv.org/abs/1901.01963v5,1901.01963v5
http://arxiv.org/abs/2007.03832v1,2020-07-08 00:35:39+00:00,2020-07-08 00:35:39+00:00,Fast Training of Deep Neural Networks Robust to Adversarial Perturbations,"[arxiv.Result.Author('Justin Goodwin'), arxiv.Result.Author('Olivia Brown'), arxiv.Result.Author('Victoria Helus')]","Deep neural networks are capable of training fast and generalizing well
within many domains. Despite their promising performance, deep networks have
shown sensitivities to perturbations of their inputs (e.g., adversarial
examples) and their learned feature representations are often difficult to
interpret, raising concerns about their true capability and trustworthiness.
Recent work in adversarial training, a form of robust optimization in which the
model is optimized against adversarial examples, demonstrates the ability to
improve performance sensitivities to perturbations and yield feature
representations that are more interpretable. Adversarial training, however,
comes with an increased computational cost over that of standard (i.e.,
nonrobust) training, rendering it impractical for use in large-scale problems.
Recent work suggests that a fast approximation to adversarial training shows
promise for reducing training time and maintaining robustness in the presence
of perturbations bounded by the infinity norm. In this work, we demonstrate
that this approach extends to the Euclidean norm and preserves the
human-aligned feature representations that are common for robust models.
Additionally, we show that using a distributed training scheme can further
reduce the time to train robust deep networks. Fast adversarial training is a
promising approach that will provide increased security and explainability in
machine learning applications for which robust optimization was previously
thought to be impractical.",,,,cs.LG,"['cs.LG', 'stat.ML']","[arxiv.Result.Link('http://arxiv.org/abs/2007.03832v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.03832v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2007.03832v1,"{'id': 'http://arxiv.org/abs/2007.03832v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2007.03832v1', 'updated': '2020-07-08T00:35:39Z', 'updated_parsed': time.struct_time(tm_year=2020, tm_mon=7, tm_mday=8, tm_hour=0, tm_min=35, tm_sec=39, tm_wday=2, tm_yday=190, tm_isdst=0), 'published': '2020-07-08T00:35:39Z', 'published_parsed': time.struct_time(tm_year=2020, tm_mon=7, tm_mday=8, tm_hour=0, tm_min=35, tm_sec=39, tm_wday=2, tm_yday=190, tm_isdst=0), 'title': 'Fast Training of Deep Neural Networks Robust to Adversarial\n  Perturbations', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Fast Training of Deep Neural Networks Robust to Adversarial\n  Perturbations'}, 'summary': 'Deep neural networks are capable of training fast and generalizing well\nwithin many domains. Despite their promising performance, deep networks have\nshown sensitivities to perturbations of their inputs (e.g., adversarial\nexamples) and their learned feature representations are often difficult to\ninterpret, raising concerns about their true capability and trustworthiness.\nRecent work in adversarial training, a form of robust optimization in which the\nmodel is optimized against adversarial examples, demonstrates the ability to\nimprove performance sensitivities to perturbations and yield feature\nrepresentations that are more interpretable. Adversarial training, however,\ncomes with an increased computational cost over that of standard (i.e.,\nnonrobust) training, rendering it impractical for use in large-scale problems.\nRecent work suggests that a fast approximation to adversarial training shows\npromise for reducing training time and maintaining robustness in the presence\nof perturbations bounded by the infinity norm. In this work, we demonstrate\nthat this approach extends to the Euclidean norm and preserves the\nhuman-aligned feature representations that are common for robust models.\nAdditionally, we show that using a distributed training scheme can further\nreduce the time to train robust deep networks. Fast adversarial training is a\npromising approach that will provide increased security and explainability in\nmachine learning applications for which robust optimization was previously\nthought to be impractical.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Deep neural networks are capable of training fast and generalizing well\nwithin many domains. Despite their promising performance, deep networks have\nshown sensitivities to perturbations of their inputs (e.g., adversarial\nexamples) and their learned feature representations are often difficult to\ninterpret, raising concerns about their true capability and trustworthiness.\nRecent work in adversarial training, a form of robust optimization in which the\nmodel is optimized against adversarial examples, demonstrates the ability to\nimprove performance sensitivities to perturbations and yield feature\nrepresentations that are more interpretable. Adversarial training, however,\ncomes with an increased computational cost over that of standard (i.e.,\nnonrobust) training, rendering it impractical for use in large-scale problems.\nRecent work suggests that a fast approximation to adversarial training shows\npromise for reducing training time and maintaining robustness in the presence\nof perturbations bounded by the infinity norm. In this work, we demonstrate\nthat this approach extends to the Euclidean norm and preserves the\nhuman-aligned feature representations that are common for robust models.\nAdditionally, we show that using a distributed training scheme can further\nreduce the time to train robust deep networks. Fast adversarial training is a\npromising approach that will provide increased security and explainability in\nmachine learning applications for which robust optimization was previously\nthought to be impractical.'}, 'authors': [{'name': 'Justin Goodwin'}, {'name': 'Olivia Brown'}, {'name': 'Victoria Helus'}], 'author_detail': {'name': 'Victoria Helus'}, 'author': 'Victoria Helus', 'links': [{'href': 'http://arxiv.org/abs/2007.03832v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2007.03832v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Justin Goodwin,http://arxiv.org/abs/2007.03832v1,2007.03832v1
http://arxiv.org/abs/2005.10856v2,2020-05-30 10:14:58+00:00,2020-05-21 18:49:45+00:00,Hyperspectral Unmixing Network Inspired by Unfolding an Optimization Problem,[arxiv.Result.Author('Chao Zhou')],"The hyperspectral image (HSI) unmixing task is essentially an inverse
problem, which is commonly solved by optimization algorithms under a predefined
(non-)linear mixture model. Although these optimization algorithms show
impressive performance, they are very computational demanding as they often
rely on an iterative updating scheme. Recently, the rise of neural networks has
inspired lots of learning based algorithms in unmixing literature. However,
most of them lack of interpretability and require a large training dataset. One
natural question then arises: can one leverage the model based algorithm and
learning based algorithm to achieve interpretable and fast algorithm for HSI
unmixing problem? In this paper, we propose two novel network architectures,
named U-ADMM-AENet and U-ADMM-BUNet, for abundance estimation and blind
unmixing respectively, by combining the conventional optimization-model based
unmixing method and the rising learning based unmixing method. We first
consider a linear mixture model with sparsity constraint, then we unfold
Alternating Direction Method of Multipliers (ADMM) algorithm to construct the
unmixing network structures. We also show that the unfolded structures can find
corresponding interpretations in machine learning literature, which further
demonstrates the effectiveness of proposed methods. Benefit from the
interpretation, the proposed networks can be initialized by incorporating prior
information about the HSI data. Different from traditional unfolding networks,
we propose a new training strategy for proposed networks to better fit in the
HSI applications. Extensive experiments show that the proposed methods can
achieve much faster convergence and competitive performance even with very
small size of training data, when compared with state-of-art algorithms.",mis-uploading. It is a paper still under construciton,,,eess.IV,"['eess.IV', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2005.10856v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2005.10856v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2005.10856v2,"{'id': 'http://arxiv.org/abs/2005.10856v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2005.10856v2', 'updated': '2020-05-30T10:14:58Z', 'updated_parsed': time.struct_time(tm_year=2020, tm_mon=5, tm_mday=30, tm_hour=10, tm_min=14, tm_sec=58, tm_wday=5, tm_yday=151, tm_isdst=0), 'published': '2020-05-21T18:49:45Z', 'published_parsed': time.struct_time(tm_year=2020, tm_mon=5, tm_mday=21, tm_hour=18, tm_min=49, tm_sec=45, tm_wday=3, tm_yday=142, tm_isdst=0), 'title': 'Hyperspectral Unmixing Network Inspired by Unfolding an Optimization\n  Problem', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Hyperspectral Unmixing Network Inspired by Unfolding an Optimization\n  Problem'}, 'summary': 'The hyperspectral image (HSI) unmixing task is essentially an inverse\nproblem, which is commonly solved by optimization algorithms under a predefined\n(non-)linear mixture model. Although these optimization algorithms show\nimpressive performance, they are very computational demanding as they often\nrely on an iterative updating scheme. Recently, the rise of neural networks has\ninspired lots of learning based algorithms in unmixing literature. However,\nmost of them lack of interpretability and require a large training dataset. One\nnatural question then arises: can one leverage the model based algorithm and\nlearning based algorithm to achieve interpretable and fast algorithm for HSI\nunmixing problem? In this paper, we propose two novel network architectures,\nnamed U-ADMM-AENet and U-ADMM-BUNet, for abundance estimation and blind\nunmixing respectively, by combining the conventional optimization-model based\nunmixing method and the rising learning based unmixing method. We first\nconsider a linear mixture model with sparsity constraint, then we unfold\nAlternating Direction Method of Multipliers (ADMM) algorithm to construct the\nunmixing network structures. We also show that the unfolded structures can find\ncorresponding interpretations in machine learning literature, which further\ndemonstrates the effectiveness of proposed methods. Benefit from the\ninterpretation, the proposed networks can be initialized by incorporating prior\ninformation about the HSI data. Different from traditional unfolding networks,\nwe propose a new training strategy for proposed networks to better fit in the\nHSI applications. Extensive experiments show that the proposed methods can\nachieve much faster convergence and competitive performance even with very\nsmall size of training data, when compared with state-of-art algorithms.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'The hyperspectral image (HSI) unmixing task is essentially an inverse\nproblem, which is commonly solved by optimization algorithms under a predefined\n(non-)linear mixture model. Although these optimization algorithms show\nimpressive performance, they are very computational demanding as they often\nrely on an iterative updating scheme. Recently, the rise of neural networks has\ninspired lots of learning based algorithms in unmixing literature. However,\nmost of them lack of interpretability and require a large training dataset. One\nnatural question then arises: can one leverage the model based algorithm and\nlearning based algorithm to achieve interpretable and fast algorithm for HSI\nunmixing problem? In this paper, we propose two novel network architectures,\nnamed U-ADMM-AENet and U-ADMM-BUNet, for abundance estimation and blind\nunmixing respectively, by combining the conventional optimization-model based\nunmixing method and the rising learning based unmixing method. We first\nconsider a linear mixture model with sparsity constraint, then we unfold\nAlternating Direction Method of Multipliers (ADMM) algorithm to construct the\nunmixing network structures. We also show that the unfolded structures can find\ncorresponding interpretations in machine learning literature, which further\ndemonstrates the effectiveness of proposed methods. Benefit from the\ninterpretation, the proposed networks can be initialized by incorporating prior\ninformation about the HSI data. Different from traditional unfolding networks,\nwe propose a new training strategy for proposed networks to better fit in the\nHSI applications. Extensive experiments show that the proposed methods can\nachieve much faster convergence and competitive performance even with very\nsmall size of training data, when compared with state-of-art algorithms.'}, 'authors': [{'name': 'Chao Zhou'}], 'author_detail': {'name': 'Chao Zhou'}, 'author': 'Chao Zhou', 'arxiv_comment': 'mis-uploading. It is a paper still under construciton', 'links': [{'href': 'http://arxiv.org/abs/2005.10856v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2005.10856v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'eess.IV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Chao Zhou,http://arxiv.org/abs/2005.10856v2,2005.10856v2
http://arxiv.org/abs/2208.05126v1,2022-08-10 03:41:48+00:00,2022-08-10 03:41:48+00:00,D-BIAS: A Causality-Based Human-in-the-Loop System for Tackling Algorithmic Bias,"[arxiv.Result.Author('Bhavya Ghai'), arxiv.Result.Author('Klaus Mueller')]","With the rise of AI, algorithms have become better at learning underlying
patterns from the training data including ingrained social biases based on
gender, race, etc. Deployment of such algorithms to domains such as hiring,
healthcare, law enforcement, etc. has raised serious concerns about fairness,
accountability, trust and interpretability in machine learning algorithms. To
alleviate this problem, we propose D-BIAS, a visual interactive tool that
embodies human-in-the-loop AI approach for auditing and mitigating social
biases from tabular datasets. It uses a graphical causal model to represent
causal relationships among different features in the dataset and as a medium to
inject domain knowledge. A user can detect the presence of bias against a
group, say females, or a subgroup, say black females, by identifying unfair
causal relationships in the causal network and using an array of fairness
metrics. Thereafter, the user can mitigate bias by acting on the unfair causal
edges. For each interaction, say weakening/deleting a biased causal edge, the
system uses a novel method to simulate a new (debiased) dataset based on the
current causal model. Users can visually assess the impact of their
interactions on different fairness metrics, utility metrics, data distortion,
and the underlying data distribution. Once satisfied, they can download the
debiased dataset and use it for any downstream application for fairer
predictions. We evaluate D-BIAS by conducting experiments on 3 datasets and
also a formal user study. We found that D-BIAS helps reduce bias significantly
compared to the baseline debiasing approach across different fairness metrics
while incurring little data distortion and a small loss in utility. Moreover,
our human-in-the-loop based approach significantly outperforms an automated
approach on trust, interpretability and accountability.",Accepted to IEEE VIS Conference 2022,,,cs.LG,"['cs.LG', 'cs.CY', 'cs.HC', 'stat.ME']","[arxiv.Result.Link('http://arxiv.org/abs/2208.05126v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2208.05126v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2208.05126v1,"{'id': 'http://arxiv.org/abs/2208.05126v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2208.05126v1', 'updated': '2022-08-10T03:41:48Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=8, tm_mday=10, tm_hour=3, tm_min=41, tm_sec=48, tm_wday=2, tm_yday=222, tm_isdst=0), 'published': '2022-08-10T03:41:48Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=8, tm_mday=10, tm_hour=3, tm_min=41, tm_sec=48, tm_wday=2, tm_yday=222, tm_isdst=0), 'title': 'D-BIAS: A Causality-Based Human-in-the-Loop System for Tackling\n  Algorithmic Bias', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'D-BIAS: A Causality-Based Human-in-the-Loop System for Tackling\n  Algorithmic Bias'}, 'summary': 'With the rise of AI, algorithms have become better at learning underlying\npatterns from the training data including ingrained social biases based on\ngender, race, etc. Deployment of such algorithms to domains such as hiring,\nhealthcare, law enforcement, etc. has raised serious concerns about fairness,\naccountability, trust and interpretability in machine learning algorithms. To\nalleviate this problem, we propose D-BIAS, a visual interactive tool that\nembodies human-in-the-loop AI approach for auditing and mitigating social\nbiases from tabular datasets. It uses a graphical causal model to represent\ncausal relationships among different features in the dataset and as a medium to\ninject domain knowledge. A user can detect the presence of bias against a\ngroup, say females, or a subgroup, say black females, by identifying unfair\ncausal relationships in the causal network and using an array of fairness\nmetrics. Thereafter, the user can mitigate bias by acting on the unfair causal\nedges. For each interaction, say weakening/deleting a biased causal edge, the\nsystem uses a novel method to simulate a new (debiased) dataset based on the\ncurrent causal model. Users can visually assess the impact of their\ninteractions on different fairness metrics, utility metrics, data distortion,\nand the underlying data distribution. Once satisfied, they can download the\ndebiased dataset and use it for any downstream application for fairer\npredictions. We evaluate D-BIAS by conducting experiments on 3 datasets and\nalso a formal user study. We found that D-BIAS helps reduce bias significantly\ncompared to the baseline debiasing approach across different fairness metrics\nwhile incurring little data distortion and a small loss in utility. Moreover,\nour human-in-the-loop based approach significantly outperforms an automated\napproach on trust, interpretability and accountability.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'With the rise of AI, algorithms have become better at learning underlying\npatterns from the training data including ingrained social biases based on\ngender, race, etc. Deployment of such algorithms to domains such as hiring,\nhealthcare, law enforcement, etc. has raised serious concerns about fairness,\naccountability, trust and interpretability in machine learning algorithms. To\nalleviate this problem, we propose D-BIAS, a visual interactive tool that\nembodies human-in-the-loop AI approach for auditing and mitigating social\nbiases from tabular datasets. It uses a graphical causal model to represent\ncausal relationships among different features in the dataset and as a medium to\ninject domain knowledge. A user can detect the presence of bias against a\ngroup, say females, or a subgroup, say black females, by identifying unfair\ncausal relationships in the causal network and using an array of fairness\nmetrics. Thereafter, the user can mitigate bias by acting on the unfair causal\nedges. For each interaction, say weakening/deleting a biased causal edge, the\nsystem uses a novel method to simulate a new (debiased) dataset based on the\ncurrent causal model. Users can visually assess the impact of their\ninteractions on different fairness metrics, utility metrics, data distortion,\nand the underlying data distribution. Once satisfied, they can download the\ndebiased dataset and use it for any downstream application for fairer\npredictions. We evaluate D-BIAS by conducting experiments on 3 datasets and\nalso a formal user study. We found that D-BIAS helps reduce bias significantly\ncompared to the baseline debiasing approach across different fairness metrics\nwhile incurring little data distortion and a small loss in utility. Moreover,\nour human-in-the-loop based approach significantly outperforms an automated\napproach on trust, interpretability and accountability.'}, 'authors': [{'name': 'Bhavya Ghai'}, {'name': 'Klaus Mueller'}], 'author_detail': {'name': 'Klaus Mueller'}, 'author': 'Klaus Mueller', 'arxiv_comment': 'Accepted to IEEE VIS Conference 2022', 'links': [{'href': 'http://arxiv.org/abs/2208.05126v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2208.05126v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CY', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.HC', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ME', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}",Bhavya Ghai,http://arxiv.org/abs/2208.05126v1,2208.05126v1
