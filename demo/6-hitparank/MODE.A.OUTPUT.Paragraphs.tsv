9999	LABEL:UYYbisXZ	Paragraph_ID	Book_Paragraph_ID	Paragraph_Reference	CollectedHitRankSummary	CollectedHits	TokensInParagraph	TotalUniquedScore	TotalUniquedScorePercentage	TotalHitCount	HitTypeTokenRatio	SummaryUnique_Hits_U	Unique_Hits_U	SummaryUnique_Hits_Y	Unique_Hits_Y	SummaryUnique_Hits_Ybis	Unique_Hits_Ybis	SummaryUnique_Hits_X	Unique_Hits_X	SummaryUnique_Hits_Z	Unique_Hits_Z	Paragraph_(lemmatized)	Paragraph_(tokenized)
7	NYNYN	P_000001	B_0000P_0001	I2010.07384v2.p.1	Y:1:1Y:3:1,X:3:1	-- interpretability[1] interpretable_model[1] -- explainability[6] --	252	2	0.79	8	25.00	n.a.	--	Y:1:1 Y:3:1	interpretability[1] interpretable_model[1]	n.a.	--	X:3:1	explainability[6]	n.a.	--	LEM:--- abstract : | the importance of explainability in machine learning continue to grow , as both neural-network architecture and the datum they model become increasingly complex . unique challenge arise when a model 's input feature become high dimensional : on one hand , principled model-agnostic approach to explainability become too computationally expensive ; on the other , more efficient explainability algorithm lack natural interpretation for general user . in this work , we introduce a framework for human-interpretable explainability on high-dimensional datum , consist of two module . first , we apply a semantically-meaningful latent representation , both to reduce the raw dimensionality of the datum , and to ensure its human interpretability . these latent feature can be learn , e.g. explicitly as disentangle representation or implicitly through image-to-image translation , or they can be base on any computable quantity the user choose . second , we adapt the shapley paradigm for model-agnostic explainability to operate on these latent feature . this lead to interpretable model explanation that be both theoretically-controlled and computationally-tractable . we benchmark our approach on synthetic datum and demonstrate its effectiveness on several image-classification task . author : - | * * damien de mijolla [ ^ 1 ] , christopher frye $ ^ * $ , markus kunesch , john mansir , & ilya feige * * \ faculty , 54 welbeck street , london , uk bibliography : - semantic_shapley . bib title : | human-interpretable model explainability \ on high-dimensional datum --- 	TOK:--- abstract : | The importance of explainability in machine learning continues to grow , as both neural-network architectures and the data they model become increasingly complex . Unique challenges arise when a model 's input features become high dimensional : on one hand , principled model-agnostic approaches to explainability become too computationally expensive ; on the other , more efficient explainability algorithms lack natural interpretations for general users . In this work , we introduce a framework for human-interpretable explainability on high-dimensional data , consisting of two modules . First , we apply a semantically-meaningful latent representation , both to reduce the raw dimensionality of the data , and to ensure its human interpretability . These latent features can be learnt , e.g. explicitly as disentangled representations or implicitly through image-to-image translation , or they can be based on any computable quantities the user chooses . Second , we adapt the Shapley paradigm for model-agnostic explainability to operate on these latent features . This leads to interpretable model explanations that are both theoretically-controlled and computationally-tractable . We benchmark our approach on synthetic data and demonstrate its effectiveness on several image-classification tasks . author : - | * * Damien de Mijolla [ ^ 1 ] , Christopher Frye $ ^ * $ , Markus Kunesch , John Mansir , & Ilya Feige * * \ Faculty , 54 Welbeck Street , London , UK bibliography : - semantic_shapley . bib title : | Human-interpretable model explainability \ on high-dimensional data --- 
3	NNNYN	P_000003	B_0000P_0003	I2010.07384v2.p.3	X:3:1	-- -- -- explainability[2] --	62	0	0.00	2	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[2]	n.a.	--	LEM:the explainability of ai system be important , both for model development and model assurance . this importance continue to rise as ai model -- and the datum on which they be train -- become ever more complex . moreover , method for ai explainability must be adapt to maintain the human-interpretability of explanation in the regime of highly complex datum . 	TOK:The explainability of AI systems is important , both for model development and model assurance . This importance continues to rise as AI models -- and the data on which they are trained -- become ever more complex . Moreover , methods for AI explainability must be adapted to maintain the human-interpretability of explanations in the regime of highly complex data . 
3	NNNYN	P_000004	B_0000P_0004	I2010.07384v2.p.4	X:3:1	-- -- -- explainability[1] --	79	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:many explainability method exist in the literature . model-specific technique refer to the internal structure of a model in formulate explanation [ @ chen 2016 xgboost ; @ shrikumar 2017 learning ] , while model-agnostic method be base solely on input-output relationship and treat the model as a black-box [ @ breiman 2001 random ; @ lime ] . model-agnostic method offer wide applicability and , importantly , fix a common language for explanation across different model type . 	TOK:Many explainability methods exist in the literature . Model-specific techniques refer to the internal structure of a model in formulating explanations [ @ chen 2016 xgboost ; @ shrikumar 2017 learning ] , while model-agnostic methods are based solely on input-output relationships and treat the model as a black-box [ @ breiman 2001 random ; @ LIME ] . Model-agnostic methods offer wide applicability and , importantly , fix a common language for explanations across different model types . 
3	NNNYN	P_000005	B_0000P_0005	I2010.07384v2.p.5	X:3:1	-- -- -- explainability[2] --	90	0	0.00	2	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[2]	n.a.	--	LEM:the shapley framework for model-agnostic explainability stand out , due to its theoretically principled foundation and incorporation of interaction effect between the datum 's feature [ @ shapley 52 ; @ shap ] . the shapley framework have be use for explainability in machine learn for year [ @ lipovetsky 2001 analysis ; @ kononenko 2010 efficient ; @ vstrumbelj 2014 explaining ; @ datta 2016 algorithmic ] . unfortunately , the combinatoric require to capture interaction effect make shapley value computationally intensive and thus ill-suite for high-dimensional datum . 	TOK:The Shapley framework for model-agnostic explainability stands out , due to its theoretically principled foundation and incorporation of interaction effects between the data 's features [ @ shapley 52 ; @ SHAP ] . The Shapley framework has been used for explainability in machine learning for years [ @ lipovetsky 2001 analysis ; @ kononenko 2010 efficient ; @ vstrumbelj 2014 explaining ; @ datta 2016 algorithmic ] . Unfortunately , the combinatorics required to capture interaction effects make Shapley values computationally intensive and thus ill-suited for high-dimensional data . 
3	NNNYN	P_000007	B_0000P_0007	I2010.07384v2.p.7	X:3:1	-- -- -- explainability[1] --	30	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:! [ our propose framework for semantic explainability . ] ( figure / pixel_baseline . pdf ) { # fig : framework width = " \ \ textwidth " } 	TOK:! [ Our proposed framework for semantic explainability . ] ( figures / pixel_baseline . pdf ) { # fig : framework width = " \ \ textwidth " } 
3	NNNYN	P_000008	B_0000P_0008	I2010.07384v2.p.8	X:3:1	-- -- -- explainability[1] --	30	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:! [ our propose framework for semantic explainability . ] ( figure / semantic-shapley . pdf ) { # fig : framework width = " \ \ textwidth " } 	TOK:! [ Our proposed framework for semantic explainability . ] ( figures / semantic-shapley . pdf ) { # fig : framework width = " \ \ textwidth " } 
3	NNNYN	P_000009	B_0000P_0009	I2010.07384v2.p.9	X:3:1	-- -- -- explainability[1] --	143	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:when intricately understand by the practitioner , these method for model explainability can be useful , e.g. for model development . however , many alternative method exist to achieve broadly the same goal ( i.e. to monitor how output change as input vary ) with alternative design choice that make their explanation uncomparable to a general user : e.g. the distinct explanation in fig . [ \ [ fig : pixel-baseline \ ] ] ( # fig : pixel-baseline ) { reference-type = " ref " reference = " fig : pixel-baseline " } describe the same model prediction . ideally , a set of axiom ( agree upon or debate ) would constrain the space of explanation , thus lead to a framework of curate method that the user can choose from base on which axiom be relevant to the application . 	TOK:When intricately understood by the practitioner , these methods for model explainability can be useful , e.g. for model development . However , many alternative methods exist to achieve broadly the same goal ( i.e. to monitor how outputs change as inputs vary ) with alternative design choices that make their explanations uncomparable to a general user : e.g. the distinct explanations in Fig . [ \ [ fig : pixel-baseline \ ] ] ( # fig : pixel-baseline ) { reference-type = " ref " reference = " fig : pixel-baseline " } describe the same model prediction . Ideally , a set of axioms ( agreed upon or debated ) would constrain the space of explanations , thus leading to a framework of curated methods that the user can choose from based on which axioms are relevant to the application . 
3	NNNYN	P_000012	B_0000P_0012	I2010.07384v2.p.12	X:3:1	-- -- -- explainability[3] --	88	0	0.00	3	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[3]	n.a.	--	LEM:once a set of semantic latent feature be select , one must choose an explainability algorithm to obtain quantitative information about why a certain model prediction be make . fortunately , since the set of latent feature be low-dimensional by construction , a shapley-based approach become once again viable . in this work , we develop a method to apply shapley explainability at the level of semantic latent feature , thus provide a theoretically-controlled , model-agnostic foundation for explainability on high-dimensional datum . our main contribution be : 	TOK:Once a set of semantic latent features is selected , one must choose an explainability algorithm to obtain quantitative information about why a certain model prediction was made . Fortunately , since the set of latent features is low-dimensional by construction , a Shapley-based approach becomes once again viable . In this work , we develop a method to apply Shapley explainability at the level of semantic latent features , thus providing a theoretically-controlled , model-agnostic foundation for explainability on high-dimensional data . Our main contributions are : 
3	NNNYN	P_000013	B_0000P_0013	I2010.07384v2.p.13	X:3:1	-- -- -- explainability[2] --	56	0	0.00	2	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[2]	n.a.	--	LEM:- we introduce an approach to model explainability on high-dimensional datum that involve encode the raw input feature into a digestible number of semantically meaningful latent feature . we develop a procedure to apply shapley explainability in this context , obtain shapley value that describe the high-dimensional model 's dependence on each semantic latent feature . 	TOK:- We introduce an approach to model explainability on high-dimensional data that involves encoding the raw input features into a digestible number of semantically meaningful latent features . We develop a procedure to apply Shapley explainability in this context , obtaining Shapley values that describe the high-dimensional model 's dependence on each semantic latent feature . 
3	NNNYN	P_000015	B_0000P_0015	I2010.07384v2.p.15	X:3:1	-- -- -- explainability[1] --	40	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:semantic shapley explainability { # sec : semantic-shapley } = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 	TOK:Semantic Shapley explainability { # sec : semantic-shapley } = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 
3	NNNYN	P_000017	B_0000P_0017	I2010.07384v2.p.17	X:3:1	-- -- -- explainability[1] --	87	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:we will begin by describe module ( ii ) in sec . [ 2.1 ] ( # sec : shapley ) { reference-type = " ref " reference = " sec : shapley " } , where we will show how to adapt shapley explainability to latent feature . then we will describe several option for module ( i ) in sec . [ 2.2 ] ( # sec : semantic-rep ) { reference-type = " ref " reference = " sec : semantic-rep " } . 	TOK:We will begin by describing module ( ii ) in Sec . [ 2.1 ] ( # sec : shapley ) { reference-type = " ref " reference = " sec : shapley " } , where we will show how to adapt Shapley explainability to latent features . Then we will describe several options for module ( i ) in Sec . [ 2.2 ] ( # sec : semantic-reps ) { reference-type = " ref " reference = " sec : semantic-reps " } . 
3	NNNYN	P_000020	B_0000P_0020	I2010.07384v2.p.20	X:3:1	-- -- -- explainability[1] --	335	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:the method can be adapt to explain the output of a machine learning model by interpret the model 's input feature $ x = ( x _ 1 , \ ldots , x_n ) $ as the player of a game . consider a classification task , and let $ f_y(x) $ be the model 's predict probability that datum point $ x $ belong to class $ y $ . to apply shapley explainability , one must define a value function represent the model 's expect output give only a subset of the input feature $ x_s $ . the most common choice be $ $ \ begin{aligned} \ label { eq : value-function } v _ { f_y(x) } ( s ) = \ mathbb e _ { p ( x ' ) } \ big [ f_y ( x_s \ sqcup x ' _ { \ bar s } ) \ big ] \ end{aligned} $ $ where $ p ( x ' ) $ be the distribution from which the data be draw , $ \ bar s $ be the complement of $ s $ , and $ x_s \ sqcup x ' _ { \ bar s } $ represent the splice datum point with in-coalition feature from $ x $ and out-of-coalition feature from $ x ' $ . then , insert the value function of eq . ( [ \ [ eq : value-function \ ] ] ( # eq : value-function ) { reference-type = " ref " reference = " eq : value-function " } ) into the definition of eq . ( [ \ [ eq : shapley \ ] ] ( # eq : shapley ) { reference-type = " ref " reference = " eq : shapley " } ) , one obtain shapley value $ \ phi _ { f_y(x) } ( i ) $ represent the portion of the prediction $ f_y(x) $ attributable to feature $ x_i $ . 	TOK:The method can be adapted to explain the output of a machine learning model by interpreting the model 's input features $ x = ( x _ 1 , \ ldots , x_n ) $ as the players of a game . Consider a classification task , and let $ f_y(x) $ be the model 's predicted probability that data point $ x $ belongs to class $ y $ . To apply Shapley explainability , one must define a value function representing the model 's expected output given only a subset of the input features $ x_S $ . The most common choice is $ $ \ begin{aligned} \ label { eq : value-function } v _ { f_y(x) } ( S ) = \ mathbb E _ { p ( x ' ) } \ big [ f_y ( x_S \ sqcup x ' _ { \ bar S } ) \ big ] \ end{aligned} $ $ where $ p ( x ' ) $ is the distribution from which the data is drawn , $ \ bar S $ is the complement of $ S $ , and $ x_S \ sqcup x ' _ { \ bar S } $ represents the spliced data point with in-coalition features from $ x $ and out-of-coalition features from $ x ' $ . Then , inserting the value function of Eq . ( [ \ [ eq : value-function \ ] ] ( # eq : value-function ) { reference-type = " ref " reference = " eq : value-function " } ) into the definition of Eq . ( [ \ [ eq : shapley \ ] ] ( # eq : shapley ) { reference-type = " ref " reference = " eq : shapley " } ) , one obtains Shapley values $ \ phi _ { f_y(x) } ( i ) $ representing the portion of the prediction $ f_y(x) $ attributable to feature $ x_i $ . 
3	NNNYN	P_000022	B_0000P_0022	I2010.07384v2.p.22	X:3:1	-- -- -- explainability[1] --	399	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:to adapt the shapley framework to latent feature , suppose ( as in fig . [ 2 ] ( # fig : framework ) { reference-type = " ref " reference = " fig : framework " } ) that a map $ x \ to z $ exist to transform the raw model input $ x $ into a semantically meaningful representation $ z(x) $ , and that an ( approximate ) inverse map $ z \ to \ tilde x $ exist as well . then we can obtain an explanation of the model prediction $ f_y(x) $ in term of the latent feature $ z(x) $ by apply shapley explainability instead to the function $ f_y ( \ tilde x(z) ) $ at the point $ z = z(x) $ . to be precise , we define a value function $ $ \ begin{aligned} \ label { eq : latent-value } \ tilde v _ { f_y(x) } ( s ) = \ mathbb e _ { p ( x ' ) } \ bigg [ \ , f_y \ big ( \ tilde x \ big ( \ , z_s(x) \ sqcup z _ { \ bar s } ( x ' ) \ , \ big ) \ big ) \ , \ bigg ] \ end{aligned} $ $ which represent the marginalisation of $ f_y ( \ tilde x(z) ) $ over out-of-coalition feature $ z _ { \ bar s } $ . here $ z_s(x) $ be the in-coalition slice of $ z(x) $ , and $ z _ { \ bar s } ( x ' ) $ be the out-of-coalition slice correspond to a different data point . these get splice together in latent space before transform back to model-input-space and feed into the model . insert the value function of eq . ( [ \ [ eq : latent-value \ ] ] ( # eq : latent-value ) { reference-type = " ref " reference = " eq : latent-value " } ) into the definition of eq . ( [ \ [ eq : shapley \ ] ] ( # eq : shapley ) { reference-type = " ref " reference = " eq : shapley " } ) produce semantic shapley value that explain $ f_y(x) $ in term of latent feature $ z_i $ . 	TOK:To adapt the Shapley framework to latent features , suppose ( as in Fig . [ 2 ] ( # fig : framework ) { reference-type = " ref " reference = " fig : framework " } ) that a mapping $ x \ to z $ exists to transform the raw model inputs $ x $ into a semantically meaningful representation $ z(x) $ , and that an ( approximate ) inverse mapping $ z \ to \ tilde x $ exists as well . Then we can obtain an explanation of the model prediction $ f_y(x) $ in terms of the latent features $ z(x) $ by applying Shapley explainability instead to the function $ f_y ( \ tilde x(z) ) $ at the point $ z = z(x) $ . To be precise , we define a value function $ $ \ begin{aligned} \ label { eq : latent-value } \ tilde v _ { f_y(x) } ( S ) = \ mathbb E _ { p ( x ' ) } \ Bigg [ \ , f_y \ Big ( \ tilde x \ big ( \ , z_S(x) \ sqcup z _ { \ bar S } ( x ' ) \ , \ big ) \ Big ) \ , \ Bigg ] \ end{aligned} $ $ which represents the marginalisation of $ f_y ( \ tilde x(z) ) $ over out-of-coalition features $ z _ { \ bar S } $ . Here $ z_S(x) $ is the in-coalition slice of $ z(x) $ , and $ z _ { \ bar S } ( x ' ) $ is the out-of-coalition slice corresponding to a different data point . These get spliced together in latent space before transforming back to model-input-space and feeding into the model . Inserting the value function of Eq . ( [ \ [ eq : latent-value \ ] ] ( # eq : latent-value ) { reference-type = " ref " reference = " eq : latent-value " } ) into the definition of Eq . ( [ \ [ eq : shapley \ ] ] ( # eq : shapley ) { reference-type = " ref " reference = " eq : shapley " } ) produces semantic Shapley values that explain $ f_y(x) $ in terms of latent features $ z_i $ . 
3	NNNYN	P_000024	B_0000P_0024	I2010.07384v2.p.24	X:3:1	-- -- -- explainability[1] --	50	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:a wide variety of method exist to transform from the high-dimensional set of raw model input to an alternative set of feature that offer semantic insight into the datum be model . in this section , we consider several option for this semantic component of our approach to explainability . 	TOK:A wide variety of methods exist to transform from the high-dimensional set of raw model inputs to an alternative set of features that offer semantic insight into the data being modelled . In this section , we consider several options for this semantic component of our approach to explainability . 
3	NNNYN	P_000032	B_0000P_0032	I2010.07384v2.p.32	X:3:1	-- -- -- explainability[1] --	188	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:disentangled representation base on variational autoencoder [ @ vaekingma ] include an encoder $ z(x) $ and a decoder $ \ tilde x(z) $ , thus fit neatly into our framework ( fig . [ 2 ] ( # fig : framework ) { reference-type = " ref " reference = " fig : framework " } ) for semantic explainability . the value function of eq . ( [ \ [ eq : latent-value \ ] ] ( # eq : latent-value ) { reference-type = " ref " reference = " eq : latent-value " } ) then lead to shapley value that explain a model 's prediction in term of the disentangle factor of variation underlie the datum . we will demonstrate this for unsupervised ( sec . [ 3.3 ] ( # sec : dsprite ) { reference-type = " ref " reference = " sec : dsprite " } ) and supervise ( sec . [ 3.4 ] ( # sec : mnist ) { reference-type = " ref " reference = " sec : mnist " } ) disentanglement with experiment . 	TOK:Disentangled representations based on variational autoencoders [ @ VAEKingma ] include an encoder $ z(x) $ and a decoder $ \ tilde x(z) $ , thus fitting neatly into our framework ( Fig . [ 2 ] ( # fig : framework ) { reference-type = " ref " reference = " fig : framework " } ) for semantic explainability . The value function of Eq . ( [ \ [ eq : latent-value \ ] ] ( # eq : latent-value ) { reference-type = " ref " reference = " eq : latent-value " } ) then leads to Shapley values that explain a model 's predictions in terms of the disentangled factors of variation underlying the data . We will demonstrate this for unsupervised ( Sec . [ 3.3 ] ( # sec : dsprites ) { reference-type = " ref " reference = " sec : dsprites " } ) and supervised ( Sec . [ 3.4 ] ( # sec : mnist ) { reference-type = " ref " reference = " sec : mnist " } ) disentanglement with experiments . 
3	NNNYN	P_000035	B_0000P_0035	I2010.07384v2.p.35	X:3:1	-- -- -- explainability[1] --	158	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:image-to-image translation method can straightforwardly be incorporate into our framework for semantic explainability . to do so , one insert the result of the translation $ x \ to \ tilde x \ big ( z_s(x) \ sqcup z _ { \ bar s } ( x ' ) \ big ) $ , which correspond to the modification of semantic attribute $ z _ { \ bar s } ( x ) \ to z _ { \ bar s } ( x ' ) $ , into the value function of eq . ( [ \ [ eq : latent-value \ ] ] ( # eq : latent-value ) { reference-type = " ref " reference = " eq : latent-value " } ) . we demonstrate this in sec . [ 3.5 ] ( # sec : celeba ) { reference-type = " ref " reference = " sec : celeba " } below . 	TOK:Image-to-image translation methods can straightforwardly be incorporated into our framework for semantic explainability . To do so , one inserts the result of the translation $ x \ to \ tilde x \ big ( z_S(x) \ sqcup z _ { \ bar S } ( x ' ) \ big ) $ , which corresponds to the modification of semantic attributes $ z _ { \ bar S } ( x ) \ to z _ { \ bar S } ( x ' ) $ , into the value function of Eq . ( [ \ [ eq : latent-value \ ] ] ( # eq : latent-value ) { reference-type = " ref " reference = " eq : latent-value " } ) . We demonstrate this in Sec . [ 3.5 ] ( # sec : celeba ) { reference-type = " ref " reference = " sec : celeba " } below . 
3	NNNYN	P_000040	B_0000P_0040	I2010.07384v2.p.40	X:3:1	-- -- -- explainability[1] --	127	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:we begin by apply our explainability framework use the fourier transform as the semantic representation . we do this on cifar -10 [ @ cifar 10 ] and investigate whether sensitivity to adversarial example [ @ adversarialexamplesdiscovery ; @ adversarialexamplesexplainharness ; @ networkfoole ] be link to dependence on high-frequency ( i.e. small length-scale ) fluctuation . we consider two classifier on cifar -10 : a robust model [ @ robustpaper ; @ robustness_github ] train to be insensitive to adversarial perturbation , and a non-robust model [ @ resnet ] train naturally . we compute semantic shapley value accord to sec . [ 2.2.1 ] ( # sec : fourier ) { reference-type = " ref " reference = " sec : fourier " } . 	TOK:We begin by applying our explainability framework using the Fourier transform as the semantic representation . We do this on CIFAR -10 [ @ cifar 10 ] and investigate whether sensitivity to adversarial examples [ @ adversarialexamplesdiscovery ; @ adversarialexamplesexplainharness ; @ networkFooling ] is linked to dependence on high-frequency ( i.e. small length-scale ) fluctuations . We considered two classifiers on CIFAR -10 : a robust model [ @ robustPaper ; @ robustness_github ] trained to be insensitive to adversarial perturbations , and a non-robust model [ @ resnet ] trained naturally . We computed semantic Shapley values according to Sec . [ 2.2.1 ] ( # sec : fourier ) { reference-type = " ref " reference = " sec : fourier " } . 
3	NNNYN	P_000043	B_0000P_0043	I2010.07384v2.p.43	X:3:1	-- -- -- explainability[1] --	125	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:the second row of fig . [ 3 ] ( # fig : cifar ) { reference-type = " ref " reference = " fig : cifar " } show global semantic explanation for these model , which correspond to aggregate local explanation across the datum set . we see that the trend find above for one particular image hold in general throughout the datum . our framework for semantic explainability thus lead to the interesting result that adversarially robust classifier be less sensitive to high-frequency information . see app . [ 6.2 ] ( # app : imagenet ) { reference-type = " ref " reference = " app : imagenet " } for similar result on imagenet [ @ imagenet 09 ] . 	TOK:The second row of Fig . [ 3 ] ( # fig : cifar ) { reference-type = " ref " reference = " fig : cifar " } shows global semantic explanations for these models , which correspond to aggregating local explanations across the data set . We see that the trends found above for one particular image hold in general throughout the data . Our framework for semantic explainability thus leads to the interesting result that adversarially robust classifiers are less sensitive to high-frequency information . See App . [ 6.2 ] ( # app : imagenet ) { reference-type = " ref " reference = " app : imagenet " } for similar results on ImageNet [ @ imagenet 09 ] . 
3	NNNYN	P_000049	B_0000P_0049	I2010.07384v2.p.49	X:3:1	-- -- -- explainability[1] --	91	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:here we apply our explainability framework use unsupervise disentangled representation to extract the semantic feature . we do this on dsprites [ @ dsprites 17 ] , synthetic image of sprite ( see fig . [ 5 ] ( # fig : dsprite ) { reference-type = " ref " reference = " fig : dsprite " } ) generate from a know latent space with 5 dimension : shape , scale , orientation , and horizontal and vertical position . this experiment serve as a benchmark of our approach . 	TOK:Here we apply our explainability framework using unsupervised disentangled representations to extract the semantic features . We do this on dSprites [ @ dsprites 17 ] , synthetic images of sprites ( see Fig . [ 5 ] ( # fig : dsprites ) { reference-type = " ref " reference = " fig : dsprites " } ) generated from a known latent space with 5 dimensions : shape , scale , orientation , and horizontal and vertical positions . This experiment serves as a benchmark of our approach . 
3	NNNYN	P_000052	B_0000P_0052	I2010.07384v2.p.52	X:3:1	-- -- -- explainability[1] --	54	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:this experiment validate our framework for semantic explainability , as the modelling task and factor of variation in the datum be fully understand and consistent with our result . moreover , this example showcase an explanation that differentiate between shape and scale -- semantic feature that cannot be distinguish in a pixel-based explanation . 	TOK:This experiment validates our framework for semantic explainability , as the modelling task and factors of variation in the data are fully understood and consistent with our results . Moreover , this example showcases an explanation that differentiates between shape and scale -- semantic features that cannot be distinguished in a pixel-based explanation . 
3	NNNYN	P_000060	B_0000P_0060	I2010.07384v2.p.60	X:3:1	-- -- -- explainability[1] --	149	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:here we apply our explainability framework use image-to-image translation as the implicit semantic representation . we do this on celeba [ @ celeba 15 ] to demonstrate , on real world datum , that our method elucidate pattern in model behaviour that be miss by other method . in particular , we train a cnn to predict the label attractiveness of individual in the datum . we choose this admittedly banal label because it be influence by a variety of other high level attribute , include sensitive characteristic ( e.g. gender , age , skin tone ) that should not influence fair model prediction . pixel-based explanation ( e.g. fig . [ \ [ fig : pixel-baseline \ ] ] ( # fig : pixel-baseline ) { reference-type = " ref " reference = " fig : pixel-baseline " } ) provide no insight into these important issue . 	TOK:Here we apply our explainability framework using image-to-image translation as the implicit semantic representation . We do this on CelebA [ @ celeba 15 ] to demonstrate , on real world data , that our method elucidates patterns in model behaviour that are missed by other methods . In particular , we train a CNN to predict the labelled attractiveness of individuals in the data . We choose this admittedly banal label because it is influenced by a variety of other higher level attributes , including sensitive characteristics ( e.g. gender , age , skin tone ) that should not influence fair model predictions . Pixel-based explanations ( e.g. Fig . [ \ [ fig : pixel-baseline \ ] ] ( # fig : pixel-baseline ) { reference-type = " ref " reference = " fig : pixel-baseline " } ) provide no insight into these important issues . 
4	NYNYN	P_000067	B_0000P_0067	I2010.07384v2.p.67	Y:1:1,X:3:1	-- interpretable_model[1] -- explainability[1] --	122	1	0.82	2	50.00	n.a.	--	Y:1:1	interpretable_model[1]	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:in this work , we have focus on explain model prediction in term of semantic latent feature rather than the model 's raw input . in related work , natural language processing be leveraged to produce interpretable model explanation textually [ @ groundingvisualexplantions ; @ nlpexplanation ] . other method [ @ conceptactivationvector ; @ interpretablebasisdecomposition ] learn to associate pattern of neuron activation with semantic concept . however , each of these require large set of annotate datum for training -- a barrier to widespread application -- whereas we offer both unsupervised and analytic option for semantic explainability in our approach . orthogonal effort exist to train high-capacity model that be intrinsically interpretable [ @ selfexplainingnn ; @ thislookslikesthat ] . 	TOK:In this work , we have focused on explaining model predictions in terms of semantic latent features rather than the model 's raw inputs . In related work , natural language processing is leveraged to produce interpretable model explanations textually [ @ GroundingVisualExplantions ; @ NLPexplanation ] . Other methods [ @ ConceptActivationVectors ; @ InterpretableBasisDecomposition ] learn to associate patterns of neuron activations with semantic concepts . However , each of these requires large sets of annotated data for training -- a barrier to widespread application -- whereas we offer both unsupervised and analytic options for semantic explainability in our approach . Orthogonal efforts exist to train high-capacity models that are intrinsically interpretable [ @ SelfExplainingNN ; @ ThisLooksLikesThat ] . 
3	NNNYN	P_000072	B_0000P_0072	I2010.07384v2.p.72	X:3:1	-- -- -- explainability[2] --	153	0	0.00	2	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[2]	n.a.	--	LEM:in this work , we introduce an approach to model explainability on high-dimensional datum , in which explanation be articulate in term of a digestible set of semantic latent feature . we adapt the shapley paradigm to this task , in order to attribute a model 's prediction to the latent feature underlie its input datum . these two development form a principled , flexible framework for human-interpretable explainability on complex model . to demonstrate its flexibility , we highlight fourier transform , latent disentanglement , and image-to-image translation as option for the semantic representation that offer vary level of user control . we benchmarke our method on synthetic datum , where the underlie latent feature be control , and demonstrate its effectiveness in an extensive set of experiment use off-the-shelf pretraine model . we hope this framework will find wide applicability and offer practitioner a new way to probe their model . 	TOK:In this work , we introduced an approach to model explainability on high-dimensional data , in which explanations are articulated in terms of a digestible set of semantic latent features . We adapted the Shapley paradigm to this task , in order to attribute a model 's prediction to the latent features underlying its input data . These two developments form a principled , flexible framework for human-interpretable explainability on complex models . To demonstrate its flexibility , we highlighted Fourier transforms , latent disentanglement , and image-to-image translation as options for the semantic representation that offer varying levels of user control . We benchmarked our method on synthetic data , where the underlying latent features are controlled , and demonstrated its effectiveness in an extensive set of experiments using off-the-shelf pretrained models . We hope this framework will find wide applicability and offer practitioners a new way to probe their models . 
4	NYNNN	P_000110	B_0000P_0110	I2103.12308v1.p.1	Y:1:1Y:3:1	-- interpretability[2] interpretable_model[1] -- -- --	406	2	0.49	3	66.67	n.a.	--	Y:1:1 Y:3:1	interpretability[2] interpretable_model[1]	n.a.	--	n.a.	--	n.a.	--	LEM:--- abstract : | interpretability in machine learning model be important in high-stakes decision , such as whether to order a biopsy base on a mammographic exam . mammography pose important challenge that be not present in other computer vision task : dataset be small , confound information be present , and it can be difficult even for a radiologist to decide between watchful waiting and biopsy base on a mammogram alone . in this work , we present a framework for interpretable machine learning-based mammography . in addition to predict whether a lesion be malignant or benign , our work aim to follow the reasoning process of radiologist in detect clinically relevant semantic feature of each image , such as the characteristic of the mass margin . the framework include a novel interpretable neural network algorithm that use case-based reasoning for mammography . our algorithm can incorporate a combination of datum with whole image labelling and datum with pixel-wise annotation , lead to well accuracy and interpretability even with a small number of image . our interpretable model be able to highlight the classification-relevant part of the image , whereas other method highlight healthy tissue and confound information . our model be decision aid , rather than decision maker , aim at well overall human-machine collaboration . we do not observe a loss in mass margin classification accuracy over a black box neural network train on the same datum . author : - | alina jade barnett \ * duke university * \ alina . barnett \ @ duke . edu \ - | fides regina schwartz \ * duke university * \ fides . schwartz \ @ duke . edu \ - | chaofan tao \ * duke university * \ chaofan . tao \ @ gmail . com \ - | chaofan chen \ * university of maine * \ chaofan.ch en \ @ maine . edu \ - | yinhao ren \ * duke university * \ yinhao . ren \ @ duke . edu \ - | joseph y. lo \ * duke university * \ joseph . lo \ @ duke . edu \ - | cynthia rudin \ * duke university * \ cynthia \ @ cs . duke . edu \ bibliography : - main . bib title : ' iaia-bl : a case-based interpretable deep learning model for classification of mass lesion in digital mammography ' --- 	TOK:--- abstract : | Interpretability in machine learning models is important in high-stakes decisions , such as whether to order a biopsy based on a mammographic exam . Mammography poses important challenges that are not present in other computer vision tasks : datasets are small , confounding information is present , and it can be difficult even for a radiologist to decide between watchful waiting and biopsy based on a mammogram alone . In this work , we present a framework for interpretable machine learning-based mammography . In addition to predicting whether a lesion is malignant or benign , our work aims to follow the reasoning processes of radiologists in detecting clinically relevant semantic features of each image , such as the characteristics of the mass margins . The framework includes a novel interpretable neural network algorithm that uses case-based reasoning for mammography . Our algorithm can incorporate a combination of data with whole image labelling and data with pixel-wise annotations , leading to better accuracy and interpretability even with a small number of images . Our interpretable models are able to highlight the classification-relevant parts of the image , whereas other methods highlight healthy tissue and confounding information . Our models are decision aids , rather than decision makers , aimed at better overall human-machine collaboration . We do not observe a loss in mass margin classification accuracy over a black box neural network trained on the same data . author : - | Alina Jade Barnett \ * Duke University * \ alina . barnett \ @ duke . edu \ - | Fides Regina Schwartz \ * Duke University * \ Fides . Schwartz \ @ duke . edu \ - | Chaofan Tao \ * Duke University * \ chaofan . tao \ @ gmail . com \ - | Chaofan Chen \ * University of Maine * \ chaofan.ch en \ @ maine . edu \ - | Yinhao Ren \ * Duke University * \ yinhao . ren \ @ duke . edu \ - | Joseph Y. Lo \ * Duke University * \ joseph . lo \ @ duke . edu \ - | Cynthia Rudin \ * Duke University * \ cynthia \ @ cs . duke . edu \ bibliography : - main . bib title : ' IAIA-BL : A Case-based Interpretable Deep Learning Model for Classification of Mass Lesions in Digital Mammography ' --- 
1	NNNYN	P_000112	B_0000P_0112	I2103.12308v1.p.3	X:1:1	-- -- -- explainable[1] --	108	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:1:1	explainable[1]	n.a.	--	LEM:! [ ( a ) uninterpretable approach give no explanation for their output . ( b ) other interpretable or explainable approach might point out which region be use for decision making , but provide no information about what attribute of the region be important for classification decision . ( c ) iaia-bl provide an explanation framework that localize relevant area , associate the relevant area with a specific medical feature and use only the explain evidence to make a prediction . ] ( . / fig / intro_fig_v 2 -2 . png ) { # fig : intro_fig width = " .8 \ \ linewidth " } 	TOK:! [ ( a ) Uninterpretable approaches give no explanations for their output . ( b ) Other interpretable or explainable approaches might point out which regions are used for decision making , but provide no information about what attributes of the region are important for classification decisions . ( c ) IAIA-BL provides an explanation framework that localizes relevant areas , associates the relevant area with a specific medical feature and uses only the explained evidence to make a prediction . ] ( . / figs / intro_fig_v 2 -2 . png ) { # fig : intro_fig width = " .8 \ \ linewidth " } 
4	NYNNN	P_000126	B_0000P_0126	I2103.12308v1.p.17	Y:1:1Y:3:1	-- interpretability[2] interpretable_model[1] -- -- --	113	2	1.77	3	66.67	n.a.	--	Y:1:1 Y:3:1	interpretability[2] interpretable_model[1]	n.a.	--	n.a.	--	n.a.	--	LEM:& ours & [ @ wu 2019 deep ] & [ @ kim 2018 icadx ] & [ @ wu 2018 deepminer ] \ inherently interpretable model ( not posthoc ) & & & & \ provide global interpretability ( on model ) & & & & \ provide local interpretability ( on each case ) & & & & \ explanation be guarantee to match model reasoning & & & & \ incorporate domain-specific terminology & & & & \ provide similar prototype for comparison & & & & \ can incorporate fine annotation & & & & \ can be train on datum with mixed labeling & & & & \ 	TOK:& Ours & [ @ wu 2019 deep ] & [ @ kim 2018 icadx ] & [ @ wu 2018 deepminer ] \ Inherently interpretable model ( not posthoc ) & & & & \ Provides global interpretability ( on model ) & & & & \ Provides local interpretability ( on each case ) & & & & \ Explanation is guaranteed to match model reasoning & & & & \ Incorporate domain-specific terminology & & & & \ Provides similar prototypes for comparison & & & & \ Can incorporate fine annotation & & & & \ Can be trained on data with mixed labeling & & & & \ 
1	NYNNN	P_000132	B_0000P_0132	I2103.12308v1.p.23	Y:1:1	-- interpretable_model[1] -- -- --	130	1	0.77	1	100.00	n.a.	--	Y:1:1	interpretable_model[1]	n.a.	--	n.a.	--	n.a.	--	LEM:model need to be collaborator in the medical decision-making process in order to be useful . in mammography , the initial clinical decision be express as a bi-rads category of 1 to 5 , correspond to the recommendation of whether the patient need a biopsy [ @ orel 1999 bi ; @ sickle 2013 acr ] . an inscrutable model predict malignant / benign be not useful as a decision aid , as a biopsy be recommend for every lesion with great than 2 % chance of malignancy ( bi-rads 4 and 5 ) . to alter clinical management , an interpretable model be need to describe its reasoning process for why the patient should or should not receive a biopsy rather than provide an inscrutable prediction of malignancy . 	TOK:Models need to be collaborators in the medical decision-making process in order to be useful . In mammography , the initial clinical decision is expressed as a BI-RADS category of 1 to 5 , corresponding to the recommendation of whether the patient needs a biopsy [ @ orel 1999 bi ; @ sickles 2013 acr ] . An inscrutable model predicting malignant / benign is not useful as a decision aid , as a biopsy is recommended for every lesion with greater than 2 % chance of malignancy ( BI-RADS 4 and 5 ) . To alter clinical management , an interpretable model is needed to describe its reasoning process for why the patient should or should not receive a biopsy rather than provide an inscrutable prediction of malignancy . 
3	NYNNN	P_000133	B_0000P_0133	I2103.12308v1.p.24	Y:3:1	-- interpretability[1] -- -- --	191	1	0.52	1	100.00	n.a.	--	Y:3:1	interpretability[1]	n.a.	--	n.a.	--	n.a.	--	LEM:our ai approach include an explicit reasoning system that resemble that of a practice radiologist . exist interpretability technique for mammography include localization as in figure [ 1 ] ( # fig : intro_fig ) { reference-type = " ref " reference = " fig : intro_fig " } b , but there be no explanation of why an area be select , what attribute of the region be use for classification , or what part of the training set these association be learn from . in a non-medical image analogy , though localization may provide a good interpretation for whether or not an image contain a vase ( perhaps by highlight the vase ) , it do not provide a good interpretation for classification of the vase pattern as roman vs $ . $ asian antiquity ( highlight the vase pattern provide no further insight ) . many recently publish ai-mammography algorithm be still entirely uninterpretable as in figure [ 1 ] ( # fig : intro_fig ) { reference-type = " ref " reference = " fig : intro_fig " } a [ @ mckinney 2020 international ] . 	TOK:Our AI approach includes an explicit reasoning system that resembles that of a practicing radiologist . Existing interpretability techniques for mammography include localization as in Figure [ 1 ] ( # fig : intro_fig ) { reference-type = " ref " reference = " fig : intro_fig " } b , but there is no explanation of why an area is selected , what attributes of the region are used for classification , or what parts of the training set these associations are learned from . In a non-medical image analogy , though localization may provide a good interpretation for whether or not an image contains a vase ( perhaps by highlighting the vase ) , it does not provide a good interpretation for classification of the vase pattern as Roman vs $ . $ Asian antiquity ( highlighting the vase pattern provides no further insight ) . Many recently published AI-mammography algorithms are still entirely uninterpretable as in Figure [ 1 ] ( # fig : intro_fig ) { reference-type = " ref " reference = " fig : intro_fig " } a [ @ mckinney 2020 international ] . 
1	NYNNN	P_000136	B_0000P_0136	I2103.12308v1.p.27	Y:1:1	-- interpretable_model[1] -- -- --	149	1	0.67	1	100.00	n.a.	--	Y:1:1	interpretable_model[1]	n.a.	--	n.a.	--	n.a.	--	LEM:when start to build an interpretable model for breast lesion classification , we naïvely apply the case-based reasoning protopnet architecture to medical image . though the model appear to be learn medically relevant feature because of its high validation accuracy , the model make prediction use region of the image that do not correspond to the medical information ; in other word , the model use confound information rather than medically relevant information . this be consistent with observation make by other group of the danger of confound in medical imaging [ @ wang 2019 remove ] . for non-medical image classification task , a typical approach might be to increase the size of the training set . however , as discuss above , one major barrier to implementation of ai in the medical field be the limited availability of annotate datum [ @ soffer 2019 convolutional ] . 	TOK:When starting to build an interpretable model for breast lesion classification , we naïvely applied the case-based reasoning ProtoPNet architecture to medical images . Though the model appeared to be learning medically relevant features because of its high validation accuracy , the model made predictions using regions of the image that did not correspond to the medical information ; in other words , the model used confounding information rather than medically relevant information . This is consistent with observations made by other groups of the dangers of confounding in medical imaging [ @ wang 2019 removing ] . For non-medical image classification tasks , a typical approach might be to increase the size of the training set . However , as discussed above , one major barrier to implementation of AI in the medical field is the limited availability of annotated data [ @ soffer 2019 convolutional ] . 
3	NYNNN	P_000147	B_0000P_0147	I2103.12308v1.p.38	Y:3:1	-- interpretability[2] -- -- --	124	1	0.81	2	50.00	n.a.	--	Y:3:1	interpretability[2]	n.a.	--	n.a.	--	n.a.	--	LEM:this architecture can provide both local interpretability by explain each prediction in term of the similarity between a give input image and the learn prototype , as in figure [ \ [ fig : iaia_expls \ ] ] ( # fig : iaia_expls ) { reference-type = " ref " reference = " fig : iaia_expls " } , and global interpretability in term of the cluster structure of the latent feature space ( where semantically similar convolutional feature patch be cluster around prototype represent the same semantic concept ) . the set of learn prototype be provide in appendix [ 12 ] ( # app : prototype_set ) { reference-type = " ref " reference = " app : prototype_set " } . 	TOK:This architecture can provide both local interpretability by explaining each prediction in terms of the similarity between a given input image and the learned prototypes , as in Figure [ \ [ fig : IAIA_expls \ ] ] ( # fig : IAIA_expls ) { reference-type = " ref " reference = " fig : IAIA_expls " } , and global interpretability in terms of the clustering structure of the latent feature space ( where semantically similar convolutional feature patches are clustered around prototypes representing the same semantic concepts ) . The set of learned prototypes is provided in Appendix [ 12 ] ( # app : prototype_sets ) { reference-type = " ref " reference = " app : prototype_sets " } . 
3	NYNNN	P_000168	B_0000P_0168	I2103.12308v1.p.59	Y:3:1	-- interpretability[1] -- -- --	9	1	11.11	1	100.00	n.a.	--	Y:3:1	interpretability[1]	n.a.	--	n.a.	--	n.a.	--	LEM:interpretability metric { # subsec : activprec } ----------------------- 	TOK:Interpretability Metric { # subsec : activprec } ----------------------- 
3	NYNNN	P_000169	B_0000P_0169	I2103.12308v1.p.60	Y:3:1	-- interpretability[1] -- -- --	522	1	0.19	1	100.00	n.a.	--	Y:3:1	interpretability[1]	n.a.	--	n.a.	--	n.a.	--	LEM:we design the interpretability metric * activation precision * to quantify what proportion of the information use to classify the mass margin come from the relevant region as mark by the radiologist-annotator . use the notation define in section [ 3.5 ] ( # sec : model_training ) { reference-type = " ref " reference = " sec : model_training " } , the activation precision for a single prototype $ \ mathbf {p}_j $ on a single image $ \ mathbf {x}_i $ that have mass-margin type $ y ^ { \ text{margin} } _ i $ and come with a fine-annotation mask $ \ mathbf {m}_i $ , be define as : $ $ \ label { eq : ap } \ textrm{ap} ( \ mathbf {p}_j , \ mathbf {x}_i , y ^ { \ text{margin} } _ i , \ mathbf {m}_i ) = \ leave ( \ frac { \ sum \ leave [ ( 1 - \ mathbf {m}_i ) \ odot t _ { \ tau } \ leave ( \ textrm{upsample} \ leave ( g _ { \ mathbf {p}_j } ( f ( \ mathbf {x}_i ) ) \ right ) \ right ) \ right ] } { \ sum t _ { \ tau } \ leave ( \ textrm{upsample} \ leave ( g _ { \ mathbf {p}_j } ( f ( \ mathbf {x}_i ) ) \ right ) \ right ) } \ right ) \ text { where } \ text{class} ( \ mathbf {p}_j ) = y ^ { \ text{margin} } _ i , $ $ where $ t _ { \ tau } $ be a threshold function that return the top $ ( 1 - \ tau ) \ time 100 \ % $ of the input value as $ 1 $ and the bottom $ \ tau \ time 100 \ % $ as $ 0 $ . activation precision be only define where the prototype have the same class identity as the image . the fraction in equation ( [ \ [ eq : ap \ ] ] ( # eq : ap ) { reference-type = " ref " reference = " eq : ap " } ) give a proportion of highly activate pixel that be medically relevant . we elaborate on this in appendix [ 15 ] ( # app : activation_precision ) { reference-type = " ref " reference = " app : activation_precision " } . to evaluate activation precision for gradcam [ @ selvaraju _ 2017 _ iccv ] and gradcam++ [ @ chattopadhay 2018 grad ] , we calculate as in equation ( [ \ [ eq : ap \ ] ] ( # eq : ap ) { reference-type = " ref " reference = " eq : ap " } ) but replace the prototype activation map $ \ textrm{upsample} \ leave ( g _ { \ mathbf {p}_j } ( f ( \ mathbf {x}_i ) ) \ right ) $ with the normalize gradient map for the correct class . 	TOK:We designed the interpretability metric * activation precision * to quantify what proportion of the information used to classify the mass margin comes from the relevant region as marked by the radiologist-annotator . Using the notations defined in Section [ 3.5 ] ( # sec : model_training ) { reference-type = " ref " reference = " sec : model_training " } , the activation precision for a single prototype $ \ mathbf {p}_j $ on a single image $ \ mathbf {x}_i $ that has mass-margin type $ y ^ { \ text{margin} } _ i $ and comes with a fine-annotation mask $ \ mathbf {m}_i $ , is defined as : $ $ \ label { eq : ap } \ textrm{AP} ( \ mathbf {p}_j , \ mathbf {x}_i , y ^ { \ text{margin} } _ i , \ mathbf {m}_i ) = \ left ( \ frac { \ sum \ left [ ( 1 - \ mathbf {m}_i ) \ odot T _ { \ tau } \ left ( \ textrm{Upsample} \ left ( g _ { \ mathbf {p}_j } ( f ( \ mathbf {x}_i ) ) \ right ) \ right ) \ right ] } { \ sum T _ { \ tau } \ left ( \ textrm{Upsample} \ left ( g _ { \ mathbf {p}_j } ( f ( \ mathbf {x}_i ) ) \ right ) \ right ) } \ right ) \ text { where } \ text{class} ( \ mathbf {p}_j ) = y ^ { \ text{margin} } _ i , $ $ where $ T _ { \ tau } $ is a threshold function that returns the top $ ( 1 - \ tau ) \ times 100 \ % $ of the input values as $ 1 $ and the bottom $ \ tau \ times 100 \ % $ as $ 0 $ . Activation precision is only defined where the prototype has the same class identity as the image . The fraction in Equation ( [ \ [ eq : ap \ ] ] ( # eq : ap ) { reference-type = " ref " reference = " eq : ap " } ) gives a proportion of highly activated pixels that are medically relevant . We elaborate on this in Appendix [ 15 ] ( # app : activation_precision ) { reference-type = " ref " reference = " app : activation_precision " } . To evaluate activation precision for GradCAM [ @ Selvaraju _ 2017 _ ICCV ] and GradCAM++ [ @ chattopadhay 2018 grad ] , we calculate as in Equation ( [ \ [ eq : ap \ ] ] ( # eq : ap ) { reference-type = " ref " reference = " eq : ap " } ) but replace the prototype activation map $ \ textrm{Upsample} \ left ( g _ { \ mathbf {p}_j } ( f ( \ mathbf {x}_i ) ) \ right ) $ with the normalized gradient map for the correct class . 
3	NYNNN	P_000172	B_0000P_0172	I2103.12308v1.p.63	Y:3:1	-- interpretability[1] -- -- --	88	1	1.14	1	100.00	n.a.	--	Y:3:1	interpretability[1]	n.a.	--	n.a.	--	n.a.	--	LEM:activation precision be a measure of interpretability , in the sense that the high the activation precision , the well a prototype ( or a set of prototype ) be at detect medically relevant feature for mass-margin classification . in our experiment , we use $ \ tau = 0.95 $ because iaia-bl use the top 5 % of activate patch in its prediction . 95 % confidence interval be derive use non-parametric bootstrap resample with 5000 sample each equal to the size of the test set . 	TOK:Activation precision is a measure of interpretability , in the sense that the higher the activation precision , the better a prototype ( or a set of prototypes ) is at detecting medically relevant features for mass-margin classification . In our experiments , we used $ \ tau = 0.95 $ because IAIA-BL uses the top 5 % of activated patches in its predictions . 95 % confidence intervals were derived using non-parametric bootstrap resampling with 5000 samples each equal to the size of the test set . 
3	NYNNN	P_000178	B_0000P_0178	I2103.12308v1.p.69	Y:3:1	-- interpretability[1] -- -- --	124	1	0.81	1	100.00	n.a.	--	Y:3:1	interpretability[1]	n.a.	--	n.a.	--	n.a.	--	LEM:* * baseline 2 a and 2 b : vgg -16 [ @ simonyan 2015 very ] with gradcam [ @ selvaraju _ 2017 _ iccv ] and gradcam++ [ @ chattopadhay 2018 grad ] . * * we train a vgg -16 model with two add fully connect layer to account for the large number of parameter in our model . pre-traine on imagenet , it be train for 250 epoch and the epoch with the high test accuracy be select for comparison . there be no native way to incorporate our fine annotation into vgg -16 . vgg -16 provide no inherent interpretability or localization . use the posthoc gradcam and gradcam++ technique we show localization information and calculate activation precision . 	TOK:* * Baselines 2 a and 2 b : VGG -16 [ @ simonyan 2015 very ] with GradCAM [ @ Selvaraju _ 2017 _ ICCV ] and GradCAM++ [ @ chattopadhay 2018 grad ] . * * We trained a VGG -16 model with two added fully connected layers to account for the larger number of parameters in our model . Pre-trained on ImageNet , it was trained for 250 epochs and the epoch with the highest test accuracy is selected for comparison . There is no native way to incorporate our fine annotation into VGG -16 . VGG -16 provides no inherent interpretability or localization . Using the posthoc GradCAM and GradCAM++ techniques we show localization information and calculate activation precision . 
3	NYNNN	P_000185	B_0000P_0185	I2103.12308v1.p.76	Y:3:1	-- interpretability[1] -- -- --	447	1	0.22	1	100.00	n.a.	--	Y:3:1	interpretability[1]	n.a.	--	n.a.	--	n.a.	--	LEM:& \ & & & vgg -16 [ @ simonyan 2015 very ] & vgg -16 [ @ simonyan 2015 very ] \ & iaia-bl & protopnet [ @ ppnet ] & with gradcam [ @ selvaraju _ 2017 _ iccv ] & with gradcam++ [ @ chattopadhay 2018 grad ] \ performance ( auroc ) & & & & \ mass margin class . & * * 0.951 * * \ [ 0.905 , 0.996 \ ] & * 0.911 * \ [ 0.848 , 0.974 \ ] & * 0.947 * \ [ 0.898 , 0.996 \ ] & * 0.947 * \ [ 0.898 , 0.996 \ ] \ spiculate vs. all & * 0.96 * \ [ 0.90 , 1.00 \ ] & * * 0.97 * * \ [ 0.93 , 1.00 \ ] & * 0.95 * \ [ 0.89 , 1.00 \ ] & * 0.95 * \ [ 0.89 , 1.00 \ ] \ indistinct vs. all & * 0.93 * \ [ 0.88 , 0.99 \ ] & * 0.87 * \ [ 0.78 , 0.94 \ ] & * * 0.94 * * \ [ 0.89 , 0.99 \ ] & * * 0.94 * * \ [ 0.89 , 0.99 \ ] \ circumscribed vs. all & * * 0.97 * * \ [ 0.94 , 1.00 \ ] & * 0.93 * \ [ 0.87 , 1.00 \ ] & * 0.95 * \ [ 0.91 , 1.00 \ ] & * 0.95 * \ [ 0.91 , 1.00 \ ] \ cohen 's $ \ kappa $ & * * 0.74 * * \ [ 0.60 , 0.86 \ ] & * 0.64 * \ [ 0.49 , 0.78 \ ] & * * 0.74 * * \ [ 0.60 , 0.87 \ ] & * * 0.74 * * \ [ 0.60 , 0.87 \ ] \ interpretability & & & & \ fine-scale act . prec . & * * 0.41 * * \ [ 0.39 , 0.45 \ ] & 0.24 \ [ 0.17 , 0.31 \ ] & 0.21 \ [ 0.05 , 0.43 \ ] $ ^ { \ textrm{a} } $ & 0.24 \ [ 0.08 , 0.45 \ ] $ ^ { \ textrm{a} } $ \ lesion-scale act . prec . & * * 0.94 * * \ [ 0.92 , 0.97 \ ] & 0.51 \ [ 0.34 , 0.68 \ ] & 0.45 \ [ 0.37 , 0.54 \ ] $ ^ { \ textrm{a} } $ & 0.53 \ [ 0.44 , 0.61 \ ] $ ^ { \ textrm{a} } $ \ 	TOK:& \ & & & VGG -16 [ @ simonyan 2015 very ] & VGG -16 [ @ simonyan 2015 very ] \ & IAIA-BL & ProtoPNet [ @ PPNet ] & with GradCAM [ @ Selvaraju _ 2017 _ ICCV ] & with GradCAM++ [ @ chattopadhay 2018 grad ] \ Performance ( AUROC ) & & & & \ Mass Margin Class . & * * 0.951 * * \ [ 0.905 , 0.996 \ ] & * 0.911 * \ [ 0.848 , 0.974 \ ] & * 0.947 * \ [ 0.898 , 0.996 \ ] & * 0.947 * \ [ 0.898 , 0.996 \ ] \ Spiculated vs. all & * 0.96 * \ [ 0.90 , 1.00 \ ] & * * 0.97 * * \ [ 0.93 , 1.00 \ ] & * 0.95 * \ [ 0.89 , 1.00 \ ] & * 0.95 * \ [ 0.89 , 1.00 \ ] \ Indistinct vs. all & * 0.93 * \ [ 0.88 , 0.99 \ ] & * 0.87 * \ [ 0.78 , 0.94 \ ] & * * 0.94 * * \ [ 0.89 , 0.99 \ ] & * * 0.94 * * \ [ 0.89 , 0.99 \ ] \ Circumscribed vs. all & * * 0.97 * * \ [ 0.94 , 1.00 \ ] & * 0.93 * \ [ 0.87 , 1.00 \ ] & * 0.95 * \ [ 0.91 , 1.00 \ ] & * 0.95 * \ [ 0.91 , 1.00 \ ] \ Cohen 's $ \ kappa $ & * * 0.74 * * \ [ 0.60 , 0.86 \ ] & * 0.64 * \ [ 0.49 , 0.78 \ ] & * * 0.74 * * \ [ 0.60 , 0.87 \ ] & * * 0.74 * * \ [ 0.60 , 0.87 \ ] \ Interpretability & & & & \ Fine-scale Act . Prec . & * * 0.41 * * \ [ 0.39 , 0.45 \ ] & 0.24 \ [ 0.17 , 0.31 \ ] & 0.21 \ [ 0.05 , 0.43 \ ] $ ^ { \ textrm{a} } $ & 0.24 \ [ 0.08 , 0.45 \ ] $ ^ { \ textrm{a} } $ \ Lesion-scale Act . Prec . & * * 0.94 * * \ [ 0.92 , 0.97 \ ] & 0.51 \ [ 0.34 , 0.68 \ ] & 0.45 \ [ 0.37 , 0.54 \ ] $ ^ { \ textrm{a} } $ & 0.53 \ [ 0.44 , 0.61 \ ] $ ^ { \ textrm{a} } $ \ 
3	NYNNN	P_000188	B_0000P_0188	I2103.12308v1.p.79	Y:3:1	-- interpretability[1] -- -- --	110	1	0.91	1	100.00	n.a.	--	Y:3:1	interpretability[1]	n.a.	--	n.a.	--	n.a.	--	LEM:performance of iaia-bl be well than that of protopnet ( baseline 1 ) , which do not have the stabilization of the gradient provide by the average pooling improvement of iaia-bl . vgg -16 ( baseline 2 a and 2 b ) perform comparably to iaia-bl for auroc . remember that the baseline model be permit to use confound information that iaia-bl be not encourage to use , and we will see that when we consider the interpretability result . as we know , it be easy to perform well on training datum despite use logic that a radiologist would claim be incorrect [ @ zech 2018 variable ] . 	TOK:Performance of IAIA-BL is better than that of ProtoPNet ( Baseline 1 ) , which does not have the stabilization of the gradient provided by the average pooling improvement of IAIA-BL . VGG -16 ( Baselines 2 a and 2 b ) performed comparably to IAIA-BL for AUROC . Remember that the baseline models are permitted to use confounding information that IAIA-BL is not encouraged to use , and we will see that when we consider the interpretability results . As we know , it is easy to perform well on training data despite using logic that a radiologist would claim is incorrect [ @ zech 2018 variable ] . 
3	NYNNN	P_000190	B_0000P_0190	I2103.12308v1.p.81	Y:3:1	-- interpretability[3] -- -- --	143	1	0.70	3	33.33	n.a.	--	Y:3:1	interpretability[3]	n.a.	--	n.a.	--	n.a.	--	LEM:* * interpretability result : * * to measure interpretability , we use the interpretability metric activation precision from section [ 4.2 ] ( # subsec : activprec ) { reference-type = " ref " reference = " subsec : activprec " } , show in the low two row of table [ \ [ tab : margin_results \ ] ] ( # tab : margin_result ) { reference-type = " ref " reference = " tab : margin_result " } . for the unpruned iaia-bl model ( not show in the table , because it be almost identical to iaia-bl ) , the lesion-scale activation precision of the learn prototype be 0.93 ( 95 % ci : 0.91 , 0.96 ) and the fine-scale activation precision of the learn prototype be 0.41 ( 95 % ci : 0.39 , 0.43 ) . 	TOK:* * Interpretability Results : * * To measure interpretability , we used the interpretability metric activation precision from Section [ 4.2 ] ( # subsec : activprec ) { reference-type = " ref " reference = " subsec : activprec " } , shown in the lower two rows of Table [ \ [ tab : margin_results \ ] ] ( # tab : margin_results ) { reference-type = " ref " reference = " tab : margin_results " } . For the unpruned IAIA-BL model ( not shown in the table , because it is almost identical to IAIA-BL ) , the lesion-scale activation precision of the learned prototypes is 0.93 ( 95 % CI : 0.91 , 0.96 ) and the fine-scale activation precision of the learned prototypes is 0.41 ( 95 % CI : 0.39 , 0.43 ) . 
3	NYNNN	P_000192	B_0000P_0192	I2103.12308v1.p.83	Y:3:1	-- interpretability[1] -- -- --	72	1	1.39	1	100.00	n.a.	--	Y:3:1	interpretability[1]	n.a.	--	n.a.	--	n.a.	--	LEM:* * * to summarize , iaia-bl 's predictive performance be as good or well than the analogous black-box model . its performance in mimic our annotator be well than the typical interannotator agreement between radiologist . its interpretability , measure by how well its attention agree with a radiologist annotator 's hand-drawn attention map , exceed that of exist method and do not resort to post-hoc analysis . * * * 	TOK:* * * To summarize , IAIA-BL 's predictive performance was as good or better than the analogous black-box model . Its performance in mimicking our annotator was better than the typical interannotator agreement between radiologists . Its interpretability , measured by how well its attention agreed with a radiologist annotator 's hand-drawn attention maps , exceeded that of existing methods and does not resort to post-hoc analysis . * * * 
4	NYNNN	P_000201	B_0000P_0201	I2103.12308v1.p.92	Y:1:1Y:3:1	-- interpretability[1] interpretable_model[1] -- -- --	147	2	1.36	2	100.00	n.a.	--	Y:1:1 Y:3:1	interpretability[1] interpretable_model[1]	n.a.	--	n.a.	--	n.a.	--	LEM:the high performance of uninterpretable model that appear to be leverage mainly confound information be a point of concern when incorporate model into clinical practice . though a radiologist may not choose to view an explanation for every prediction , interpretable model still provide value over uninterpretable model . because we know that ai system fail [ @ zech 2018 variable ] , we design a system that can alert a radiologist to faulty reasoning at the time the prediction be make instead of only after the consequence of misprediction have be realize . the global interpretability ( namely , the set of prototype ) allow the train model to be fine-tuned by domain expert through pruning of prototype that do not correspond to medically relevant feature . the explanation provide can also be use for debug a model and for retrospective analysis of model failure . 	TOK:The high performance of uninterpretable models that appear to be leveraging mainly confounding information is a point of concern when incorporating models into clinical practice . Though a radiologist may not choose to view an explanation for every prediction , interpretable models still provide value over uninterpretable models . Because we know that AI systems fail [ @ zech 2018 variable ] , we designed a system that can alert a radiologist to faulty reasoning at the time the prediction is made instead of only after the consequences of misprediction have been realized . The global interpretability ( namely , the set of prototypes ) allows the trained model to be fine-tuned by domain experts through pruning of prototypes that do not correspond to medically relevant features . The explanations provided can also be used for debugging a model and for retrospective analysis of model failures . 
3	NYNNN	P_000252	B_0000P_0252	I2103.12308v1.p.143	Y:3:1	-- interpretability[2] -- -- --	123	1	0.81	2	50.00	n.a.	--	Y:3:1	interpretability[2]	n.a.	--	n.a.	--	n.a.	--	LEM:this architecture provide both local interpretability by explain each prediction in term of the similarity between a give input image and the learn prototype , as in figure [ \ [ fig : iaia_expls \ ] ] ( # fig : iaia_expls ) { reference-type = " ref " reference = " fig : iaia_expls " } , and global interpretability in term of the cluster structure of the latent feature space ( where semantically similar convolutional feature patch be cluster around prototype represent the same semantic concept ) . the set of learn prototype be provide in appendix [ 12 ] ( # app : prototype_set ) { reference-type = " ref " reference = " app : prototype_set " } . 	TOK:This architecture provides both local interpretability by explaining each prediction in terms of the similarity between a given input image and the learned prototypes , as in Figure [ \ [ fig : IAIA_expls \ ] ] ( # fig : IAIA_expls ) { reference-type = " ref " reference = " fig : IAIA_expls " } , and global interpretability in terms of the clustering structure of the latent feature space ( where semantically similar convolutional feature patches are clustered around prototypes representing the same semantic concepts ) . The set of learned prototypes is provided in Appendix [ 12 ] ( # app : prototype_sets ) { reference-type = " ref " reference = " app : prototype_sets " } . 
3	NYNNN	P_000268	B_0000P_0268	I2106.01043v1.p.1	Y:3:1	-- interpretability[1] -- -- --	292	1	0.34	1	100.00	n.a.	--	Y:3:1	interpretability[1]	n.a.	--	n.a.	--	n.a.	--	LEM:--- abstract : | in recent year , causal modelling have be use widely to improve generalization and to provide interpretability in machine learning model . to determine cause-effect relationship in the absence of a randomized trial , we can model causal system with counterfactual and intervention give enough domain knowledge . however , there be several case where domain knowledge be almost absent and the only recourse be use a statistical method to estimate causal relationship . while there have be several work do in estimate causal relationship in unstructured datum , we be yet to find a well-defined framework for estimate causal relationship in knowledge graphs ( kg ) . it be commonly use to provide a semantic framework for datum with complex inter-domain relationship . in this work , we define a hybrid approach that allow we to discover cause-effect relationship in kg . the propose approach be base around the finding of the instantaneous causal structure of a non-experimental matrix use a non-gaussian model , i.e ; find the causal ordering of the variable in a non-gaussian setting . the non-experimental matrix be a low-dimensional tensor projection obtain by decompose the adjacency tensor of a kg . we use two different pre-existe algorithm , one for the causal discovery and the other for decompose the kg and combine they to get the causal structure in a kg . author : - | rohan giriraj , sinnu susan thomas \ department of computer science and engineering \ digital university kerala ( iiitmk ) \ india 695317 \ ` rohan . mi 19 , sinnu.thomas@iiitmk.ac.in ` \ bibliography : - reference . bib title : ' causal discovery in knowledge graphs by exploit asymmetric properties of non-gaussian distributions ' --- 	TOK:--- abstract : | In recent years , causal modelling has been used widely to improve generalization and to provide interpretability in machine learning models . To determine cause-effect relationships in the absence of a randomized trial , we can model causal systems with counterfactuals and interventions given enough domain knowledge . However , there are several cases where domain knowledge is almost absent and the only recourse is using a statistical method to estimate causal relationships . While there have been several works done in estimating causal relationships in unstructured data , we are yet to find a well-defined framework for estimating causal relationships in Knowledge Graphs ( KG ) . It is commonly used to provide a semantic framework for data with complex inter-domain relationships . In this work , we define a hybrid approach that allows us to discover cause-effect relationships in KG . The proposed approach is based around the finding of the instantaneous causal structure of a non-experimental matrix using a non-Gaussian model , i.e ; finding the causal ordering of the variables in a non-Gaussian setting . The non-experimental matrix is a low-dimensional tensor projection obtained by decomposing the adjacency tensor of a KG . We use two different pre-existing algorithms , one for the causal discovery and the other for decomposing the KG and combining them to get the causal structure in a KG . author : - | Rohan Giriraj , Sinnu Susan Thomas \ Department of Computer Science and Engineering \ Digital University Kerala ( IIITMK ) \ India 695317 \ ` rohan . mi 19 , sinnu.thomas@iiitmk.ac.in ` \ bibliography : - references . bib title : ' Causal Discovery in Knowledge Graphs by Exploiting Asymmetric Properties of Non-Gaussian Distributions ' --- 
1	NYNNN	P_000270	B_0000P_0270	I2106.01043v1.p.3	Y:1:1	-- interpretable_model[1] -- -- --	334	1	0.30	1	100.00	n.a.	--	Y:1:1	interpretable_model[1]	n.a.	--	n.a.	--	n.a.	--	LEM:accord to pearl , causal inference be nothing more than a formalize approach to answer the simple question of " why " in statistic . most machine learning be base around the principle of correlation and usually operate under the assumption that the training and testing sample be $ i.i.d $ . i.e ; the training and testing sample belong to the same distribution . where machine learning fall short be in its ability to generalize from previous experience and transfer they to current problem . this be call * out-of-distribution * generalization [ @ schoelkopf 2019 ] . the implication of causal inference in machine learning have be huge , range from well , more interpretable model to helping solve adversarial vulnerability and enable robust generalization in common supervised learning task . there have be several know approach to define causality as a formal study of cause-effect relationship , the most prominent one be the neyman-rubin causal model ( c . 1923 ) which define the * fundamental problem of causal inference * \ [ insert reference here \ ] and a potential outcome ' framework as an alternative to randomized trial / experiment . pearl far extend the formal study of causality by define a structural causal model ( scm ) \ [ insert reference here pearl 2009 a . \ ] . the scm use a combination of structural equation and graphical causal diagram to represent causal relationship . the scm viewpoint be intuitive for machine learning which be base around the approximation of function . in the scm model , we have a set of observable $ x _ 1 , ... , x_n $ model as random variable associate with the vertex of direct acyclic graph . we assume each observable to be the result of a function which can be define as : $ $ \ label{scm} x_i : = f_i ( pa_i , u_i ) , ( i = 1 , ... , n ) $ $ 	TOK:According to Pearl , causal inference is nothing more than a formalized approach to answer the simple question of " Why " in statistics . Most machine learning is based around the principle of correlation and usually operates under the assumption that the training and testing samples are $ i.i.d $ . i.e ; the training and testing samples belong to the same distribution . Where machine learning falls short is in its ability to generalize from previous experiences and transfer them to current problems . This is called * out-of-distribution * generalization [ @ Schoelkopf 2019 ] . The implications of causal inference in machine learning have been huge , ranging from better , more interpretable models to helping solve adversarial vulnerability and enabling robust generalization in common supervised learning tasks . There have been several known approaches to define causality as a formal study of cause-effect relationships , the most prominent one being the Neyman-Rubin causal model ( c . 1923 ) which defines the * fundamental problem of causal inference * \ [ insert reference here \ ] and a potential outcomes ' framework as an alternative to randomized trials / experiments . Pearl further extended the formal study of causality by defining a structural causal model ( SCM ) \ [ insert reference here Pearl 2009 a . \ ] . The SCM uses a combination of structural equations and graphical causal diagrams to represent causal relationships . The SCM viewpoint is intuitive for machine learning which is based around the approximation of functions . In the SCM model , we have a set of observables $ X _ 1 , ... , X_n $ modelled as random variables associated with the vertices of directed acyclic graphs . We assume each observable to be the result of a function which can be defined as : $ $ \ label{scm} X_i : = f_i ( PA_i , U_i ) , ( i = 1 , ... , n ) $ $ 
1	NNNYN	P_000276	B_0000P_0276	I2106.01043v1.p.9	X:1:1	-- -- -- explainable[1] --	133	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:1:1	explainable[1]	n.a.	--	LEM:the implication of causal discovery in the context of kg be huge , allow well path discovery for explainable reasoning and querying . in this work , we define a hybrid theoretical approach to attempt solve the problem of causal discovery in kg . the contribution of this work be threefold . at first , we use tucker [ @ balazevic 2019 ] , a method for embed a kg after decompose its adjacency tensor into a core tensor and constituent matrix . secondly , we take the decompose tensor , and we project they into a matrix , * * q * * . finally , the project matrix be then pass to the directlingam [ @ shimizu 2011 ] algorithm that find the causal ordering from the give datum matrix . 	TOK:The implications of causal discovery in the context of KG are huge , allowing better path discovery for explainable reasoning and querying . In this work , we define a hybrid theoretical approach to attempt solving the problem of causal discovery in KG . The contributions of this work are threefold . At first , we use TuckER [ @ Balazevic 2019 ] , a method for embedding a KG after decomposing its adjacency tensors into a core tensor and constituent matrices . Secondly , we take the decomposed tensor , and we project them into a matrix , * * Q * * . Finally , the projected matrix is then passed to the DirectLiNGAM [ @ Shimizu 2011 ] algorithm that finds the causal ordering from the given data matrix . 
3	NYNNN	P_000350	B_0000P_0350	I2202.02830v3.p.1	Y:3:1	-- interpretability[1] -- -- --	319	1	0.31	1	100.00	n.a.	--	Y:3:1	interpretability[1]	n.a.	--	n.a.	--	n.a.	--	LEM:--- abstract : | interactive * recommender system * have emerge as a promising paradigm to overcome the limitation of the primitive user feedback use by traditional recommender system ( e.g. , click , item consumption , rating ) . they allow user to express intent , preference , constraint , and context in a rich fashion , often use natural language ( include faceted search and dialogue ) . yet more research be need to find the most effective way to use this feedback . one challenge be * infer a user 's semantic intent * from the open-ended term or attribute often use to describe a desire item , and use it to refine recommendation result . leveraging * concept activation vector ( cavs ) * [ @ kimtcav : icml 18 ] , a recently develop approach for model interpretability in machine learning , we develop a framework to learn a representation that capture the semantic of such attribute and connect they to user preference and behavior in recommender system . one novel feature of our approach be its ability to distinguish objective and * subjective * attribute ( both subjectivity of * degree * and of * sense * ) , and associate * different sense * of subjective attribute with different user . we demonstrate on both synthetic and real-world datum set that our cav representation not only accurately interpret user ' subjective semantic , but can also be use to improve recommendation through * interactive item critiquing * . author : - christina göpfert - alex haig - ' chih-wei hsu ' - yinlam chow - ivan vendrov - tyler lu - deepak ramachandran - hubert pham - mohammad ghavamzadeh - craig boutilier bibliography : - long . bib - standard . bib - tcav . bib title : discover personalized semantic for soft attributes in recommender systems use concept activation vector --- 	TOK:--- abstract : | Interactive * recommender systems * have emerged as a promising paradigm to overcome the limitations of the primitive user feedback used by traditional recommender systems ( e.g. , clicks , item consumption , ratings ) . They allow users to express intent , preferences , constraints , and contexts in a richer fashion , often using natural language ( including faceted search and dialogue ) . Yet more research is needed to find the most effective ways to use this feedback . One challenge is * inferring a user 's semantic intent * from the open-ended terms or attributes often used to describe a desired item , and using it to refine recommendation results . Leveraging * concept activation vectors ( CAVs ) * [ @ kimTCAV : icml 18 ] , a recently developed approach for model interpretability in machine learning , we develop a framework to learn a representation that captures the semantics of such attributes and connects them to user preferences and behaviors in recommender systems . One novel feature of our approach is its ability to distinguish objective and * subjective * attributes ( both subjectivity of * degree * and of * sense * ) , and associate * different senses * of subjective attributes with different users . We demonstrate on both synthetic and real-world data sets that our CAV representation not only accurately interprets users ' subjective semantics , but can also be used to improve recommendations through * interactive item critiquing * . author : - Christina Göpfert - Alex Haig - ' Chih-wei Hsu ' - Yinlam Chow - Ivan Vendrov - Tyler Lu - Deepak Ramachandran - Hubert Pham - Mohammad Ghavamzadeh - Craig Boutilier bibliography : - long . bib - standard . bib - tcav . bib title : Discovering Personalized Semantics for Soft Attributes in Recommender Systems using Concept Activation Vectors --- 
3	NYNNN	P_000359	B_0000P_0359	I2202.02830v3.p.10	Y:3:1	-- interpretability[1] -- -- --	258	1	0.39	1	100.00	n.a.	--	Y:3:1	interpretability[1]	n.a.	--	n.a.	--	n.a.	--	LEM:at a high-level , our approach work as follow . we assume we be give : ( i ) a collaborative filtering-style model ( e.g. , probabilistic matrix factorization or dual encoder ) which embed item and user in a latent space base on user-item rating ; and ( ii ) a ( small ) set of * tag * ( i.e. , soft attribute label ) provide by a * subset of user * for a * subset of item * . we develop method that associate with each item the degree to which it exhibit a soft attribute , thus determine that attribute 's semantic . we do this by apply * concept activation vector ( cavs ) * [ @ kimtcav : icml 18 ] --- a recent method develop for interpretability of machine-learned model --- to the collaborative filter model to detect whether it * learn a representation of the attribute * . the projection of this cav in embed space provide a ( local ) * directional semantic * for the attribute that can then be apply to item ( and user ) . moreover , the technique can be use to identify the * subjective nature * of an attribute , specifically , whether different user have different meaning ( or tag * sense * ) in mind when use that tag . such a * personalized semantic * for subjective attribute can be vital to the sound interpretation of a user 's true intent when try to assess her preference . 	TOK:At a high-level , our approach works as follows . we assume we are given : ( i ) a collaborative filtering-style model ( e.g. , probabilistic matrix factorization or dual encoder ) which embeds items and users in a latent space based on user-item ratings ; and ( ii ) a ( small ) set of * tags * ( i.e. , soft attribute labels ) provided by a * subset of users * for a * subset of items * . We develop methods that associate with each item the degree to which it exhibits a soft attribute , thus determining that attribute 's semantics . We do this by applying * concept activation vectors ( CAVs ) * [ @ kimTCAV : icml 18 ] --- a recent method developed for interpretability of machine-learned models --- to the collaborative filtering model to detect whether it * learned a representation of the attribute * . The projection of this CAV in embedding space provides a ( local ) * directional semantics * for the attribute that can then be applied to items ( and users ) . Moreover , the technique can be used to identify the * subjective nature * of an attribute , specifically , whether different users have different meanings ( or tag * senses * ) in mind when using that tag . Such a * personalized semantics * for subjective attributes can be vital to the sound interpretation of a user 's true intent when trying to assess her preferences . 
4	NNNYN	P_000595	B_0000P_0595	I2303.01378v1.p.1	X:1:1X:3:1	-- -- -- explainability[1] explainable[1] --	211	0	0.00	2	0.00	n.a.	--	n.a.	--	n.a.	--	X:1:1 X:3:1	explainability[1] explainable[1]	n.a.	--	LEM:--- abstract : | the recent effort in automation of machine learning or datum science have achieve success in various task such as hyper-parameter optimization or model selection . however , key area such as utilize domain knowledge and datum semantic be area where we have see little automation . datum scientist have long leverage common sense reasoning and domain knowledge to understand and enrich datum for build predictive model . in this paper we discuss important shortcoming of current datum science and machine learning solution . we then envision how leverage * semantic * understanding and reasoning on datum in combination with novel tool for data science automation can help with consistent and explainable datum augmentation and transformation . additionally , we discuss how semantic can assist datum scientist in a new manner by help with challenge relate to trust , bias , and explainability in machine learning . semantic annotation can also help well explore and organize large datum source . author : - ' udayan khurana $ ^ 1 $ , kavitha srinivas $ ^ 1 $ , sainyam galhotra $ ^ 2 $ , horst samulowitz $ ^ 1 $ ' bibliography : - main . bib title : a vision for semantically enrich data science --- 	TOK:--- abstract : | The recent efforts in automation of machine learning or data science has achieved success in various tasks such as hyper-parameter optimization or model selection . However , key areas such as utilizing domain knowledge and data semantics are areas where we have seen little automation . Data Scientists have long leveraged common sense reasoning and domain knowledge to understand and enrich data for building predictive models . In this paper we discuss important shortcomings of current data science and machine learning solutions . We then envision how leveraging * semantic * understanding and reasoning on data in combination with novel tools for data science automation can help with consistent and explainable data augmentation and transformation . Additionally , we discuss how semantics can assist data scientists in a new manner by helping with challenges related to trust , bias , and explainability in machine learning . Semantic annotation can also help better explore and organize large data sources . author : - ' Udayan Khurana $ ^ 1 $ , Kavitha Srinivas $ ^ 1 $ , Sainyam Galhotra $ ^ 2 $ , Horst Samulowitz $ ^ 1 $ ' bibliography : - main . bib title : A Vision for Semantically Enriched Data Science --- 
3	NNNYN	P_000602	B_0000P_0602	I2303.01378v1.p.8	X:3:1	-- -- -- explainability[1] --	174	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:in addition to data preparation , other aspect of machine learning ( ml ) and datum science be also dependent on semantic interpretation of datum and model . for example , model explainability be base in part on interpretation of concept in the datum and the meaning of operation perform on they . in order to ensure fairness through de-biasing , we require recognition of traditional point of bias [ @ mehrabi 2021 survey ] and ensure the same be not repeat in the future . to automate this process , semantic concept recognition and an understanding of the causal dependency between identify concept would be central to a trustworthy system that identify and remove it . similarly , enforce business rule through symbolic-ai [ ^ 5 ] and semantic enhance the case for semantically-driven automate datum science . in addition , initial work have target understanding hyper-parameter and extraction of constraint in apply they [ @ hpodoc ] , as well as use of semantic for datum cleansing [ @ 6982731 ] . 	TOK:In addition to data preparation , other aspects of machine learning ( ML ) and data science are also dependent on semantic interpretation of data and models . For example , model explainability is based in part on interpretation of concepts in the data and the meaning of operations performed on them . In order to ensure fairness through de-biasing , we require recognition of traditional points of bias [ @ mehrabi 2021 survey ] and ensuring the same are not repeated in the future . To automate this process , semantic concept recognition and an understanding of the causal dependencies between identified concepts would be central to a trustworthy system that identifies and removes it . Similarly , enforcing business rules through symbolic-AI [ ^ 5 ] and semantics enhances the case for semantically-driven automated data science . In addition , initial work has targeted understanding hyper-parameters and extraction of constraints in applying them [ @ hpodoc ] , as well as use of semantics for data cleansing [ @ 6982731 ] . 
3	NNNYN	P_000606	B_0000P_0606	I2303.01378v1.p.12	X:3:1	-- -- -- explainability[1] --	229	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:[ \ [ data_section \ ] ] { # data_section label = " data_section " } the foremost step in data science be the need to understand datum . understand table , as show in figure [ 2 ] ( # fig : overview ) { reference-type = " ref " reference = " fig : overview " } , in turn help guide the use of semantic in feature generation , as well as choose model appropriate for the domain . explainability in model be enhance by a knowledge of causal relation between key variable in the domain . the figure illustrate how semantic can greatly enhance each component in datum science workflow . rcent research have demonstrate progess in understand the semantic of datum , which be a crucial first step for semantic datum science . broadly , the research in this area can be categorize into : ( a ) table understanding -- which be to map cell value , column , and indeed entire table into a well define set of concept draw from ontology or knowledge graph ; ( b ) inter-table datum discovery -- which discover for instance , other table that can be join or unioned base on a large index of table ; ( c ) understand semantic in code , text or other form or less structured datum source . 	TOK:[ \ [ data_section \ ] ] { # data_section label = " data_section " } The foremost step in data science is the need to understand data . Understanding tables , as shown in Figure [ 2 ] ( # fig : overview ) { reference-type = " ref " reference = " fig : overview " } , in turn helps guide the use of semantics in feature generation , as well as choose models appropriate for the domain . Explainability in models is enhanced by a knowledge of causal relations between key variables in the domain . The figure illustrates how semantics can greatly enhance each component in data science workflows . Rcent research has demonstrated progess in understanding the semantics of data , which is a crucial first step for semantic data science . Broadly , the research in this area can be categorized into : ( a ) table understanding -- which is to map cell values , columns , and indeed entire tables into a well defined set of concepts drawn from ontologies or knowledge graphs ; ( b ) inter-table data discovery -- which discovers for instance , other tables that can be joined or unioned based on a larger index of tables ; ( c ) Understanding semantics in code , text or other forms or less structured data sources . 
3	NNNYN	P_000608	B_0000P_0608	I2303.01378v1.p.14	X:3:1	-- -- -- explainability[1] --	79	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:table understanding help with explainability , trust , bias and feature engineering aspect of data science because it help pick more suitable feature overall to build model . table understanding can also guide search for relevant datum , include table that make sense to join or union . here we cover a sample of the literature on table understanding to show that while challenging , it be possible to an extent that be useful for semantic datum science . 	TOK:Table understanding helps with explainability , trust , bias and feature engineering aspects of data science because it helps pick more suitable features overall to build models . Table understanding can also guide search for relevant data , including tables that make sense to join or union . Here we cover a sample of the literature on table understanding to show that while challenging , it is possible to an extent that is useful for semantic data science . 
3	NNNYN	P_000640	B_0000P_0640	I2303.01378v1.p.46	X:3:1	-- -- -- explainability[1] --	221	0	0.00	1	0.00	n.a.	--	n.a.	--	n.a.	--	X:3:1	explainability[1]	n.a.	--	LEM:much of current ml ( include deep learning ) be operationally devoid of semantic . there be no explicit notion ( e.g. , as part of the input ) of what a feature , value , state , object or label actually ' mean ' . as a result , semantic be not utilize explicitly to build , tune or deploy model . this include even the powerful foundational model for language ( e.g. , gpt 3 [ @ gpt 3 ] ) , speech , image ( e.g. , imagenet [ @ imagenet ] ) , or video recognition , that display impressive performance on specific task . while such model be indeed able to generate effective response which might seem semantically aware on occasion , the underlie model have virtually no intrinsic understanding of semantic . these ml model be neither provide with , nor infer semantic of the input datum . the model be mostly base on statistical and mathematical property of datum . while a data-driven way of derive insight be desire , the meaning of datum itself beyond statistical understanding have be neglect and the result issue be well know range from bias and explainability [ ^ 11 ] , spurious correlation [ ^ 12 ] , inappropriate / meaningless response [ ^ 13 ] . 	TOK:Much of current ML ( including Deep Learning ) is operationally devoid of semantics . There is no explicit notion ( e.g. , as part of the input ) of what a feature , value , state , object or label actually ' means ' . As a result , semantics are not utilized explicitly to build , tune or deploy models . This includes even the powerful foundational models for language ( e.g. , GPT 3 [ @ gpt 3 ] ) , speech , images ( e.g. , Imagenet [ @ imagenet ] ) , or video recognition , that display impressive performance on specific tasks . While such models are indeed able to generate effective responses which might seem semantically aware on occasions , the underlying models have virtually no intrinsic understanding of semantics . These ML models are neither provided with , nor infer semantics of the input data . The models are mostly based on statistical and mathematical properties of data . While a data-driven way of deriving insights is desired , the meaning of data itself beyond statistical understanding has been neglected and the resulting issues are well known ranging from bias and explainability [ ^ 11 ] , spurious correlations [ ^ 12 ] , inappropriate / meaningless responses [ ^ 13 ] . 
4	NNNYN	P_000648	B_0000P_0648	I2303.01378v1.p.54	X:1:1X:3:1	-- -- -- explainability[1] explainable[1] --	111	0	0.00	2	0.00	n.a.	--	n.a.	--	n.a.	--	X:1:1 X:3:1	explainability[1] explainable[1]	n.a.	--	LEM:the use of machine learn technique to make high-stakes decision have raise the importance of trust , which be generally capture in term of fairness , explainability , robustness , among other . most of the prior technique study societal impact of machine learn by explore correlation between dataset attribute without understand their true meaning . consequently , fairness-aware learning technique be brittle to noise in dataset and do not generalize to real-world setting where dataset may suffer from selection bias , and may even contain spurious correlation . explainable ai technique generate explanation that be not semantically coherent [ @ ustun 2019 actionable ] and be difficult to comprehend . 	TOK:The use of machine learning techniques to make high-stakes decisions has raised the importance of trust , which is generally captured in terms of fairness , explainability , robustness , among others . Most of the prior techniques study societal impact of Machine learning by exploring correlation between dataset attributes without understanding their true meaning . Consequently , fairness-aware learning techniques are brittle to noise in datasets and do not generalize to real-world settings where datasets may suffer from selection bias , and may even contain spurious correlations . Explainable AI techniques generate explanations that are not semantically coherent [ @ ustun 2019 actionable ] and are difficult to comprehend . 
