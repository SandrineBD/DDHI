\subsection{Desiderata and Major Themes of Research}
\label{sec:themes}
% \topic{In this section, we'll hit on a small subset of papers and try to give a pedagogical overview of this field and the primary developments. I imagine this section starts with the Wachter paper, defines the loss function, and then proceeds by adding more and more desirable properties of counterfactuals, properties such as sparsity, data manifold, actionability, causality.}


The previous example alludes to many desirable properties of an effective counterfactual explanation. For Alice, the counterfactual should quantify a relatively small change, which will lead to the desired alternative outcome. Alice might need to increase her income by $\$10$K to get approved for a loan, and even though an increase of $\$50$K would do the job, it is most pragmatic for her if she can make the smallest possible change. Additionally, Alice might care about a simpler explanation -- it is easier for her to focus on changing a few things (such as only \emph{Income}) instead of trying to change many features. Alice certainly also cares that the counterfactual she receives is giving her advice, which is realistic and actionable. It would be of little use if the recommendation were to decrease her age by ten years. 

These desiderata, among others, have set the stage for recent developments in the field of counterfactual explainability. As we describe in this section, major themes of research have sought to incorporate increasingly complex constraints on counterfactuals, all in the spirit of ensuring the resulting explanation is truly actionable and helpful. 
% These constraints include sparsity, closeness, realism (similarity to other data), causality, and more. 
Development in this field has focused on addressing these desiderata in a way that is generalizable across algorithms and is computationally efficient. 

\belowcaptionskip=-3pt
\abovecaptionskip=3pt
\begin{figure}
    \centering
    % \includegraphics[width=0.9\columnwidth]{cf_fig.png}
    \includegraphics[width=0.7\columnwidth]{cf_fig.png}
    \caption{Two possible paths for a datapoint (shown in \textcolor{blue}{blue}), originally classified in the negative class, to cross the decision boundary. The endpoints of both the paths (shown in \textcolor{red}{red} and \textcolor{green}{green}) are valid counterfactuals for the original point. Note that the \textcolor{red}{red} path is the shortest, whereas the \textcolor{green}{green} path adheres closely to the manifold of the training data, but is longer.}
    \label{fig:cf_fig}
\end{figure}

\begin{enumerate}[leftmargin=*]
    \item \textit{Validity}: \citet{wachter_counterfactual_2017} first proposed counterfactual explanations in 2017. They posed CFE as an optimization problem. 
    \Cref{eq:cfe1_primal} states the optimization objective, which is to minimize the distance between the counterfactual ($x^{\prime}$) and the original datapoint ($x$) subject to the constraint that the output of the classifier on the counterfactual is the desired label ($y^{\prime} \in \mathcal{Y}$). Converting the objective into a differentiable, unconstrained form yields two terms (see \Cref{eq:cfe1}). 
    The first term encourages the output of the classifier on the counterfactual to be close to the desired class, and the second term forces the counterfactual to be close to the original datapoint. A metric $d$ is used to measure the distance between two datapoints $x, x^{\prime} \in \mathcal{X}$, which can be the L1/L2 distance, or quadratic distance, or distance functions which take as input the CDF of the features~\citep{Ustun19:Actionable}, or pairwise feature costs as perceived by users~\citep{hima-beyond-recourse-globalcfe}. Thus, this original definition already emphasized that an effective counterfactual must be \emph{small change} relative to the starting point. 
    
    \begin{equation}
        {\mathrm{arg \ }}\underset{x^{\prime}}{\mathrm{min \ }} d(x, x^{\prime}) \text{ subject to } f(x^{\prime}) = y^{\prime}
        \label{eq:cfe1_primal}
    \end{equation}
    
    \begin{equation}
        {\mathrm{arg \ }}\underset{x^{\prime}}{\mathrm{min \ }}\underset{\lambda}{\mathrm{max \ }} \lambda(f(x^{\prime}) - y^{\prime})^2 + d(x, x^{\prime})
        \label{eq:cfe1}
    \end{equation}
    % where $\lambda$ is used for trade-off between the two terms and $d(x, x^{\prime})$ is any distance metric between and input $x$ and a potential counterfactual $x^{\prime}$. 
    A counterfactual that indeed is classified in the desired class is a valid counterfactual. As illustrated in \cref{fig:cf_fig}, the points shown in \textcolor{red}{red} and \textcolor{green}{green} are valid counterfactuals, as they are in the positive class region. The distance to the \textcolor{red}{red} counterfactual is smaller than the distance to the \textcolor{green}{green} counterfactual. 
    
    \item \textit{Actionability}: An important consideration while making a recommendation is about which features are mutable (e.g., income, age) and which are not (e.g., race, country of origin).
    A recommended counterfactual should never change the immutable features. In fact, if a change to a legally sensitive feature produces a change in prediction, it shows inherent bias in the model. Several papers have also mentioned that an applicant might have a preference order amongst the mutable features (which can also be hidden.) The optimization problem is modified to take this into account. We might call the set of actionable features $\mathcal{A}$, and update our loss function to be,
    \begin{equation}
        {\mathrm{arg \ }}\underset{x^{\prime }\in \mathcal{A}}{\mathrm{min \ }}\underset{\lambda}{\mathrm{max \ }} \lambda(f(x^{\prime}) - y^{\prime})^2 + d(x, x^{\prime})
        \label{eq:cfe2}
    \end{equation}
    
    \item \textit{Sparsity}: There can be a trade-off between the number of features changed and the total amount of change made to obtain the counterfactual. A counterfactual ideally should change a smaller number of features in order to be the most effective. It has been argued that people find it easier to understand shorter explanations~\citep{Miller-xai:2019,naumann2021consequenceaware}, making sparsity an important consideration. We update our loss function to include a penalty function that encourages sparsity in the difference between the modified and the original datapoint, $g(x^{\prime} - x)$, e.g., L0/L1 norm.   
    \begin{equation}
        {\mathrm{arg \ }}\underset{x^{\prime }\in \mathcal{A}}{\mathrm{min \ }}\underset{\lambda}{\mathrm{max \ }} \lambda(f(x^{\prime}) - y^{\prime})^2 + d(x, x^{\prime}) + g(x^{\prime} - x)
        \label{eq:cfe3}
    \end{equation}
    
    \item \textit{Data Manifold closeness}: It would be hard to trust a counterfactual if it resulted in a combination of features that were utterly unlike any observations the classifier has seen before. In this sense, the counterfactual would be ``unrealistic", not easy to realize, and anomalous to the training datapoints \citep{Brown_Talbert_cfe-anomalous}. Therefore, a generated counterfactual should be realistic in the sense that it is near the training data and adheres to observed correlations among the features. Many papers have proposed various ways of quantifying this. We might update our loss function to include a penalty for adhering to the data manifold defined by the training set $\mathcal{X}$, denoted by $l(x^{\prime} ; \mathcal{X})$
    % \small
    \begin{equation}
    % \begin{small}
        % \scalebox{0.9\columnwidth}{
        \resizebox{.85\columnwidth}{!}{$ {\mathrm{arg \ }}\underset{x^{\prime }\in \mathcal{A}}{\mathrm{min \ }}\underset{\lambda}{\mathrm{max \ }} \lambda(f(x^{\prime}) - y^{\prime})^2 + d(x, x^{\prime}) + g(x^{\prime} - x) + l(x^{\prime} ; \mathcal{X}) $ }
        \label{eq:cfe4}
    % \end{small}
    \end{equation}
    % \normalsize
    In \cref{fig:cf_fig}, the region between the dashed lines shows the data manifold. There are two possible paths to cross the decision boundary for the \textcolor{blue}{blue} datapoint. The shorter, red path takes it to a counterfactual that is outside the data manifold, whereas a bit longer, the green path takes it to a counterfactual that follows the data manifold. Adding the data manifold loss term encourages the algorithm to choose the green path over the red path, even if it is slightly longer. 
    
    \item \textit{Causality}: Features in a dataset are rarely independent, therefore, changing one feature in the real world affects other features. For example, getting a new educational degree necessitates increasing the individual's age by at least some amount. In order to be realistic and actionable, a counterfactual should maintain any known causal relations between features. 
    % Another important consideration is to encode any strong prior knowledge we might have about the features. Specifically, causal relations between features should be taken into account while generating counterfactuals.
    % The second term in \cref{eq:cfe1} is modified to account for such relations.
    % We add the loss term for causal relations in the optimization objective. 
    Generally, our loss function now accounts for (1) counterfactual validity, (2) sparsity in feature vector (and actionability of features); (3) similarity to the training data; and (4) causal relations. 
    \end{enumerate}
    
    The following research themes are not added as terms in the optimization objective; they are properties of the algorithm generating the CFEs. 
    
    \begin{enumerate}[resume]
    \item \textit{Amortized inference}: Generating a counterfactual is expensive, which involves solving an optimization process for each datapoint. 
    \citet{mahajan_preserving_2020} proposed generative technique for ``amortized inference'' of CFEs. Learning to predict a CFE allows the algorithm to quickly compute a counterfactual (or several) for any new input $x$, without requiring to solve an optimization problem. \citet{verma2021amortized} proposed another approach that uses RL to generate amortized CFEs. 
    
    \item \textit{Black-box access}: If a CFE generating approach can work with the black-box access to an ML model, i.e., with only accessing its `predict' function, it can then be used in settings where the access to the ML model cannot be given due to proprietary or legal reasons. \citet{dandl_multi-objective_2020} propose a genetic algorithm and \citet{verma2021amortized} propose a RL-based algorithm to this end. 
    
    \item \textit{Model Agnosticity}: A closely linked concept is model agnosticity. An approach that is model agnostic can work with different kinds of ML models and hence is more desirable than a model-specific approach. An approach that requires black-box access to the model is model-agnostic by definition. 
    
    % \item \textit{Alternative methods}: Finally, several papers solve the counterfactual generation problem using linear programming, mixed-integer programming, Answer Set Programming, or SMT solvers. These approaches give guarantees and optimize fast but are limited to classifiers with linear (or piece-wise linear) structures. 
    
\end{enumerate}


\subsection{Relationship to other related terms} 

Out of the papers collected, different terminology often captures the basic idea of counterfactual explanations, although subtle differences exist between the terms. Several terms worth noting include:
\begin{itemize}[leftmargin=*]
    \item \emph{Algorithmic Recourse}: \citet{Ustun19:Actionable} point out that counterfactuals do not take into account the actionability of the prescribed changes, which recourse does. Works taking a causal view of the problem further fortify this claim \citep{karimi_algorithmic_2020,karimi-imperfect:2020}. 
    Recent papers in counterfactual generation take actionability and feasibility of the prescribed changes, and therefore the difference with recourse has blurred. In this work, we use the term counterfactual explanation, its abbreviation CFE, and recourse interchangeably. 
    
    \item \emph{Inverse classification}: Inverse classification aims to perturb an input in a meaningful way in order to classify it into its desired class~\citep{inverse-classification1,inverse-classification2}. Such an approach prescribes the actions to be taken in order to get the desired classification. Therefore inverse classification has the same goals as CFEs. 
    
    \item \emph{Contrastive explanation}: Contrastive explanations generate explanations of the form ``an input x is classified as y because features $f_1, f_2,\dots, f_k$ are present and $f_n,\dots,f_r$ are absent''. 
    The features that are minimally sufficient for a classification are called {\em pertinent positives}, and the features whose absence is necessary for the final classification are termed {\em pertinent negatives}. 
    To generate both pertinent positives and pertinent negatives, one needs to solve the optimization problem to find the minimum perturbations needed to maintain the same class label or change it, respectively. Therefore contrastive explanations (specifically pertinent negatives) are related to CFEs. 
    
    \item \emph{Adversarial learning}: Adversarial learning is closely related, but the terms are not interchangeable. Adversarial learning aims to generate the least amount of change in a given input to classify it differently, often with the goal of far-exceeding the decision boundary and resulting in a highly-confident misclassification. While the optimization problem is similar to the one posed in a counterfactual generation, the desiderata are different. For example, in adversarial learning (often applied to images), the goal is an imperceptible change in the input image. This is often at odds with the CFE's goal of sparsity and parsimony (though single-pixel attacks are an exception). Further, notions of data manifold and actionability/causality are rarely considerations in adversarial learning. A few works point to the similarity and synergy between the two domains: 
    \citet{pawelczyk2021_CFE_AE_connection} explore the connection between the optimization objectives and results of the adversarial and CFE generating techniques.
    \citet{freiesleben2020CFE_AE1} state that the differences in the desired class label and distance from the original datapoint distinguish CFEs from adversarial examples.
    \citet{Elliott2021_adversarial_cfe_images} propose generating semantically meaningful adversarial perturbations to generate CFEs for images. 
    \citet{semantic-explanation_adversarial_cfe_diff} point out that the constraint of producing plausible datapoints distinguishes CFEs from adversarial examples. 
      
\end{itemize}

 