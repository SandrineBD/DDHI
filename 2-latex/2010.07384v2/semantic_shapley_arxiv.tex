\documentclass{article}

\usepackage{iclr2021_conference,times}
\usepackage{hyperref}
\usepackage{url}

\usepackage{booktabs}
\usepackage{graphicx}
\usepackage[inline]{enumitem}
\usepackage{verbatim}
\usepackage{subcaption}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\DeclareRobustCommand{\Sec}[1]{Sec.~\ref{sec:#1}}
\DeclareRobustCommand{\Secs}[2]{Secs.~\ref{sec:#1} and \ref{sec:#2}}
\DeclareRobustCommand{\App}[1]{App.~\ref{app:#1}}
\DeclareRobustCommand{\Apps}[2]{Apps.~\ref{app:#1} and \ref{app:#2}}
\DeclareRobustCommand{\Tab}[1]{Table~\ref{tab:#1}}
\DeclareRobustCommand{\Tabs}[2]{Tables~\ref{tab:#1} and \ref{tab:#2}}
\DeclareRobustCommand{\Fig}[1]{Fig.~\ref{fig:#1}}
\DeclareRobustCommand{\Figs}[2]{Figs.~\ref{fig:#1} and \ref{fig:#2}}
\DeclareRobustCommand{\Eq}[1]{Eq.~(\ref{eq:#1})}
\DeclareRobustCommand{\Eqs}[2]{Eqs.~(\ref{eq:#1}) and (\ref{eq:#2})}
\DeclareRobustCommand{\Eqss}[3]{Eqs.~(\ref{eq:#1}), (\ref{eq:#2}), and (\ref{eq:#3})}
\DeclareRobustCommand{\Ref}[1]{Ref.~\cite{#1}}
\DeclareRobustCommand{\Refs}[2]{Refs.~\cite{#1} and \cite{#2}}
\DeclareRobustCommand{\Axiom}[1]{Axiom~\ref{ax:#1}}
\DeclareRobustCommand{\Axioms}[2]{Axioms~\ref{ax:#1} and \ref{ax:#2}}

% Equation shortcuts
\usepackage{amsmath,amssymb}
\newcommand{\eqn}[1]{\begin{align}#1\end{align}}
\newcommand{\eqna}[1]{\begin{align}\begin{aligned}#1\end{aligned}\end{align}}
\newcommand{\eqdef}{\overset{\rm def}{=}}
\allowdisplaybreaks

% Typesetting shortcuts
\newcommand{\bbE}{\mathbb{E}}

% Comments
\usepackage{color}
\definecolor{darkred}{rgb}{1.0,0.1,0.1}
\definecolor{darkgreen}{rgb}{0.1,0.7,0.1}
\definecolor{darkblue}{rgb}{0.1,0.1,1.0}
\definecolor{darkorange}{rgb}{1.0, 0.55, 0.0}
\newcommand{\CF}[1]{\textbf{\color{darkblue}[#1 --CF]}}
\newcommand{\IF}[1]{\textbf{\color{darkred}[#1 --IF]}}

% Lists
% \usepackage{enumitem}
% \setlist[enumerate]{leftmargin=6mm, label=(*)}
% \setlist[itemize]{leftmargin=6mm}

\iclrfinalcopy


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Human-interpretable model explainability\\on high-dimensional data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\author{%
\textbf{Damien de Mijolla\thanks{Authors contributed equally.}~, ~~ Christopher Frye$^*$\!, ~~ Markus Kunesch, ~~ John Mansir, ~\&~ Ilya Feige} \\[15pt]
Faculty, 54 Welbeck Street, London, UK
}


\begin{document}

\maketitle

\begin{abstract}
%
The importance of explainability in machine learning continues to grow, as both neural-network architectures and the data they model become increasingly complex. 
%
Unique challenges arise when a model's input features become high dimensional: on one hand, principled model-agnostic approaches to explainability become too computationally expensive; on the other, more efficient explainability algorithms lack natural interpretations for general users.
%
In this work, we introduce a framework for human-interpretable explainability on high-dimensional data, consisting of two modules.
%
First, we apply a semantically-meaningful latent representation, both to reduce the raw dimensionality of the data, and to ensure its human interpretability.
%
These latent features can be learnt, e.g.~explicitly as disentangled representations or implicitly through image-to-image translation, or they can be based on any computable quantities the user chooses.
%
Second, we adapt the Shapley paradigm for model-agnostic explainability to operate on these latent features. 
%
This leads to interpretable model explanations that are both theoretically-controlled and computationally-tractable.
%
We benchmark our approach on synthetic data and demonstrate its effectiveness on several image-classification tasks. 
%
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%


The explainability of AI systems is important, both for model development and model assurance. This importance continues to rise as AI models -- and the data on which they are trained -- become ever more complex. Moreover, methods for AI explainability must be adapted to maintain the human-interpretability of explanations in the regime of highly complex data.

Many explainability methods exist in the literature. Model-specific techniques refer to the internal structure of a model in formulating explanations \citep{chen2016xgboost, shrikumar2017learning}, while model-agnostic methods are based solely on input-output relationships and treat the model as a black-box \citep{breiman2001random, LIME}. Model-agnostic methods offer wide applicability and, importantly, fix a common language for explanations across different model types. 
    
The Shapley framework for model-agnostic explainability stands out, due to its theoretically principled foundation and incorporation of interaction effects between the data's features \citep{shapley52,SHAP}. The Shapley framework has been used for explainability in machine learning for years \citep{lipovetsky2001analysis, kononenko2010efficient, vstrumbelj2014explaining, datta2016algorithmic}. Unfortunately, the combinatorics required to capture interaction effects make Shapley values computationally intensive and thus ill-suited for high-dimensional data.

More computationally-efficient methods have been developed to explain model predictions on high-dimensional data. Gradient- and perturbation-based methods measure a model prediction's sensitivity to each of its raw input features \citep{GradCam,GlobalAveragePooling,PredDiffAnalysis}. Other methods estimate the mutual information between input features and the model's prediction \citep{pmlr-v80-chen18j,InformationBottleneckSaliency}, or generate counterfactual feature values that change the model's prediction \citep{HaoCounterfactual,CounterfactualVisualExplanations,CounterfactualClass}. See \Fig{pixel-baseline} for explanations produced by several of these methods (with details given in \Sec{celeba}).


\begin{figure}[!tbp]
  \centering
  \begin{minipage}[b]{0.57\textwidth}
    \includegraphics[width=\textwidth]{figures/pixel_baseline.pdf}
    % \vspace*{+1mm}
    \caption{Pixel-based explanations of a model trained to predict the attractiveness label in CelebA.}
    \label{fig:pixel-baseline}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.37\textwidth}
    \includegraphics[width=\textwidth]{figures/semantic-shapley.pdf}
    \caption{Our proposed framework for semantic explainability.}
    \label{fig:framework}
  \end{minipage}
\end{figure}


When intricately understood by the practitioner, these methods for model explainability can be useful, e.g.~for model development. However, many alternative methods exist to achieve broadly the same goal (i.e.~to monitor how outputs change as inputs vary) with alternative design choices that make their explanations uncomparable to a general user: e.g.~the distinct explanations in \Fig{pixel-baseline} describe the same model prediction. Ideally, a set of axioms (agreed upon or debated) would constrain the space of explanations, thus leading to a framework of curated methods that the user can choose from based on which axioms are relevant to the application. 
    
A further challenge on high-dimensional data is the sheer complexity of an explanation: in the methods described above, explanations have the same dimensionality as the data itself. Moreover, the importance of raw input features (e.g.~pixels) are not individually meaningful to the user. Even when structured patterns emerge in an explanation (e.g.~in \Fig{pixel-baseline}) this is not sufficient to answer higher-level questions. For example, did the subject's protected attributes (e.g.~gender, age, or ethnicity) have any influence on the model's decision?
    
In this work, we develop methods for explaining predictions in terms of a digestible number of semantically meaningful concepts. We provide several options for transforming from the high-dimensional raw features to a lower-dimensional latent space, which allow varying levels of user control. Regardless of the method used, transformation to a low-dimensional human-interpretable basis is a useful step, if explanations are to satisfy experts and non-experts alike.
    
Once a set of semantic latent features is selected, one must choose an explainability algorithm to obtain quantitative information about why a certain model prediction was made. Fortunately, since the set of latent features is low-dimensional by construction, a Shapley-based approach becomes once again viable. In this work, we develop a method to apply Shapley explainability at the level of semantic latent features, thus providing a theoretically-controlled, model-agnostic foundation for explainability on high-dimensional data.
Our main contributions are:
\begin{itemize}[leftmargin=15pt]
    %
    \item We introduce an approach to model explainability on high-dimensional data that involves encoding the raw input features into a digestible number of semantically meaningful latent features. We develop a procedure to apply Shapley explainability in this context, obtaining Shapley values that describe the high-dimensional model's dependence on each semantic latent feature. 
    %
    \item We demonstrate 3 methods to extract semantic features for the explanations: Fourier transforms, disentangled representations, and image-to-image translation. We benchmark our approach on dSprites -- with known latent space -- and showcase its effectiveness in computer vision tasks such as MNIST, CIFAR-10, ImageNet, Describable Textures, \mbox{and CelebA.}
    %
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Semantic Shapley explainability}
\label{sec:semantic-shapley}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


In this section, we present a simple modular framework for obtaining meaningful low-dimensional explanations of model predictions on high-dimensional data. The framework contains two modules: (i) a mechanism for transforming from the high-dimensional space of raw model inputs to a low-dimensional space of semantic latent features, and (ii) an algorithm for generating explanations of the model's predictions in terms of these semantic features. See \Fig{framework}.

We will begin by describing module (ii) in \Sec{shapley}, where we will show how to adapt Shapley  explainability to latent features. Then we will describe several options for module (i) in \Sec{semantic-reps}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Shapley values for latent features}
\label{sec:shapley}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Shapley values \citep{shapley52} were developed in cooperative game theory to distribute the value $v(N)$ earned by a team $N = \{1, 2, \ldots, n\}$ among its players. The Shapley value $\phi_v(i)$ represents the marginal value added by player $i$ upon joining the team, averaged over all orderings in which the team can form. In particular,
\eqn{
\label{eq:shapley}
\phi_v(i) = \sum_{S \subseteq N \setminus \{i\}}
\frac{|S|! \, (n-|S|-1)!}{n!} \,
\big[ v(S \cup \{i\}) - v(S) \big]
}
where $v(S)$ represents the value that a coalition $S$ obtains without the rest of their teammates. Shapley values are the unique attribution method satisfying 4 natural axioms \citep{shapley52}. For example, they sum to the total value earned: $\sum_i \phi_v(i) = v(N) - v(\{\})$, and they are symmetric if two players $i$ and $j$ are functionally interchangeable: $\phi_v(i) = \phi_v(j)$. Shapley values thus serve as a well-founded explanation of an output (the earned value) in terms of inputs (the players).

The method can be adapted to explain the output of a machine learning model by interpreting the model's input features $x = (x_1, \ldots, x_n)$ as the players of a game. Consider a classification task, and let $f_y(x)$ be the model's predicted probability that data point $x$ belongs to class $y$. To apply Shapley explainability, one must define a value function representing the model's expected output given only a subset of the input features $x_S$. The most common choice is
\eqn{
\label{eq:value-function}
v_{f_y(x)}(S) = \mathbb E_{p(x')} \big[ f_y(x_S \sqcup x'_{\bar S}) \big]
}
where $p(x')$ is the distribution from which the data is drawn, $\bar S$ is the complement of $S$, and $x_S \sqcup x'_{\bar S}$ represents the spliced data point with in-coalition features from $x$ and out-of-coalition features from $x'$. Then, inserting the value function of \Eq{value-function} into the definition of \Eq{shapley}, one obtains Shapley values $\phi_{f_y(x)}(i)$ representing the portion of the prediction $f_y(x)$ attributable to feature $x_i$.

The Shapley values presented above provide a \emph{local} explanation of the model's behaviour on an individual data point. For a \emph{global} explanation, local values can be aggregated \citep{frye2020shapley}
\eqn{
\label{eq:global-shapley}
\Phi_f(i) = \mathbb E_{p(x, y)} \big[ \phi_{f_y(x)}(i) \big]
}
where $p(x, y)$ is the joint distribution from which the labelled data is drawn. This aggregation preserves the Shapley axioms and is motivated by the sum rule:
\eqn{
\sum_i \Phi_f(i) = \mathbb E_{p(x, y)} \big[f_y(x)\big] - \mathbb E_{p(x')p(y)} \big[f_y(x')\big]
}
which can be interpreted as the model accuracy above a class-balance baseline. Global Shapley values thus represent the portion of model accuracy attributable to each feature. 

To adapt the Shapley framework to latent features, suppose (as in \Fig{framework}) that a mapping $x \to z$ exists to transform the raw model inputs $x$ into a semantically meaningful representation $z(x)$, and that an (approximate) inverse mapping $z \to \tilde x$ exists as well. Then we can obtain an explanation of the model prediction $f_y(x)$ in terms of the latent features $z(x)$ by applying Shapley explainability instead to the function $f_y(\tilde x(z))$ at the point $z = z(x)$. To be precise, we define a value function
\eqn{
\label{eq:latent-value}
\tilde v_{f_y(x)}(S) = \mathbb E_{p(x')} \Bigg[\, 
    f_y \Big( 
        \tilde x \big(\,
            z_S(x) \sqcup z_{\bar S}(x')
        \,\big)
    \Big) 
\,\Bigg]
}
which represents the marginalisation of $f_y(\tilde x(z))$ over out-of-coalition features $z_{\bar S}$.
Here $z_S(x)$ is the in-coalition slice of $z(x)$, and $z_{\bar S}(x')$ is the out-of-coalition slice corresponding to a different data point. These get spliced together in latent space before transforming back to model-input-space and feeding into the model. Inserting the value function of \Eq{latent-value} into the definition of \Eq{shapley} produces semantic Shapley values that explain $f_y(x)$ in terms of latent features $z_i$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Landscape of semantic representations}
\label{sec:semantic-reps}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


A wide variety of methods exist to transform from the high-dimensional set of raw model inputs to an alternative set of features that offer semantic insight into the data being modelled. In this section, we consider several options for this semantic component of our approach to explainability.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Fourier transforms}
\label{sec:fourier}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Despite their high dimensionality, pixel-based explanations as in \Fig{pixel-baseline} manage to convey meaning to their consumers through the local structure of images. A central claim of our work, however, is that such meaning remains limited and incomplete. One way to complement the location information of pixel-based explanations is with frequency-based explanations via Fourier transforms.

If an image consists of a value $x_i$ for each pixel $i = 1, \ldots, n$, then its discrete Fourier transform \citep{fft} consists of a value $z_k$ for each Fourier mode $k = 1, \ldots, n$. Each mode $k$ corresponds to a set of frequencies in the horizontal and vertical directions, ranging from 0 (uniform imagery) to $1/2$ (one oscillation every 2 pixels). As the inverse Fourier transform $\tilde x(z(x)) = x$ exists, we can obtain Shapley values for the Fourier modes of an image using \Eq{latent-value}. 

We can also aggregate Fourier-based Shapley values into frequency bins to reduce the complexity of the calculation and lower the dimensionality of the explanation. Such frequency-based explanations can offer insight into whether a model's decisions are based on shapes or textures (see \Sec{textures}) and whether a model is robust to adversarial examples (see \Sec{cifar}).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Disentangled representations}
\label{sec:disentanglement}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


The goal of disentangled representation learning \citep{Bengio,Schmidhuber} is to learn a mapping from a data set's raw high-dimensional features to a lower-dimensional basis of semantically meaningful factors of variation. In an image, these factors of variation might correspond to the subject's position and orientation, their emotional state, the lighting, and the setting. Disentangled representations are highly aligned with our goal of achieving semantic explanations.

Different approaches offer varying levels of control over which semantic features are learnt in the representation. Unsupervised disentanglement extracts factors of variation directly from the data. Such methods are often based on variational inference \citep{BetaVAEH,BetaVAEB,FactorVAE,Btcvae} -- though other approaches exist \citep{InfoGAN} -- and often seek a factorised latent representation. 
Supervised disentanglement \citep{Schmidhuber,FaderNetworks} allows the practitioner to specify a subset of the sought-after semantic features by labelling them -- in full or in part \citep{WeaklySupervisedDisentanglement} -- in the data. Such methods then involve learning a representation that factorises the specified and unspecified factors of variation.

Disentangled representations based on variational autoencoders \citep{VAEKingma} include an encoder $z(x)$ and a decoder $\tilde x(z)$, thus fitting neatly into our framework (\Fig{framework}) for semantic explainability. The value function of \Eq{latent-value} then leads to Shapley values that explain a model's predictions in terms of the disentangled factors of variation underlying the data. We will demonstrate this for unsupervised (\Sec{dsprites}) and supervised (\Sec{mnist}) disentanglement with experiments.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Image-to-image translation}
\label{sec:translation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


In image-to-image translation \citep{image2image}, one is not interested in directly extracting semantic factors of variation, but instead in transforming images by selectively modifying underlying semantic features. Generative adversarial methods can accomplish this goal without passing through an explicit compressed representation \citep{CycleGAN}. Other adversarial methods allow the user to selectively perturb the semantic attributes of an image (e.g. hair colour or gender) for attributes that are labelled in the data set \citep{stargan,Attgan,Stgan}. 

Image-to-image translation methods can straightforwardly be incorporated into our framework for semantic explainability. To do so, one inserts the result of the translation $x \to \tilde x \big(z_S(x) \sqcup z_{\bar S}(x')\big)$, which corresponds to the modification of semantic attributes $z_{\bar S}(x) \to z_{\bar S}(x')$, into the value function of \Eq{latent-value}. We demonstrate this in \Sec{celeba} below.


%%%%%%%%%%%%%%%%%%%
\section{Results}
\label{sec:results}
%%%%%%%%%%%%%%%%%%%


Here we demonstrate the practical utility of our approach on a variety of data sets and for a diverse set of semantic representations. We use pre-trained models for the semantic representations to show that our method can be applied with existing tools. See \App{details} for full experimental details.


%%%%%%%%%%%%%%%%%%%%%
\subsection{CIFAR-10}
\label{sec:cifar}
%%%%%%%%%%%%%%%%%%%%%


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/cifar.pdf}
    \caption{Fourier-space model explanations on CIFAR-10 shedding light on adversarial sensitivity.}
    \label{fig:cifar}
\end{figure}

We begin by applying our explainability framework using the Fourier transform as the semantic representation. We do this on CIFAR-10 \citep{cifar10} and investigate whether sensitivity to adversarial examples \citep{adversarialexamplesdiscovery,adversarialexamplesexplainharness,networkFooling} is linked to dependence on high-frequency (i.e.~small length-scale) fluctuations. We considered two classifiers on CIFAR-10: a robust model \citep{robustPaper,robustness_github} trained to be insensitive to adversarial perturbations, and a non-robust model \citep{resnet} trained naturally. We computed semantic Shapley values according to \Sec{fourier}.

The first row of \Fig{cifar} shows the semantic explanations that result for a single image. The Shapley values in Fourier space show that, on this particular image, the non-robust model is sensitive to high-frequency features (less detectable by the human eye), while the robust prediction is based exclusively on low-frequency information near the Fourier-space origin (large-scale structure). The 1-dimensional comparison, in which frequency modes were binned according to Euclidean norm, allows for quantitative comparison that confirms this effect.

We also computed the adversarial perturbation of the local image in \Fig{cifar}, using projected gradient descent \citep{NoceWrig06} and $\epsilon = 8/255$ (separately for each model). Such adversarial perturbations do not significantly alter the image to the human eye, and the robust model's prediction accordingly remains unchanged; however, the non-robust model's prediction is perturbed from ``horse'' to ``ship''. \Fig{cifar} explains the model decisions on these adversarially perturbed images, showing that the non-robust model's mistake is due to high-frequency features.

The second row of \Fig{cifar} shows global semantic explanations for these models, which correspond to aggregating local explanations across the data set. We see that the trends found above for one particular image hold in general throughout the data. Our framework for semantic explainability thus leads to the interesting result that adversarially robust classifiers are less sensitive to high-frequency information. See \App{imagenet} for similar results on ImageNet \citep{imagenet09}. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Describable Textures}
\label{sec:textures}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Here we apply our framework -- again using the Fourier transform -- to explain whether a model's predictions are based on shapes or textures. Shapes tend to correspond to extended objects and thus sit at the lower end of the frequency spectrum. Textures correspond to small-scale patterns and thus occupy the higher end of the frequency spectrum, often with distinctive peaks that represent a periodic structure. We explore this question by explaining a ResNet-50 \citep{resnet} trained on the Describable Textures Dataset \citep{describablestextures}. 

\Fig{textures} shows randomly drawn ``banded'' and ``woven'' images from the data set, as well as their pixel-based explanations computed with integrated gradients \citep{IntegratedGradients}. At the level of pixels, it is difficult to judge what qualities (e.g.~colours, shapes, textures) drive the model's prediction. However, the frequency-based explanations in \Fig{textures} show clear peaks at high frequencies corresponding to regular patterns in each image. See \App{textures} for additional examples. 


%%%%%%%%%%%%%%%%%%%%%
\subsection{dSprites}
\label{sec:dsprites}
%%%%%%%%%%%%%%%%%%%%%


\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth]{figures/textures.pdf}
    \caption{Frequency-based explanations on Describable Textures, showing sensitivity to periodicity.}
    \label{fig:textures}
\end{figure}


Here we apply our explainability framework using unsupervised disentangled representations to extract the semantic features. We do this on dSprites \citep{dsprites17}, synthetic images of sprites (see \Fig{dsprites}) generated from a known latent space with 5 dimensions: shape, scale, orientation, and horizontal and vertical positions. This experiment serves as a benchmark of our approach.

In this experiment, we explain a rules-based model that classifies sprites according to the number of white pixels in the top half of the image (with 6 classes corresponding to 6 latent scales). We articulate the explanation in terms of the unsupervised disentangled representation (cf.~\Sec{disentanglement}) of a $\beta$-TCVAE \citep{Btcvae}, using a publicly available model \citep{yanndubs2019}.

\Fig{dsprites} shows semantic Shapley values for this model. Globally, the model relies on the sprite's vertical position and scale, while generally ignoring its shape, orientation, and horizontal position. This is indeed consistent with the rules-based model we laid out above. Locally, the model classifies the sprite as $y=0$, as it has no white pixels in the top half of the image. The vertical position of the sprite is the main driver of this decision. In fact, the sprite's scale (maximum) would tend to indicate a different class, so the scale receives a negative local Shapley value in this case. 

This experiment validates our framework for semantic explainability, as the modelling task and factors of variation in the data are fully understood and consistent with our results. Moreover, this example showcases an explanation that differentiates between shape and scale -- semantic features that cannot be distinguished in a pixel-based explanation.


%%%%%%%%%%%%%%%%%%
\subsection{MNIST}
\label{sec:mnist}
%%%%%%%%%%%%%%%%%%


Here we apply our framework -- again using disentangled representations -- to a binary classifier on MNIST \citep{mnist2010}. In particular, we explain a rules-based model that detects the presence of a geometric hole, which exists when all black pixels are not contiguous. We use the unsupervised disentangled representation of a JointVAE pre-trained on MNIST \citep{JointVAE}, which accommodates a combination of continuous (e.g.~handwriting) and discrete (e.g.~digit value) latent dimensions. Supervised methods also exist for disentangling the digit value in the latent representation \citep{Adversarialautoencoder}.

\Fig{mnist} shows semantic Shapley values for the rules-based model. The global values show that the digit value has the most bearing on the model's general behaviour; indeed, some digits (0, 6, 8, 9) almost always contain a geometric hole while others (1, 3, 5, 7) almost always do not. The global values also show sensitivity to writing style (e.g.~$4$ vs $\mathit{4}$) and stroke thickness (which can close a hole) as expected. The local values in \Fig{mnist} are roughly in line with the global trends.

This example demonstrates that latent disentanglement is a powerful tool for understanding model decisions at a semantic level. These results also highlight that explanations are only as good as the chosen semantic representation. One can see in the latent traversals of \Fig{mnist} that in this representation, semantic concepts are not perfectly disentangled: stroke thickness and hole size are entangled, and writing style mixes 7's and 9's. This can lead to a small corruption of the explanation.


\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/dsprites.pdf}
        \caption{}
        \label{fig:dsprites}
    \end{subfigure}
    \hskip 5mm
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/mnist.pdf}
        \caption{}
        \label{fig:mnist}
    \end{subfigure}
    \caption{Model explanations in terms of disentangled latent features on (a) dSprites and (b) MNIST.}
    \label{fig:dsprites_mnist}
\end{figure}


%%%%%%%%%%%%%%%%%%%
\subsection{CelebA}
\label{sec:celeba}
%%%%%%%%%%%%%%%%%%%


Here we apply our explainability framework using image-to-image translation as the implicit semantic representation. We do this on CelebA \citep{celeba15} to demonstrate, on real world data, that our method elucidates patterns in model behaviour that are missed by other methods. In particular, we train a CNN  to predict the labelled attractiveness of individuals in the data. We choose this admittedly banal label because it is influenced by a variety of other higher level attributes, including sensitive characteristics (e.g.~gender, age, skin tone) that should not influence fair model predictions. Pixel-based explanations (e.g.~\Fig{pixel-baseline}) provide no insight into these important issues.

We explain the model's predictions in terms of other semantic attributes also labelled in the data. We do so according to \Sec{translation}, using the implicit representation of an STGAN \citep{Stgan} pre-trained on these other attributes \citep{stgan_github}. \Fig{celeba} shows the resulting semantic Shapley values explaining the model's prediction of $y=0$ for an individual in the data. The ``unlabelled'' bar represents remaining factors of variation not captured by labelled attributes. 

Globally, the model makes significant use of age, eyeglasses, and blond hair. This reflects stereotypical western-cultural conceptions of beauty, not surprising in a database of celebrities. The attribute with the largest influence is gender, a result of bias in the data set: women are more than twice as likely to be labelled attractive as men. Locally, the subject's gender (male) thus increases the tendency for the model to predict $y=0$. The subject's age (young) receives a negative local Shapley value, as this correlates instead with $y=1$ (opposite the model's prediction) in the data. Hair colour, smile, and eyebrows also played a role in the model prediction for this individual.

Interestingly, our explanation captures dependence on features barely perceptible to the human eye. For example, while traversals of the ``young'' attribute are hardly noticeable for this individual, this feature significantly impacts the model prediction. Model evaluations on traversals of this feature confirm that this is a genuine pattern exploited by the model and not an aberration in the explanation. Figs.~\ref{fig:celeba_traversal}\;--\,\ref{fig:shapley_mouth} in \App{celeba} provide explicit comparisons between Shapley explanations and model evaluations over a  latent-space grid, as well as additional validation of our CelebA experiment.


\begin{figure}[!t]
\centering
\includegraphics[width=0.9\textwidth]{figures/celeba.pdf}
\caption{Semantic explanations of a model predicting the attractiveness label in CelebA.}
\centering
\label{fig:celeba}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:related-work}
%%%%%%%%%%%%%%%%%%%%%%%%


In this work, we have focused on explaining model predictions in terms of semantic latent features rather than the model's raw inputs. In related work, natural language processing is leveraged to produce interpretable model explanations textually \citep{GroundingVisualExplantions,NLPexplanation}. Other methods \citep{ConceptActivationVectors,InterpretableBasisDecomposition} learn to associate patterns of neuron activations with semantic concepts. However, each of these requires large sets of annotated data for training -- a barrier to widespread application -- whereas we offer both unsupervised and analytic options for semantic explainability in our approach. Orthogonal efforts exist to train high-capacity models that are intrinsically interpretable \citep{SelfExplainingNN,ThisLooksLikesThat}. 

Other related works leverage generative models to produce explanations. Several methods generate counterfactual model inputs (e.g.~images) with the features that led to the model prediction accentuated \citep{ExplanationByExageration,ExplainGAN,AuditingClassifiers}. In contrast to our work, such explanations are pixel-based. Techniques developed by \cite{CounterfactualIntrospection} generate counterfactual images that cross class boundaries while remaining nearby the original image. When employed using a disentangled generative model, such methods can assign importance to latent features; however, they aim to minimise an ad-hoc pixel-based distance metric, whereas semantic latent features generally control large-scale changes in an image.

A recent workshop \citep{singh2020transformation} showed that a change of basis in model explanations can offer insights in cosmology applications. Complementary to this work, we develop this idea in general, offer several alternatives for the basis change, and benchmark with extensive experiments. 

Recently released work-in-progress \citep{frequencyexplanations} studies the dependence of adversarially robust models on the frequency modes of an image's Fourier transform. While taking a somewhat different approach (e.g.~explaining based on ``Occluded Frequencies'') the study finds results generally consistent with ours in \Sec{cifar}: adversarial sensitivity is primarily a high-frequency phenomenon.


%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{sec:conclusion}
%%%%%%%%%%%%%%%%%%%%%%


In this work, we introduced an approach to model explainability on high-dimensional data, in which explanations are articulated in terms of a digestible set of semantic latent features. We adapted the Shapley paradigm to this task, in order to attribute a model's prediction to the latent features underlying its input data. These two developments form a principled, flexible framework for human-interpretable explainability on complex models. To demonstrate its flexibility, we highlighted Fourier transforms, latent disentanglement, and image-to-image translation as options for the semantic representation that offer varying levels of user control. We benchmarked our method on synthetic data, where the underlying latent features are controlled, and demonstrated its effectiveness in an extensive set of experiments using off-the-shelf pretrained models. We hope this framework will find wide applicability and offer practitioners a new way to probe their models.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


This work was developed and experiments were run on the Faculty Platform for machine learning. 
The authors benefited from conversations with Tom Begley and Tobias Schwedes. 
The authors are grateful to Jaan Tallinn for funding this work.
DDM was also partially supported by UCL's Centre for Doctoral Training in Data Intensive Science.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{semantic_shapley}
\bibliographystyle{iclr2021_conference}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\section{Details of experiments and additional results}
\label{app:details}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Below we provide numerical details and  supplementary results for the experiments presented in this paper. For each experiment, we describe both the semantic representation employed as well as the Shapley calculation and its statistical uncertainty.

First we mention the pixel-based explanations that appear in \Figs{pixel-baseline}{textures} (and \Fig{pixel_mouth} below). We produced these using the Captum PyTorch package \url{[github.com/pytorch/captum]}. We refer the reader to the package documentation for additional details regarding our chosen explanation methods. Explanations using integrated gradients, occlusion, and feature ablation were calculated using a zero-valued baseline.


%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{CIFAR-10}
\label{app:cifar}
%%%%%%%%%%%%%%%%%%%%%%%%


For our experiments on CIFAR-10 \citep{cifar10}, we explained the predictions of pre-trained robust and non-robust ResNet-50's that are publicly available \citep{robustness_github}. The robust model in particular was adversarially (pre-)trained with respect to $\ell_\infty$ perturbations of norm $\varepsilon = 8/255$. For the semantic representation, we used the FFT \citep{fft} functionality of NumPy \citep{numpy}.

We computed local Shapley values according to \Sec{shapley}, using 10k Monte-Carlo samples from the validation set to estimate the local Shapley value for each pixel. The colour plots in \Fig{cifar} display the means of these Monte-Carlo estimations. (We treated the RGB components of each Fourier mode as a single feature $z_k$ in these calculations. We similarly grouped together complex-conjugate modes, $z_k$ and $z_{k^*}$, as these provide redundant information for real-valued images.) The global Shapley values were similarly computed with 10k samples.

To produce the 1d-comparison plots of \Fig{cifar}, we aggregated Shapley values from the colour plots, binning Shapley values according to the $\ell_2$ norm of the corresponding 2d-frequencies. The shaded bands around the curves display the standard error of the mean in the Monte Carlo sampling discussed above. (These uncertainty bands are very narrow.)

In the final column of \Fig{cifar}, we explain model predictions on adversarially perturbed data. We computed these perturbations according to \cite{robustness_github}. In particular, we performed projected gradient descent with respect to an $\ell_\infty$ norm of $\varepsilon = 8/255$, using a step size of 0.01 and 100 iterations towards a targeted, randomly-drawn incorrect class. Each (robust and non-robust) model was evaluated on a separate set of direct adversarial attacks. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Restricted ImageNet}
\label{app:imagenet}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Here we present supplementary results on Restricted ImageNet \citep{robustness_github}, confirming that the trends found on CIFAR-10 also exist for higher-dimensional images. In particular, we explain the predictions of pre-trained robust and non-robust ResNet-50's \citep{robustness_github}; the robust model was adversarially trained with $\ell_\infty$ perturbations of norm $\varepsilon = 4/255$.

In \Fig{imagenet} we present results analogous to those in \Fig{cifar}. In the first row, we show frequency-based Shapley values for each model's prediction on a local image. There we see that the robust model primarily depends on low-frequency information in the image. By contrast, the non-robust model is sensitive to higher-frequency features. This sensitivity leads to the false classification of the adversarially perturbed image of the primate as an insect. In the second row, we show that this trend persists across the data set: the global Shapley values aggregated over many images display trends similar to those just described for an individual image.

For this calculation, we treated all the Fourier modes that fall into the same frequency bin as a single feature in the Shapley calculation. This reduced the $224 \times 224$ Fourier modes down to 25 aggregate features, thus enabling a very tractable calculation that preserves the primary information-of-interest in the explanation. All other numerical details of our experiment on Restricted ImageNet are identical to those on CIFAR-10, but replacing $8/255 \to 4/255$ for the adversarial perturbations.


\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{figures/imagenet.pdf}
\caption{Frequency-based explanations on Restricted ImageNet, providing further evidence that adversarial sensitivity is linked to high-frequency modes.}
\label{fig:imagenet}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Describable Textures}
\label{app:textures}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/textures2.pdf}
    \caption{Additional frequency-based explanations on the  Describable Textures data set.}
    \label{fig:textures2}
\end{figure}


For Describable Textures \citep{describablestextures}, we explained a ResNet-101 \citep{resnet} trained to predict the 47 textures classes. Our model was optimised using Adam and obtained a top-1 accuracy of 53.5\% and a top-5 accuracy of 83.1\% on a held-out test set.

Explanations in \Fig{textures} were computed according to the same procedure outlined in \App{imagenet}. In particular, 10k Monte Carlo samples were used to estimate the local Shapley value in each frequency bin, and narrow uncertainty bands display the standard error of the mean in each bin. \Fig{textures2} provides additional local explanations of the model on randomly-drawn ``spiralled'', ``perforated'', and ``stratified'' images. These show clear dependence on specific high-frequency patterns in the images. 


%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{dSprites}
\label{app:dsprites}
%%%%%%%%%%%%%%%%%%%%%%%%


For our experiments on dSprites \citep{dsprites17}, we explained the output of a rules-based model. The model counts the number of white pixels in the top half of the image in order to classify images into 6 classes. The white-pixel-counts naturally fall into 6 clusters because the synthetic dSprites images are generated according to 6 underlying scales.

For the semantic representation, we employed a $\beta$-TCVAE \citep{BetaVAEB}, obtaining a publicly available pre-trained network \citep{yanndubs2019}. We applied a rotation matrix to the latent representation to align translation-dimensions with the horizontal and vertical axes.

We computed explanations for \Fig{dsprites} according to \Sec{shapley}. Each Shapley value was estimated using 10k Monte Carlo samples: each bar heights represents the mean, and each error bar displays the standard error of the mean.


%%%%%%%%%%%%%%%%%%%%%
\subsection{MNIST}
\label{app:mnist}
%%%%%%%%%%%%%%%%%%%%%


For our experiments on MNIST \citep{mnist2010}, we explained the output of a rules-based model that checks for the presence of a geometric hole in an image. Such a hole exists if there is a black pixel that is not path-connected to the image's perimeter, restricting to paths that only pass through other black pixels. For example, ``0'' has a hole, because the black pixels at the centre are disconnected from the black pixels near the perimeter, but ``1'' does not. In practice, we detected holes using SciPy's \texttt{binary\_fill\_holes} function.

For the semantic representation, we employed a JointVAE \citep{JointVAE}, using the pre-trained network available at the paper's associated repository. Explanations in \Fig{mnist} were computed using 10k Monte Carlo samples to estimate the means and standard errors.


%%%%%%%%%%%%%%%%%%%%%%
\subsection{CelebA}
\label{app:celeba}
%%%%%%%%%%%%%%%%%%%%%%


\begin{figure}[!t]
\centering
\includegraphics[width=0.8\textwidth]{figures/celeba_traversals.pdf}
\caption{Model dependence on each labelled attribute in CelebA. Below each image in the latent traversals, we show the model's corresponding output. The model was trained to predict the ``attractive'' label in the data. Its dependence here is consistent with the Shapley explanation in \Fig{celeba}.}
\label{fig:celeba_traversal}
\vskip 4mm
\end{figure}


\begin{figure}[!t]
\centering
\includegraphics[width=0.8\textwidth]{figures/celeba2.pdf}
\caption{Semantic Shapley explanation of a model predicting the ``attractive'' label in CelebA. Additional local explanation to complement the one shown in \Fig{celeba} of the main text.}
\label{fig:celeba_additional}
\vskip 4mm
\end{figure}


For our experiments on CelebA \citep{celeba15}, we pre-processed the data according to the procedure described by \cite{Stgan}. This entails cropping the central $170 \times 170$ region of the images and using a bicubic linear interpolation to resize them to $128 \times 128$. We explained the predictions of a CNN trained to predict the ``attractive'' label, which exhibits a 51\,:\,49 class balance in the data. Our CNN (with 3 convolutional and 2 fully-connected layers) was optimised using Adam and the cross-entropy loss to achieve 78.7\% accuracy on a held-out test set.

For the semantic representation, we employed an STGAN \citep{Stgan}, using the pre-trained network available at the paper's associated repository. This network was trained to selectively modify the labelled attributes listed in \Fig{celeba}. Our Shapley explanations also include an ``unlabelled'' dimension: this is meant to represent all remaining factors of variation distinguishing one image from another that are not captured by the labelled attributes. In practice, this unlabelled dimension is simply the image's index in the data set. To vary an image along this unlabelled dimension, one simply draws other images from the data set while using the STGAN to hold their labelled attributes -- hair colour, age, gender, etc. -- fixed. Shapley values for CelebA were computed using 5k Monte Carlo samples to estimate the mean and standard error for each feature.

In \Fig{celeba_traversal}, we provide results that validate the hierarchy of Shapley values that appears in \Fig{celeba}. Below each image in the latent traversals of \Fig{celeba_traversal}, we  display the CNN's predicted probability that $y=1$. This shows that the model indeed depends strongly on the rightmost features while remaining relatively insensitive to the leftmost features. While the Shapley explanation of the CNN's prediction also takes into account feature interactions that are not shown here, the model evaluations in \Fig{celeba} provide a qualitative check on the Shapley values. 

\Fig{celeba_additional} provides an additional explanation of the CNN evaluated on a different image in the data set. The model predicts $y=1$ for this individual, and this is attributed in large part to the unlabelled factors of variation. The subject's gender (male) anti-correlates with the model's prediction (attractive), while his age (young) is a positive predictor of the model's behaviour.

As a further sanity check on our CelebA explanations, we present supplementary results here explaining a model that predicts the ``mouth slightly open'' label, which exhibits a 48\,:\,52 class balance in the data. Training our CNN architecture on this task resulted in 91.8\% accuracy on the held-out test set. The pixel-based explanations of this classifier are shown in \Fig{pixel_mouth}. Since (in contrast to the ``attractive'' label) this is a visually-localised prediction task, the pixel-based explanations are sensible, highlighting the region surrounding the subject's mouth. Furthermore, the semantic Shapley explanation of this classifier (\Fig{shapley_mouth}) attributes the prediction primarily to the ``mouth slightly open'' latent feature. Since this is indeed the label that the model was trained to predict, this validates our setup for computing semantic explanations on CelebA.

\newpage
~

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{figures/pixel_baseline_mouth.pdf}
\caption{Pixel-based explanations of classifier that predicts ``mouth slightly open'' label in CelebA.}
\label{fig:pixel_mouth}
\vspace{1cm}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[width=0.81\textwidth]{figures/celeba_mouth.pdf}
\caption{Semantic explanations of classifier that predicts ``mouth slightly open'' in CelebA.}
\label{fig:shapley_mouth}
\end{figure}


%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%
