\section{Burgeoning legal frameworks around explanations in AI}
\label{sec:legal}
% Right to Explanation~\citep{wachter_counterfactual_2017}
To increase the accountability of automated decision systems---specifically, AI systems---laws and regulations regarding the decisions produced by such systems have been proposed and implemented across the globe~\citep{AI-Accountability:2017}. 
The most recent version of the European Union's General Data Protection Regulation (GDPR), enforced starting on May 25, 2018, offered a right to information about the existence, logic, and envisaged consequences of such a system~\citep{Euro-GDPR2}. This also includes the right to not be a subject of an automated decision-making system. 
Although the closeness of this law to ``right to explanation'' is debatable and ambiguous~\citep{right-to-explanation-gdpr}, the official interpretation by Working Party for Article 29 has concluded that the GDPR requires explanations of specific decisions, and therefore counterfactual explanations are apt. 
In the US, the Equal Credit Opportunity Act (ECOA) and the Fair Credit Reporting Act (FCRA) require the creditor to inform the reasons for an adverse action, such as rejection of a loan request~\citep{ECOA1,ECOA2}. They generally compare the applicant's feature to the average value in the population to arrive at the principal reasons. 
Government reports from the United Kingdom~\citep{UK-XAI1} and France~\citep{Fr-XAI1,Fr-XAI2} also touched on the issue of explainability in AI systems. 
In the US, Defense Advanced Research Projects Agency (DARPA) launched the Explainable AI (XAI) program in 2016 to encourage research into designing explainable models, understanding the psychological requirements of explanations, and the design of explanation interfaces~\citep{DARPA1}. 
The European Union has taken similar initiatives as well~\citep{EU-XAIfund1,EU-XAIfund2}. 
The US White House recently put forward the Blueprint for an AI Bill of Rights \citep{blueprints-bill-of-rights} to modulate decisions from automated systems. 
The Bill outlines five principles for operating such systems: 1) safe and effective systems, 2) algorithmic discrimination protections, 3) data privacy, 4) explanations for decisions made using such systems, and 5) discussion about human alternatives. 
While many techniques have been proposed for explainable machine learning, it is yet unclear if and how these specific techniques can help address the letter of the law. Future collaboration between AI researchers, regulators, the legal community, and consumer watchdog groups will help ensure the development of trustworthy AI.

