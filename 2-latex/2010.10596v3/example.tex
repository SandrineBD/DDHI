\section{Counterfactual Explanations}
This section illustrates counterfactual explanations by giving an example and then outlines the major aspects of the problem. 

\subsection{An Example}
Suppose Alice walks into a bank and seeks a home mortgage loan. The decision is impacted in large part by a machine learning classifier that considers Alice's feature vector of \{\emph{Income}, \emph{CreditScore}, \emph{Education}, \emph{Age}\}. Unfortunately, Alice is denied the loan she seeks and is left wondering (1) why the loan was denied? and (2) what can she do differently so that the loan will be approved in the future? The former question might be answered with explanations like: ``CreditScore was too low'', and is similar to the majority of traditional explainability methods. The latter question forms the basis of a \emph{counterfactual explanation}: what small changes could be made to Alice's feature vector in order to end up on the other side of the classifier's decision boundary? Let us suppose the bank provides Alice with exactly this advice (through a CFE) of what she might change in order to be approved next time. 
A possible counterfactual recommended by the system might be to increase her \emph{Income} by $\$10$K or get a new master's degree or a combination of both. The answer to the former question does not tell Alice what action to take, while the CFE explicitly helps her. 
\Cref{fig:cf_fig} illustrates how the datapoint representing an individual, which originally got classified in the negative class, can take two paths to cross the decision boundary into the positive class region. 

The assumption in a CFE is that the underlying classifier would not change when the applicant applies in the future. And if the assumption holds, the counterfactual guarantees the desired outcome in the future time. 

