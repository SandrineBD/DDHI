\section{Background}
\label{sec:background}
% write the motivation here
This section gives the background about the social implications of machine learning, explainability research in machine learning, and some prior studies about counterfactual explanations. 

% \subsection{Primer on Machine Learning}

% \jpd{John to look in depth}
% Machine learning is a set of algorithms that infer rules for a task from example data. 
% It can be broadly categorized into:
% \begin{itemize}[leftmargin=*]
%     \item Supervised learning: This setup consists of several labeled datapoints which are given as input to the algorithm, and the goal is to learn a function mapping from the input datapoints (with m features) to labels. $\mathcal{X}^m$ is used to denote the input space, and $\mathcal{Y}$ is used to denote the output space. The learned function is the mapping $f : \mathcal{X}^m \to \mathcal{Y}$. This learned function can be used to predict labels for unseen datapoints in future. This can be further categorized into classification and regression. In classification, the labels are discrete categories, whereas in regression the labels are continuous values. 
%     \item Unsupervised learning: In this setup, unlabeled datapoints are given as input and the goal of the algorithm is to learn latent structure in the data. For example, clustering similar datapoints together. 
%     \item Reinforcement learning: In this case, data is not input directly to the algorithms. This setup consists of an agent which performs actions to interact with the environment, based on which it gets rewards. The goal is to learn a function which predicts the most rewarding action at any state of the agent's world. 
% \end{itemize}


% Counterfactual explanations, by definition, are applicable to supervised learning setup where a desired label has not been obtained for a datapoint. 
% The majority of research in this area has applied counterfactual explanations to classification settings. 

% \subsection{Counterfactual in other fields} - Written in the section "Scope of the review"

% Mention works by Lewis philosopher in 1973 and David Kahnemann, which said counterfactuals elicit causal thinking. 

\subsection{Social Implications of Machine Learning}
Establishing fairness and making an automated tool's decision explainable are two broad ways in which we can ensure equitable social implications of machine learning. 
Fairness research aims at developing algorithms that can ensure that the decisions produced by the system are not biased against a particular demographic group of individuals, which are defined with respect to sensitive or protected features, such as race, sex, and religion.
Anti-discrimination laws make it illegal to use sensitive features as the basis of any decision (see \Cref{sec:legal}). Biased decisions can also attract widespread criticism and are therefore crucial to avoid~\cite{prime-racist,apple-sexist}. 
Fairness has been captured in several notions based on a demographic grouping or individual capacity. 
\citet{verma_fairness} have enumerated and intuitively explained many fairness definitions using a unifying dataset. 
\citet{dunkelau_fairness-aware} provide an extensive overview of the major categorization of research efforts in ensuring fair machine learning and enlists important works in all categories. 
Explainable machine learning has also seen interest from other communities, specifically healthcare~\citep{tjoa2019survey1}, having huge social implications. 
Several works have summarized and reviewed other research in explainable machine learning~\citep{xai-survey2,carvalho2019:survey3,xai-survey4}. 

\subsection{Explainability in Machine Learning}
% \Kee{there's a weird transition here into the next section. Maybe we need as section 2.3 with an XAI overview, of which CF is but one part. But that might be redundant with a similar paragraph in a preceding section.}
% As the introduction section already outlines the importance of explainability in AI and its broad categorizations, 
This section gives some concrete examples that emphasize the importance of explainability and give further details of the research in this area. 
In a real-world example, the US military trained a classifier to distinguish enemy tanks from friendly tanks. Although the classifier performed well on the training and test dataset, its performance was abysmal on the battlefield. 
Later, it was found that the photos of friendly tanks were taken on sunny days, while for enemy tanks, photos clicked only on overcast days were available~\citep{xai-survey4}. 
The classifier found it much easier to use the difference between the background as the distinguishing feature. 
In a similar case, a husky was classified as a wolf because of the presence of snow in the background, which the classifier had learned as a feature associated with wolves~\citep{ribeiro_why_2016}. 
The use of an explainability technique helped discover these issues. 
% Had an explainability technique been used to understand the working of the classifier, the problems on the battlefield could have been averted. 

The explainability problem can be divided into model explanation and outcome explanation problems~\citep{xai-survey4}. %and model inspection problems. 

\emph{Model explanation} searches for an interpretable and transparent global explanation of the original model. 
% The interpretable model should have high fidelity to the original model and be understandable to humans, such as linear models, rule sets, and decision trees. 
Various papers have developed techniques to explain neural networks and tree ensembles using single decision tree~\citep{craven_exp1,KRISHNAN_exp2,Pedro_exp4} and rule sets~\citep{Deng_exp5,Andrews_exp6}.
Some approaches are model-agnostic, such as Golden Eye and  PALM~\citep{Andreas_exp7,Krishnan_exp8,Zien_exp9}. 

\emph{Outcome explanation} needs to provide an explanation for a specific prediction from the model. 
This explanation need not be a global explanation or explain the internal logic of the model. 
Model-specific approaches for deep neural networks (CAM, Grad-CAM~\citep{Khosla_cam,grad-cam}), and model agnostic approaches (LIME, MES~\citep{ribeiro_why_2016,Turner2016_MES}) have been proposed. 
These are either feature attribution or model simplification methods. 
Example-based approaches are another kind of explainability technique used to explain a particular outcome. 
This work focuses on counterfactual explanations (CFEs), which is an example-based approach. 

By definition, CFEs are applicable to supervised machine learning setups where the desired prediction has not been obtained for a datapoint. 
The majority of research in this area has applied CFEs to classification settings, which consists of several labeled datapoints that are given as input to the model, and the goal is to learn a function mapping from the input datapoints (with, say, m features) to labels. 
In classification, the labels are discrete values. 
$\mathcal{X}^m$ is used to denote the input space of the features, and $\mathcal{Y}$ is used to denote the output space of the labels. 
The learned function is the mapping $f: \mathcal{X}^m \to \mathcal{Y}$, which is used to predict labels for unseen datapoints in the future. 

% This can be further categorized into classification and regression., whereas in regression the labels are continuous values. 

% Fairness-y stuff
% Explainability vs interpretability vs transparency vs ...
\vspace{-2pt}
\subsection{History of Counterfactual Explanations}

Counterfactual explanations have a long history in other fields like philosophy, psychology, and the social sciences. Philosophers like David Lewis published articles on the ideas of counterfactuals back in 1973~\citep{Lewis1973:phil2}. 
\citet{Woodward2003:phil4} said that a satisfactory explanation must follow patterns of counterfactual dependence. 
Psychologists have demonstrated that counterfactuals elicit causal reasoning in humans~\citep{Byrne:psycho1,Byrne2019:psycho2,Kahneman1986:psycho3}. 
Philosophers have also validated the concept of causal thinking due to counterfactuals~\citep{VanFraassenBas1980:phil5,Woodward2003:phil4}. 

%Counterfactual explanations are surfacing as a popular means of providing explanations to the receivers of a machine generated decision. % \jpd{Stronger intro.  Something about the legal right to explanation.}

Studies have compared the likeability of CFEs with other explanation approaches. 
\citet{Binns:2018} and \citet{Dodge-explaining:2019} performed user studies that showed that users prefer CFEs over case-based reasoning, which is another example-based approach. 
% This was also corroborated in the user study performed by . \todo{expand}
The work by \citet{fernandez-loria_explaining_2020} provides three interesting examples where the feature importance explanation methods fail to capture the underlying model, whereas CFEs do. 
\citet{cfe-fair-adequate-Asher} argue that the partiality and locality of CFEs make them epistemically accessible and an adequate form of explanations. 

%\jpd{IMO this entire para should be in prelims or relwork or something, maybe move the para elsewhere, condense into one sentence and include in the intro as well.} 
