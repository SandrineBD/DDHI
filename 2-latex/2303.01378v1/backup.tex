
\documentclass[sigconf]{acmart}

\newcommand{\topic}[1]{\noindent \underline{ \bf #1}}


\newcommand{\ukcomment}[1]{\textcolor{red}{UK: #1}}


\newcommand{\kscomment}[1]{\textcolor{blue}{KS: #1}}


\newcommand{\sgcomment}[1]{\textcolor{red}{SG: #1}}


\newcommand{\hscomment}[1]{\textcolor{yellow}{HS: #1}}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}





%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Using Semantics for Effective and Trustworthy Data Science}


\begin{abstract}
 Data Scientists leverage common sense reasoning and domain knowledge to understand and enrich data for building predictive models. In recent years, we have witnessed a surge in tools and techniques for {\em automated machine learning}. While data scientists can employ various such tools to help with model building, many other aspects such as {\em feature engineering} that require semantic understanding of concepts, remain manual to a large extent. 
  In this paper we discuss important shortcomings of current automated data science solutions and machine learning. We then envision how leveraging semantic reasoning on data in combination with novel tools for data science automation can help with consistent and explainable data augmentation and transformation. Moreover, semantics can assist data scientists in a new manner by helping with challenges related to {\em trust}, {\em bias}, and {\em explainability} in machine learning.
\end{abstract}


\keywords{semantics, automl, data science}

\maketitle

\section{Introduction}


In recent years, the automation of machine learning (AutoML) or data science processes has received considerable attention\footnote{\url{ https://www.tinyurl.com/forbes-automated-data-science/}\hspace{0.2cm}}.%\url{https://forbes.com/sites/ryoheifujimaki/2019/07/18/how-automation-is-changing-data-science/}}
The driving force behind AutoML is the desire to reduce human intervention and thereby save time and cost, and improve efficacy of the modeling process. Such automation has recently been achieved through techniques such as reinforcement learning, multi-arm bandits, and numerical optimization methods such as bayesian optimization amongst others. 
While the automation efforts proposed so far have led to efficiencies in different parts of the data science (DS) process, one dimension that is relatively untouched is that of {\em semantics} in data. Traditionally in data science, where the process is heavily dependent on a human data scientist or a domain expert\footnote{We refer to a data scientist as the conductor of ML or DS process.}, semantics of the data plays an important role. The data scientist studies the given problem or data and relates them to the concepts in the real world and uncovers relationships between them. This is followed by linking the problem, typically a regression or a classification task defined by the target variable, to the concepts represented by features. Finally, the data scientist utilizes the knowledge of the domain, or general knowledge of the world to perform operations which enhance the modeling capability of the given data, such as feature engineering (FE). 
Amongst the automation techniques thus far, only few have made progress concerning semantics in data science. Much of this area is still open for further development; it is much needed in industry and is of interest in academic research as well. In this paper, we survey the work done so far in semantic data science. We ideantify opportunities and propose certain ideas for extending automated data science techniques and methods in a semantic direction.



%\section{A Motivating Example}
\begin{figure}
    \centering
    \includegraphics[scale=.25]{img/SemanticDataSciencePaper.pdf}
    \caption{Motivating example on modeling Diabetes with Semantics}
    \label{fig:example}
\end{figure}

Figure~\ref{fig:example} shows a motivating example for injecting semantics into data science.  The input is a table about modeling diabetes as a function of age, height, weight, etc.  It is now possible (albeit not perfectly) to imbue this data with semantics shown in green labels to help make sense of the data. The right blue panel shows the sources of knowledge available that can be leveraged to add semantics.  Knowledge graphs such as DBpedia, Wikipedia, or massive corpora of tables on the web can help make sense of columns.  As an example, column to concept mapping technology~\cite{khurana2020semantic} can help map named entity columns such as ``Patient'' to the concept \texttt{Person}.  The Person concept in knowledge graphs frequently has a set of attributes in knowledge graphs, which can be exploited to match ``Ht'' to \texttt{Height}, using approaches for column to property matching.  Units can be approximated by signatures on various attribute distributions in knowledge graphs.  In addition to knowledge graphs, there is a wealth of information buried in unstructured sources such as Wikipedia, text cells of data science notebooks and code. Mining techniques on these can be used on these corpora to automatically create features for the data science problem at hand.  As an example, the formula for calculating BMI can be mined from both code and text in Wikipedia, and BMI can be used as an semantics-based feature to improve diabetes prediction.  Furthermore, formulae mined from notebooks can either allow for the extraction of expressions for feature engineering, or provide hints on when such augmentation is likely to produce a meaningful signal, for instance, if the expression appears often across notebooks working with similar data. 

Feature engineering is central to any successful model building task. It is the process of altering the feature space representation to better suit model fitting or predictability of the target variable~\cite{fechapter}. It is also a step that heavily involves semantics. One example is illustrated in the BMI derivation in Figure~\ref{fig:example}, where we outline how an automation of this process can work via Wikipedia.  To show the generalizability of using semantics for feature engineering, we add two more examples drawn from different domains.  First, consider an example of loan approval prediction from Kaggle\footnote{\url{https://www.tinyurl.com/yonatanrabinovich-loan}}%\url{https://kaggle.com/yonatanrabinovich/loan-prediction-dataset-ml-project}} 
where the goal is to predict whether a given loan application will be approved or not. Amongst the provided attributes are the loan amount, the gender of the applicant, marital status, education, applicant income, co-applicant income and others. A data scientist knowledgeable about the domain of loans would arrive at ${applicant\_income + coapplicant\_income} \over {loan\_amount}$ as one of the predictors. Performing this step of feature engineering requires various levels of semantic operations. This includes primarily mapping given columns to real-world concepts; then, finding relationships between these feature concepts; finally, finding which relationships make sense with respect to the target concept and so on.

Our second example is drawn from Kaggle datasets on COVID\footnote{\url{https://www.kaggle.com/imdevskp/corona-virus-report}}.  A common ratio that is used for modeling the disease is the case fatality rate (CFR).  CFR is a formula relating the number of deaths to confirmed cases such that ${\text{CFR}}{\%}={\frac {\text{Number of deaths from disease}}{\text{Number of confirmed cases of disease}}}\times 100$.   Such formulas can be found in both code manipulating COVID notebooks as well as in Wikipedia, and again can be used to create new features for model building.

In addition to feature engineering, other aspects of machine learning (ML) and data science are also dependent on semantic interpretation of data and models. For example, model explainability is based in part on interpretation of concepts in the data and the meaning of operations performed on them. In order to ensure fairness through de-biasing, we require recognition of traditional points of bias~\cite{mehrabi2021survey} and ensuring the same are not repeated in the future. In order to automate this process, semantic concept recognition would be central to any meaningful system that identifies and removes it. Similarly, enforcing business rules through symbolic-AI~\footnote{\url{https://tinyurl.com/inrule-business-rules-ml}}%\url{https://inrule.com/resources/blog/business-rules-and-machine-learning/}} 
and semantics enhances the case for semantically driven automated data science.  In addition, initial work has targeted understanding hyper-parameters and extraction of constraints in applying them \cite{hpodoc}, as well as use of semantics for data cleansing \cite{6982731}.  

In this paper, we first talk about relevant works in semantic understanding and annotation of tabular data (Section~\ref{tabular_data_section}), code (Section~\ref{code_section}) and text (Section~\ref{nlp_expression_section}), as the means of building infrastructure for semantics-oriented data science. 
We then focus our attention on automated feature engineering (Section~\ref{autofe_section}) with the popular non-semantic work and the recent developments in semantics-oriented feature engineering. 
This is followed by machine learning and particularly AutoML (Section~\ref{ml_section}).
We conclude with extending these ideas for trustworthy machine learning from a semantic dimension (Section~\ref{trustworthy_section}). % and an estimated future trajectory of research in semantic AutoML (Section~\ref{conclusion_section}).


\topic{What is Semantics?:}
By semantics, we refer to machine interpretable representations of meaning in data. For instance, in the Figure~\ref{fig:c2c}, the appropriate semantics of the four columns are represented by the concepts: \texttt{IATA Code}, \texttt{City}, \texttt{Country} and \texttt{Year Established}, respectively. Similarly, concepts may define a table as a whole (e.g., \texttt{Airport}), or even multiple tables if they share an abstract concept. Relevant machine interpretable sources of such concepts are so-called knowledge graphs, which have various instances of each concept (e.g., cities such as New York) as well as the fact that various cities belong to the same set, with an is-a or type relation.  We also broaden our notion of semantics to representations of data in vector space, learnt through some AI model that may learn co-occurrences between different words and map them closer in vector space.  This latter view has dominated natural language research recently, so it seems important to consider this dimension of semantics as well.  Furthermore, for the scope of this paper, we will restrict our scope to tabular data.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{img/c2c.pdf}
    \caption{Column to concept mapping.}
    \label{fig:c2c}
\end{figure}

\section{Understanding Semantics}
\begin{figure*}
    \centering   \includegraphics[width=\textwidth]{img/SemanticVision.pdf}
    \caption{Semantics in a data science pipeline.}
    \label{fig:overview}
\end{figure*}
\label{data_section}
Any attempt to use data in a data science context starts with a need for table understanding, as shown in Figure~\ref{fig:overview}, which in turn can help guide the use of semantics in feature generation, as well as choose models appropriate for the domain.  Explainability in models is enhanced by a knowledge of causal relations between key variables in the domain.  The figure illustrates how semantics can greatly enhance each component in data science workflows. 

\subsection{Table Understanding}
\label{tabular_data_section}
Table understanding helps with explainability, trust, bias and feature engineering aspects of data science because it helps pick more suitable features overall to build models. 
 Table understanding can also guide search for relevant data, including tables that make sense to join or union.   Here we cover a sample of the literature on table understanding to show that while challenging, tabular data understanding is possible.

Table understanding involves identifying the types of columns of a table (CTA), identifying the core columns that contain entities of interest, columns that map to properties of these entities (CPA), and mapping tables themselves to broad categories. The goal here is to automatically identify the correct concepts of the columns provided in the data. Additionally, we would like to see the relationships betweeen the columns expressed in the appropriate heirarchy to reflect the intent of the dataset.
There is now a wealth of research in understanding the semantics of data, which is a crucial first step for semantic data science.  Broadly, the research in this area can be categorized into: (a) {\em table understanding}, which is to map cell values, columns, and indeed entire tables into a well defined set of concepts drawn from ontologies or knowledge graphs; (b) {\em inter-table data discovery}, which discovers for instance, other tables that can be joined or unioned based on a larger index of tables.  We describe the two separately below.

\subsubsection{Knowledge graph based approaches}
Most approaches to table understanding assume the existence of one or more knowledge graphs or ontologies that define the universe of concepts to understand tabular cells or columns.  Columns that contain largely numeric values, dates, social security numbers, etc., are unlikely to appear in knowledge graphs, and are often called L columns.  For columns that contain strings which may potentially map to entities (called NE-columns), tabular understanding is broken into cell entity annotation (CEA), column type annotation (CTA), and column property annotation (CPA), with CEA often being used by many systems to help CTA and CPA.  An additional task considered by many systems is the detection of the subject column (S) which identifies the core entity column, with other columns in the table representing properties of the core entity.  Numerous systems now target these tasks, spurred by the presence of a challenge for these tasks \cite{semtab2019}, with the base knowledge graph being DBpedia or Wikidata.  Example systems that perform one or more of these NE tasks include MantisTable \cite{DBLP:journals/fgcs/CremaschiPRS20}, ColNet \cite{city22932}, TableMiner+ \cite{10.1007/978-3-319-18818-8_25}, and Sherlock \cite{10.1145/3292500.3330993}. Most of these systems are based on neural network based meta-learning on embeddings of column vectors. These approaches are powerful in some cases when plenty of good training examples are available. However, in many cases they do not scale well over the number of column concepts, and particularly on numerical data and in cases where training data is scarce or noisy. $C^2$\cite{khurana2020semantic} presents a maximum likelihood ensemble approach over multiple sources of data such as knowledge bases and noisy webtables and is more scalable in the number of concepts. Some systems, such as Memei \cite{Takeoka_Oyamada_Nakadai_Okadome_2019} jointly train on the CTA task along with column-column relationships to be able to annotate L columns.  

While the approaches and tasks performed vary across systems (e.g., supervised or semi-supervised), in almost all cases, the matching of columns to knowledge graphs is challenging enough for realistic tables. At best, it provides limited support for entirely automated semantic feature engineering.  This is in part due to at least two key limitations of the knowledge based approach: (a) Recall of knowledge bases is severely limited.  In one estimate, only 3\% of the tables contained in the 3.5 billion HTML pages of the Common Crawl Web Corpus can be matched to
DBpedia \cite{10.1145/2872427.2883017}. (b) The approach is especially challenging for L columns, unless the S column detection is perfect and the column heading maps to a knowledge graph property using CPA.  Knowledge agnostic techniques reviewed next circumvent some of the limitations of knowledge graph based approaches for poor recall.

\subsubsection{Knowledge graph agnostic approaches}
Knowledge graph agnostic approaches use large corpora of publicly available tables as knowledge, with schema matching techniques being used to map to tables and columns across the corpus.  The challenge here is how to scale this computation to millions of annotations, to perform what is often termed in the literature as holistic schema matching. Various approaches have been taken to this problem including a binary integration strategy that matches iteratively to create a single vocabulary \cite{10.1145/27633.27634}, or vectorizing column values and using techniques such as locality sensitive hashing (LSH) to minimize the number of pairwise comparisons (\cite{10.1007/978-3-642-35176-1_4}), amongst others.  

More recently, there has been an attempt to vectorize tables from large corpora.  Inspired by work in language models such as BERT \cite{bert}, which are effective in encapsulating correlations between words in a sentence, some recent works approach the problem of assigning types to columns by building unsupervised BERT based language models for columns based on their values \cite{trabelsi2020semantic}, adding other intermediate column labels to provide context to improve annotation.  In \cite{trabelsi2020semantic}, they use 1.6 million tables from the WikiTables domain to build the model.  This approach of using a corpus as a reference knowledge base is clearly attractive because it can greatly alleviate the recall problem of knowledge graphs. The availability of large table corpora such as those gathered from the web\footnote{\url{http://webdatacommons.org/webtables/}}, Viznet\footnote{\url{https://github.com/mitmedialab/viznet}}, Data.gov\footnote{\url{https://www.data.gov/}} make this approach promising and feasible.

%https://arxiv.org/pdf/1909.04844.pdf

\subsection{Inter-table discovery}
While features can often be constructed based on a single table, it is more common in data science that the scientist joins or unions data across tables to compute a model.  Table similarity detection targets this space of data wrangling.  Table similarity detection based on a number of metrics such as the number of matching rows, number of matching columns, the amount of information gained by possibly combining related tables, etc., are used by systems such as Juneau to increase the number of possible features for feature engineering \cite{10.1145/3318464.3389726}.  The problem of finding interesting joins in a data lake is addressed by JOSIE, which performs an efficient top-k overlapping set similarity search over millions of tables \cite{10.1145/3299869.3300065}.  Joinable tables are ranked by the size of overlap to a queried key column.  JOSIE works on equi-joins.  PEXESO \cite{Dong2021EfficientJT} performs joins on columns with string values using word embeddings, because joinable columns often have slight mismatches in the string representation.  PEXESO is also evaluated for data enrichment; the system shows 1.9\% higher micro-F1 score and 10\% lower mean squared error on machine learning tasks. KAFE~\cite{galhotra2019automated} and S3D~\cite{s3d} create effective joins and unions with knowledge graphs and tables large data lakes by identifying semantic concepts of the given data with those of the database table columns.

\subsection{Mining Semantics in Code and Text}
Assuming we have the technologies to find useful sources of data to model a target variable, automated feature engineering still requires the mining of formulas that allow us to construct meaningful features for a given target, as illustrated in the BMI, CFR or loan examples.  We describe two areas for such mining - one from code, and another from text.


\subsubsection{Data science through mining code:}
\label{code_section}
%https://www.josecambronero.com/pdf/wranglesearch-draft.pdf
Recent work has targeted mining code repositories to help the data scientist.  Wranglesearch\footnote{\url{https://www.josecambronero.com/pdf/wranglesearch-draft.pdf}} is a system that mines Kaggle notebooks using dynamic analysis to extract functions and expressions that may help with data wrangling, and is perhaps the most relevant work for automated feature engineering.  Others target the larger data science development space.  \cite{10.1145/3447548.3467455} for instance mines Kaggle notebooks and scripts to find what they call decision points, where alternative models or parameters might be considered, and provide the data science users with these alternatives, so that they might build more robust models.  SOAR \cite{9402016} uses embedding techniques on the documentation of API calls combined with programming by example type approaches to suggest refactoring of data science code from one API to another (e.g. from tensorflow to PyTorch).


\subsubsection{Mining expressions from text:}
\label{nlp_expression_section}
Vast amounts of domain knowledge useful for data science is buried in text as well.  There has been work that targets the extraction of topics for mathematical equations \cite{Yasunaga_Lafferty_2019} by building joint neural models for the textual context around the equation along with the equation itself.  In a data science context, it would be more useful for searching relevant expressions to surface to a data scientist, or to extract features if the terms of the equation can be mapped to columns successfully.  \cite{10.1145/3366423.3380218} describe the use of modified information retrieval techniques on an extracted database of several million equations; they, for instance, find equations associated with {\em Jacobi Polynomial}, or complete equations.  Once again, from a data science perspective, \cite{10.1145/3366423.3380218} connect equations to textual topics, but its utility in an entirely automated feature engineering mechanism is still an open question.

A related area of work is with respect to mining causal relationships from text; causality often determines which features may be semantically meaningful to model a target variable.  Work related to automated causal extraction from text is rich (e.g., \cite{LI2019512}, \cite{hassanzadeh2019answering}). Extraction of such information from notebooks and domain specific corpora could help with feature selection.

\section{Data Preparation}
\label{autofe_section}
%The desired role of semantics in data engineering or preparation is to recognize each feature's meaning in the given problem and identify all the additional features that can be added to staisfy the goal of the given problem. This can be done by (1) identifying the relationship between pairs (or three) of features by the means of external knowledge such as those found in Wikipedia, books, code, knowledge graphs, etc., and applying the corresponding formulae; (2) Bring in additional attributes of the given features to accordingly enhance the data.

%Where we currently stand: (1) KAFE: for well defined cell entities, we can bring in attributes from Knowledge graphs and partially from webtables. (2) Mining from code: some progress so far (see AAAI demo paper); (3) Mining from text (ongoing at IBM).


In this section, we discuss the different aspects of data preparation and its impact on the downstream analytics.


delve deeper on automations in data preparation. In the context of supervised machine learning, this is also referred to as feature engineering (AutoFE). We first capture the work that ignores semantics of the data and discuss how there is a missed opportunity in incorporating semantics. Next, we talk about some recent research that is based on semantics. Finally, we present our ideas with suggestions to extend various semantic-oblivious approaches with semantics.
\subsection{Semantics Oblivious Approaches}
\label{autofe_nonsem}
There is a diverse set of approaches towards automated feature engineering which are summarized below.
FICUS~\cite{MarkovitchRosenstein02} performs a beam search over the space of possible features, constructing new features by applying constructor functions. It is guided by heuristic measures based on information gain in a decision tree.
%, and other surrogate measures of performance. 
Data Science Machine by~\cite{kanter:dsm} applies all transformations on all features at once (but no sequence of transformations), then performing feature selection and model hyper-parameter optimization over the augmented dataset. FEADIS~\cite{DorReich12} works through a combination of random feature generation and feature selection. 
%It adds constructed features greedily, and as such requires many expensive performance evaluations. 
ExploreKit~\cite{DBLP:conf/icdm/KatzSS16} expands the feature space explicitly, one feature at a time. It employs learning to rank the newly constructed features and evaluating the most promising ones. 
LFE~\cite{lfe} directly predicts the most useful transformation per feature based on learning effectiveness of transforms on sketched representations of historical data through a perceptron. 
The approach introduced in~\cite{recommendersystem2018} is fully focused on leveraging historical information using a recommender system. The approach is very effective in determining a pipeline while being limited by only being able to select from a fixed set of pre-existing pipelines.
More recent and initial work~\cite{alphad3m} inspired by AlphaGo Zero~\cite{alphazero} models pipeline generation as a single-player game and builds pipelines iteratively by selecting among a set of actions which are insertion, deletion, replacement of pipeline parts.
%It considers features independent of each other; it is demonstrated to work only for classification so far, and does not allow for composition of transformations. 
Cognito~\cite{khurana2018feature,khurana2016cognito} use reinforcement learning to optimize feature transformation through an agent.

\subsection{Missed Opportunities due to lack of Semantics in Automated Feature Engineering}
The state-of-the-art feature engineering systems mentioned above rely on trial and error for generating new features through transformation functions on the provided features. They try to maximize accuracy but are oblivious of the actual semantics of the data and transformation functions. This has several drawbacks. 
Firstly, they can not utilize external knowledge to perform specific transformations on data which might be very difficult to stumble upon using generalized exploration techniques. For example, BMI (Figure~\ref{fig:example}) is a complex formula which any of these techniques will take a very long time and number of trials before discovering. Secondly, they could violate physical laws such as addition of {\texttt height} and {\texttt weight}, which would be a valid exploration option in these methods but through worldly knowledge, we know that it does not physically make sense and should be avoided. Third, these methods do not have the capability of bringing additional data not provided in the problem but available externally and help improve the modeling capacity. 
Existence of abundance of knowledge in the form of knowledge bases, digital books, wikipedia, blogs, and data science code and so on is a missed opportunity for automated feature engineering.

\subsection{Semantic Approaches}
There is recent work on enhancing a given dataset with new columns that help make more accurate predictive models. Friedman et al.~\cite{friedman2018recursive} and Galhotra et al.~\cite{galhotra2019automated} recursively generate new features using a knowledge base for text classification and web tables, respectively. FeGeLOD~\cite{PH} extracts features from Linked Open Data via queries. These approaches, however, fail to thoroughly explore the available information from multiple data sources. S3D~\cite{s3d} is able to bring in data from multiple sources such as webtables and multiple knowledge graphs but adds new columns irrespective of the target variable. Semantic Feature Discovery~\cite{sfd} uses formulae from a historical repository of data science code files and notebooks by linking concepts in a given data problem with those in the historical code. Common approaches to mining code for new features include dynamic analysis (TBD cite from expressions paper), and more recently, program analysis.  By reusing these formulae, it generates potentially useful new features. While these approaches are promising, their impact is bounded due to the limited availability and richness of knowledge bases, and the quality of underlying concept matching algorithms. However, we expect that to improve over time.

Key challenges: (a) how do we figure out which expressions apply to a given new dataset.  For instance, should we apply all mined expressions that involve date, even if the domain is different (e.g., stocks versus covid).  (b) how to capture the domain of mined expressions even when no dataset exists (web search?, other approaches?), (c) how to perhaps establish connections between existing indexed datasets and code (this is surprisingly hard), (d) how to index expressions such that they are easily generlized to new datasets (column to concept mapping).

\subsection{Proposal for Utilizing Semantics in AutoFE}
~\cite{cherrier2019consistent} use a genetic programming formulation for feature construction, but with constraints in the domain of physics. Examples of constraints are -- energy and dimension can not be added as it does not make sense physically, and performing square after square root does not make much sense either, and so on. We argue a generalization of this approach on top of existing non-semantic approaches can significantly benefit latter's efficacy. Whether it is feature engineering through genetic programming as advised by~\cite{cherrier2019consistent} or through other optimization methods described in Section~\ref{autofe_nonsem} such as Cognito~\cite{khurana2018feature}, DSM~\cite{kanter:dsm}, ExploreKit~\cite{DBLP:conf/icdm/KatzSS16} or even metalearning approach such as LFE~\cite{lfe}, by identifying semantic types and enforcing certain constraints (some of which one can argue are global, whereas some specific to the problem) that can be expressed in an appropriate grammar are beneficial. Any of these methods can be restricted in their (non-beneficial) operations by simply checking for semantics or constraint consistency. 
This would require a common grammar to represent such constraints and an effective engine to enforce them. It might of interest for researchers in this area to develop a language that is expressive enough to capture constraints from various domains but simple enough to specify them easily or translate existing business or scientific rules to the new language.

\section{Machine Learning and Semantics}
\label{ml_section}
Similary to the notion of \emph{model-free} in Reinforcemnt Learning that refers to policy estimation without considering the dynamics in the underlying environment, one can argue that all of current ML (including DL and RL) is model-free in terms of semantics. There is no explicit notion of what a feature, state, or label 'means' and consequently none of this information is used explicitly to build a ML model.
And this even includes the powerful foundational models for language (GPT3), speech, images (ImageNet), and video, that display impressive performance on tasks such as math (Minerva), image (Dalle, Imagine) and text generation (Q\&A).
While such models are indeed able to generate semantically appropriate responses, the underlying models have no understanding of semantics. ML models are not given nor using semantics of the input data - if a given feature is about number of cars or covid cases in NYC the model is oblivious to it and it is mostly crafted by statisitical and target based measures. The data-driven way of deriving insights is obviously desired, but the meaning of data itself beyond statistical meaning has been neglected and the resulting issues can be easily highlighted (e.g. spurious correlations).
It is so difficult to incorpate semantics because  common-sense reasoning has not been accomplished by AI. However, even limited amounts of semantics can help build better AI models. For instance, if the model knows beforehand that a state-action pair is not valid (e.g. flush toilet in kitchen) there is no need to explore this scenario. The knowledge on the data itself can stem from any source - including foundational models ("Can Wikipedia Help Offline Reinforcement Learning?"). Similarly, if we know there is no semantic relationship between features one could consider not to connect them in a Neural Network to enable a model that can be easier explained. As mentioned before feature engineering can now also be based on data transformation that Data Scientist have applied before (eg computation of BMI) and are not limited to only data-driven learned features.

\subsection{AutoML}

While we focus mostly on the impact of semantics on automated data engineering and understanding, it can also be leveraged as part of the AutoML optimization itself. For instance, within hyper-parameter optimization, we may analyze code and Python DocStrings to not only determine available hyper-parameter settings automatically~\cite{hpodoc}, but also to extract constraints on hyper-parameter configurations such as mutually exclusive settings. This at least reduces the search space to valid choices automatically. %This is also a step towards AutoML adopting new algorithms on its own. 
At the pipeline configuration level, one can observe patterns in existing code such as PCA being nearly always preceeded by normalization and not the other way around and use them for instance as a way to prioritize specific pipelines in the automated pipeline exploration.

Hyperparameter search is one aspect of AutoML that has been targeted for the use of semantics.  The search space for finding the correct model is large, and the focus here is on using existing artifacts around machine learning to perform a more targeted search through the space of hyperparameters for a model.  \cite{10.1145/3447548.3467455} for instance, mines Kaggle notebooks and scripts to find what they call decision points, where alternative models or parameters might be considered, and provide the data science users with these alternatives, so that they might build more robust models.  
Another approach examines if we can mine data science code for better pipeline search for unseen datasets (\cite{KGPIP}).


\section{Using Semantics for Trustworthy Analytics }%Bias, Fairness and Explainability}
\label{trustworthy_section}
The use of machine learning techniques to make high-stakes decisions has raised the importance of trust, which is generally captured in terms of fairness, explainability, robustness, among others. Most of the prior techniques study societal impact of Machine learning by exploring correlation between dataset attributes without understanding their true meaning. 
Consequently, fairness-aware learning techniques are brittle to noise in datasets and do not generalize to real-world settings where datasets may suffer from selection bias, and may even contain spurious correlations. Explainable AI techniques generate explanations that are not semantically coherent and are difficult to comprehend. 

\subsection{Using Causal dependencies to understand semantics}
One of the initial steps towards enabling semantically coherent learning has been to leverage  causal dependencies between attributes. 
These studies have explored the use of causal information in two different ways. (i) Semantic constraints: These techniques impose constraints over input attributes to ensure that the studied combinations of attribute values are feasible. For example, it is not possible that a record with Age $<18$ has Marital Status = Married.
(ii) Dependency information: These techniques evaluate the dependencies between attributes as external knowledge. For example, increasing salary generally means an increase in savings. 


Causal relationships do not particularly capture the meaning of a particular feature but rather the dependencies between them. Consideration of causal dependencies is a step towards leveraging semantics for analytics. 
These techniques have shown a lot of promise to incorporate causal information in ML but has also raised numerous challenges. 

\noindent \textbf{How to identify attribute dependencies?} Causal dependencies between attributes are estimated from observational data, and it is often inaccurate due to the limited size of available data. Domain scientists need to manually validate semantic constraints/ causal dependencies as a post-hoc analysis to make sure that the analysis is causally consistent.

\noindent \textbf{How to incorporate semantic constraints and dependencies?}
Causation requires testing the effects of changes in attributes, which is often referred to as treatment or intervention. Incorporating interventions into the traditional correlation-based frameworks requires development of new techniques.

\noindent \textbf{How can external information help?}
Most techniques rely on domain scientists to either impose causal constraints or input dependency information in the form of a causal graph. Reducing the dependence of such techniques on such external information is an important challenge that needs more work. External sources of information like data lakes, Knowledge Graphs contain useful information that can be helpful to infer dependencies. 



\bibliographystyle{plain}
\bibliography{main}
\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
